[2025-07-18T15:21:36.514+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze manual__2025-07-18T15:21:31.773377+00:00 [queued]>
[2025-07-18T15:21:36.518+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze manual__2025-07-18T15:21:31.773377+00:00 [queued]>
[2025-07-18T15:21:36.518+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-07-18T15:21:36.523+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): stream_to_bronze> on 2025-07-18 15:21:31.773377+00:00
[2025-07-18T15:21:36.526+0000] {standard_task_runner.py:57} INFO - Started process 240 to run task
[2025-07-18T15:21:36.527+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'restaurant_pipeline', 'stream_to_bronze', 'manual__2025-07-18T15:21:31.773377+00:00', '--job-id', '93', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_pipeline.py', '--cfg-path', '/tmp/tmpg2fznmjk']
[2025-07-18T15:21:36.529+0000] {standard_task_runner.py:85} INFO - Job 93: Subtask stream_to_bronze
[2025-07-18T15:21:36.550+0000] {task_command.py:416} INFO - Running <TaskInstance: restaurant_pipeline.stream_to_bronze manual__2025-07-18T15:21:31.773377+00:00 [running]> on host 9bcfb43e0ab7
[2025-07-18T15:21:36.582+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='moran' AIRFLOW_CTX_DAG_ID='restaurant_pipeline' AIRFLOW_CTX_TASK_ID='stream_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-07-18T15:21:31.773377+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-18T15:21:31.773377+00:00'
[2025-07-18T15:21:36.582+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-07-18T15:21:36.583+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec spark-iceberg spark-submit /home/iceberg/spark/stream_to_bronze.py']
[2025-07-18T15:21:36.588+0000] {subprocess.py:86} INFO - Output:
[2025-07-18T15:21:38.052+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkContext: Running Spark version 3.5.6
[2025-07-18T15:21:38.052+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T15:21:38.053+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkContext: Java version 17.0.15
[2025-07-18T15:21:38.072+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO ResourceUtils: ==============================================================
[2025-07-18T15:21:38.072+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-18T15:21:38.072+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO ResourceUtils: ==============================================================
[2025-07-18T15:21:38.073+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkContext: Submitted application: StreamToBronze
[2025-07-18T15:21:38.086+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-18T15:21:38.093+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO ResourceProfile: Limiting resource is cpu
[2025-07-18T15:21:38.093+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-18T15:21:38.123+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SecurityManager: Changing view acls to: root,spark
[2025-07-18T15:21:38.124+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SecurityManager: Changing modify acls to: root,spark
[2025-07-18T15:21:38.124+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SecurityManager: Changing view acls groups to:
[2025-07-18T15:21:38.124+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SecurityManager: Changing modify acls groups to:
[2025-07-18T15:21:38.124+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
[2025-07-18T15:21:38.154+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-18T15:21:38.282+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Utils: Successfully started service 'sparkDriver' on port 44175.
[2025-07-18T15:21:38.296+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkEnv: Registering MapOutputTracker
[2025-07-18T15:21:38.318+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-18T15:21:38.326+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-18T15:21:38.327+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-18T15:21:38.329+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-18T15:21:38.343+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0bace984-4c0c-4e0f-982c-e56d37da7848
[2025-07-18T15:21:38.350+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-18T15:21:38.358+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-18T15:21:38.416+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-18T15:21:38.445+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-07-18T15:21:38.491+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Executor: Starting executor ID driver on host 77cb57a6bd53
[2025-07-18T15:21:38.491+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T15:21:38.491+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Executor: Java version 17.0.15
[2025-07-18T15:21:38.495+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-18T15:21:38.495+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@6b94240 for default.
[2025-07-18T15:21:38.505+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43611.
[2025-07-18T15:21:38.505+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO NettyBlockTransferService: Server created on 77cb57a6bd53:43611
[2025-07-18T15:21:38.506+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-18T15:21:38.511+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 77cb57a6bd53, 43611, None)
[2025-07-18T15:21:38.513+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO BlockManagerMasterEndpoint: Registering block manager 77cb57a6bd53:43611 with 434.4 MiB RAM, BlockManagerId(driver, 77cb57a6bd53, 43611, None)
[2025-07-18T15:21:38.514+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 77cb57a6bd53, 43611, None)
[2025-07-18T15:21:38.517+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 77cb57a6bd53, 43611, None)
[2025-07-18T15:21:38.734+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-18T15:21:38.738+0000] {subprocess.py:93} INFO - 25/07/18 15:21:38 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-07-18T15:21:39.825+0000] {subprocess.py:93} INFO - 25/07/18 15:21:39 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-07-18T15:21:39.839+0000] {subprocess.py:93} INFO - 25/07/18 15:21:39 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-07-18T15:21:39.840+0000] {subprocess.py:93} INFO - 25/07/18 15:21:39 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-07-18T15:21:40.607+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:40.623+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-07-18T15:21:40.658+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/reservations resolved to file:/tmp/checkpoints/reservations.
[2025-07-18T15:21:40.659+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:21:40.694+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/metadata using temp file file:/tmp/checkpoints/reservations/.metadata.5df1d595-a6b4-4960-8775-fca92330d971.tmp
[2025-07-18T15:21:40.742+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/.metadata.5df1d595-a6b4-4960-8775-fca92330d971.tmp to file:/tmp/checkpoints/reservations/metadata
[2025-07-18T15:21:40.773+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Starting [id = 0314df7c-5598-4928-8d91-374ee67989d1, runId = 05e9c6f0-552d-440a-916d-062a56f88daa]. Use file:/tmp/checkpoints/reservations to store the query checkpoint.
[2025-07-18T15:21:40.776+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@51f5d878] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3180416c]
[2025-07-18T15:21:40.808+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T15:21:40.809+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T15:21:40.809+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T15:21:40.811+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T15:21:40.868+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:40.876+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/checkins resolved to file:/tmp/checkpoints/checkins.
[2025-07-18T15:21:40.877+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:21:40.886+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/metadata using temp file file:/tmp/checkpoints/checkins/.metadata.18969d83-b5c0-4eb1-8107-46bb1e9113f8.tmp
[2025-07-18T15:21:40.906+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/.metadata.18969d83-b5c0-4eb1-8107-46bb1e9113f8.tmp to file:/tmp/checkpoints/checkins/metadata
[2025-07-18T15:21:40.918+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Starting [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 635bc90e-09bd-4ae9-a3d8-8039735dac8d]. Use file:/tmp/checkpoints/checkins to store the query checkpoint.
[2025-07-18T15:21:40.928+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@195cbda8] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@179b512b]
[2025-07-18T15:21:40.928+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T15:21:40.928+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T15:21:40.928+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T15:21:40.928+0000] {subprocess.py:93} INFO - 25/07/18 15:21:40 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T15:21:41.006+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.011+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/feedback resolved to file:/tmp/checkpoints/feedback.
[2025-07-18T15:21:41.012+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:21:41.020+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/metadata using temp file file:/tmp/checkpoints/feedback/.metadata.39db0efe-2276-4cec-970f-fdbfea025345.tmp
[2025-07-18T15:21:41.029+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:21:41.029+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:21:41.029+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:21:41.029+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:21:41.030+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:21:41.031+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:21:41.032+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:21:41.033+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:21:41.033+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:21:41.033+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:21:41.033+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:21:41.034+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:21:41.034+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:21:41.034+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:21:41.034+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:21:41.035+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:21:41.035+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:21:41.035+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:21:41.035+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:21:41.035+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:21:41.035+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:21:41.035+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:21:41.036+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:21:41.037+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:21:41.038+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:21:41.039+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:21:41.039+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:21:41.039+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:21:41.039+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:21:41.039+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:21:41.039+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:21:41.039+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:21:41.040+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:21:41.041+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:21:41.041+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:21:41.041+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:21:41.041+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:21:41.042+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:21:41.042+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:21:41.042+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:21:41.042+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:21:41.042+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:21:41.042+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:21:41.043+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:21:41.043+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:21:41.043+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:21:41.043+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:21:41.043+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:21:41.043+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:21:41.044+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:21:41.045+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:21:41.045+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:21:41.045+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:21:41.045+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:21:41.045+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:21:41.045+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:21:41.046+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:21:41.047+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/.metadata.39db0efe-2276-4cec-970f-fdbfea025345.tmp to file:/tmp/checkpoints/feedback/metadata
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO MicroBatchExecution: Starting [id = d3aff090-24bc-4a1c-938f-fc839231598c, runId = 2a9734b5-dec9-4b1e-a3f3-fa1e30f97415]. Use file:/tmp/checkpoints/feedback to store the query checkpoint.
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@614e1b84] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@6f89e805]
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:21:41.048+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:21:41.049+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:21:41.049+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:21:41.049+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:21:41.052+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:21:41.053+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:21:41.054+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:21:41.055+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:21:41.056+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:21:41.073+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:21:41.073+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:21:41.074+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:21:41.074+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:21:41.074+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:21:41.074+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka startTimeMs: 1752852101073
[2025-07-18T15:21:41.075+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:21:41.076+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:21:41.076+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka startTimeMs: 1752852101073
[2025-07-18T15:21:41.076+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:21:41.076+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:21:41.076+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO AppInfoParser: Kafka startTimeMs: 1752852101073
[2025-07-18T15:21:41.338+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/sources/0/0 using temp file file:/tmp/checkpoints/checkins/sources/0/.0.17e0319c-cd1d-4a8f-9231-821590e7a47a.tmp
[2025-07-18T15:21:41.339+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/sources/0/0 using temp file file:/tmp/checkpoints/reservations/sources/0/.0.12f6694e-faf0-46be-83d6-daf9930dcf18.tmp
[2025-07-18T15:21:41.339+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/sources/0/0 using temp file file:/tmp/checkpoints/feedback/sources/0/.0.3f31e4ee-96a2-4a50-9fde-f3d7b530c56c.tmp
[2025-07-18T15:21:41.363+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/sources/0/.0.12f6694e-faf0-46be-83d6-daf9930dcf18.tmp to file:/tmp/checkpoints/reservations/sources/0/0
[2025-07-18T15:21:41.364+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/sources/0/.0.3f31e4ee-96a2-4a50-9fde-f3d7b530c56c.tmp to file:/tmp/checkpoints/feedback/sources/0/0
[2025-07-18T15:21:41.364+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaMicroBatchStream: Initial offsets: {"feedback":{"0":0}}
[2025-07-18T15:21:41.365+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/sources/0/.0.17e0319c-cd1d-4a8f-9231-821590e7a47a.tmp to file:/tmp/checkpoints/checkins/sources/0/0
[2025-07-18T15:21:41.365+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaMicroBatchStream: Initial offsets: {"checkins":{"0":0}}
[2025-07-18T15:21:41.366+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaMicroBatchStream: Initial offsets: {"reservations":{"0":0}}
[2025-07-18T15:21:41.385+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/0 using temp file file:/tmp/checkpoints/reservations/offsets/.0.2d0fb766-20c7-4066-9bac-e13110478a2c.tmp
[2025-07-18T15:21:41.386+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/0 using temp file file:/tmp/checkpoints/checkins/offsets/.0.e1292320-d365-4ec3-b968-6590ac4d705b.tmp
[2025-07-18T15:21:41.386+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/0 using temp file file:/tmp/checkpoints/feedback/offsets/.0.107ce2be-932c-4187-8266-ba197b8e2ab6.tmp
[2025-07-18T15:21:41.414+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.0.107ce2be-932c-4187-8266-ba197b8e2ab6.tmp to file:/tmp/checkpoints/feedback/offsets/0
[2025-07-18T15:21:41.417+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752852101375,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:21:41.417+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.0.2d0fb766-20c7-4066-9bac-e13110478a2c.tmp to file:/tmp/checkpoints/reservations/offsets/0
[2025-07-18T15:21:41.418+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752852101375,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:21:41.418+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.0.e1292320-d365-4ec3-b968-6590ac4d705b.tmp to file:/tmp/checkpoints/checkins/offsets/0
[2025-07-18T15:21:41.419+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752852101375,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:21:41.606+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.607+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.607+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.607+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.607+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.607+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.610+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.610+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.610+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.689+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.689+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.689+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.732+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.733+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.733+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.761+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.762+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.762+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.763+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.763+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.763+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.765+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.765+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.766+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.767+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.768+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.768+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.768+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.768+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.770+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.798+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.799+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.799+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:41.799+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.799+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.799+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.799+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.800+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:41.800+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:41.800+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.800+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.800+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.800+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.803+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:41.803+0000] {subprocess.py:93} INFO - 25/07/18 15:21:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:21:42.028+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 135.3795 ms
[2025-07-18T15:21:42.029+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 135.0495 ms
[2025-07-18T15:21:42.029+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 135.189875 ms
[2025-07-18T15:21:42.120+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:21:42.120+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:21:42.120+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:21:42.156+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 434.2 MiB)
[2025-07-18T15:21:42.159+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 434.2 MiB)
[2025-07-18T15:21:42.160+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.3 KiB, free 434.2 MiB)
[2025-07-18T15:21:42.160+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 77cb57a6bd53:43611 (size: 29.3 KiB, free: 434.4 MiB)
[2025-07-18T15:21:42.160+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 77cb57a6bd53:43611 (size: 29.4 KiB, free: 434.3 MiB)
[2025-07-18T15:21:42.160+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 77cb57a6bd53:43611 (size: 29.3 KiB, free: 434.3 MiB)
[2025-07-18T15:21:42.160+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Created broadcast 0 from start at <unknown>:0
[2025-07-18T15:21:42.161+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Created broadcast 1 from start at <unknown>:0
[2025-07-18T15:21:42.161+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Created broadcast 2 from start at <unknown>:0
[2025-07-18T15:21:42.180+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:21:42.180+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:21:42.181+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:21:42.192+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:21:42.192+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:21:42.195+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:21:42.201+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:21:42.202+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-07-18T15:21:42.202+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:21:42.202+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:21:42.203+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:21:42.244+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T15:21:42.246+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:21:42.247+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 77cb57a6bd53:43611 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:21:42.247+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:21:42.258+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:21:42.258+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-18T15:21:42.270+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:21:42.270+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-07-18T15:21:42.271+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:21:42.271+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:21:42.272+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:21:42.275+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T15:21:42.276+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:21:42.278+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 77cb57a6bd53:43611 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:21:42.279+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:21:42.279+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:21:42.279+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-18T15:21:42.286+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:21:42.290+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:21:42.291+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
[2025-07-18T15:21:42.292+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:21:42.293+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:21:42.293+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:21:42.294+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:21:42.295+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T15:21:42.296+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.1 MiB)
[2025-07-18T15:21:42.296+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 77cb57a6bd53:43611 (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:21:42.297+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-18T15:21:42.298+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-18T15:21:42.298+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:21:42.299+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:21:42.299+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-18T15:21:42.301+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:21:42.301+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-18T15:21:42.393+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 24.640792 ms
[2025-07-18T15:21:42.393+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 25.073708 ms
[2025-07-18T15:21:42.394+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 25.411167 ms
[2025-07-18T15:21:42.410+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 15.500083 ms
[2025-07-18T15:21:42.411+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 15.31 ms
[2025-07-18T15:21:42.414+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 18.4645 ms
[2025-07-18T15:21:42.538+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:21:42.539+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:21:42.539+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:21:42.683+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=0 untilOffset=36, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=0 taskId=0 partitionId=0
[2025-07-18T15:21:42.690+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=0 untilOffset=36, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=0 taskId=2 partitionId=0
[2025-07-18T15:21:42.694+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=0 untilOffset=36, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=0 taskId=1 partitionId=0
[2025-07-18T15:21:42.725+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 9.256083 ms
[2025-07-18T15:21:42.758+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO CodeGenerator: Code generated in 26.661125 ms
[2025-07-18T15:21:42.774+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:21:42.776+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:21:42.776+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:21:42.776+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:21:42.777+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:21:42.777+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:21:42.777+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:21:42.777+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:21:42.777+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1
[2025-07-18T15:21:42.781+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:21:42.781+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:21:42.781+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:21:42.781+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:21:42.781+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:21:42.781+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:21:42.782+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:21:42.783+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:21:42.784+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:21:42.785+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:21:42.786+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:21:42.786+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:21:42.786+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:21:42.786+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:21:42.787+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:21:42.787+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:21:42.787+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:21:42.787+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:21:42.787+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:21:42.787+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:21:42.788+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:21:42.788+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:21:42.788+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:21:42.788+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:21:42.788+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:21:42.788+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:21:42.789+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:21:42.789+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:21:42.789+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:21:42.789+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:21:42.790+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:21:42.790+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:21:42.790+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:21:42.790+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:21:42.790+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:21:42.792+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:21:42.792+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:21:42.792+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:21:42.792+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:21:42.792+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:21:42.792+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:21:42.793+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:21:42.793+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:21:42.793+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:21:42.793+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:21:42.793+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:21:42.793+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:21:42.794+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:21:42.795+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2
[2025-07-18T15:21:42.795+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:21:42.797+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:21:42.797+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:21:42.797+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:21:42.797+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:21:42.797+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:21:42.797+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:21:42.798+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:21:42.799+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:21:42.801+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:21:42.802+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:21:42.802+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:21:42.802+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:21:42.802+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:21:42.802+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:21:42.802+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:21:42.802+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:21:42.803+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:21:42.804+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:21:42.804+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:21:42.805+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:21:42.805+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:21:42.805+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:21:42.805+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:21:42.805+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:21:42.805+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:21:42.805+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:21:42.806+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:21:42.807+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:21:42.807+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:21:42.807+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:21:42.807+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:21:42.808+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:21:42.810+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:21:42.810+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:21:42.811+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:21:42.812+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:21:42.813+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:21:42.813+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:21:42.816+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:21:42.817+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:21:42.818+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:21:42.819+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:21:42.820+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:21:42.820+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:21:42.820+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:21:42.820+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:21:42.820+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:21:42.820+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:21:42.821+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:21:42.822+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:21:42.822+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:21:42.822+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:21:42.822+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:21:42.822+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:21:42.823+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:21:42.823+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:21:42.824+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:21:42.824+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:21:42.824+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:21:42.824+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:21:42.825+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:21:42.825+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:21:42.825+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:21:42.825+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:21:42.825+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:21:42.826+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:21:42.826+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:21:42.826+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:21:42.826+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:21:42.826+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:21:42.828+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:21:42.829+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:21:42.829+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka startTimeMs: 1752852102819
[2025-07-18T15:21:42.829+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:21:42.830+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:21:42.830+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka startTimeMs: 1752852102819
[2025-07-18T15:21:42.833+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:21:42.833+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:21:42.833+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO AppInfoParser: Kafka startTimeMs: 1752852102819
[2025-07-18T15:21:42.834+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Assigned to partition(s): reservations-0
[2025-07-18T15:21:42.834+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Assigned to partition(s): checkins-0
[2025-07-18T15:21:42.834+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Assigned to partition(s): feedback-0
[2025-07-18T15:21:42.835+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to offset 0 for partition reservations-0
[2025-07-18T15:21:42.835+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to offset 0 for partition checkins-0
[2025-07-18T15:21:42.836+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to offset 0 for partition feedback-0
[2025-07-18T15:21:42.840+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:21:42.841+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:21:42.842+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:21:42.905+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:21:42.905+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:21:42.910+0000] {subprocess.py:93} INFO - 25/07/18 15:21:42 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:21:43.418+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:21:43.420+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:21:43.420+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:21:43.420+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:21:43.420+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:21:43.421+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:21:43.421+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:21:43.421+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:21:43.421+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:21:43.602+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T15:21:43.606+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T15:21:43.606+0000] {subprocess.py:93} INFO - 25/07/18 15:21:43 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T15:21:44.026+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T15:21:44.027+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T15:21:44.029+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor read 36 records through 1 polls (polled  out 36 records), taking 590268333 nanos, during time span of 1202782292 nanos.
[2025-07-18T15:21:44.029+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T15:21:44.029+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor read 36 records through 1 polls (polled  out 36 records), taking 590437375 nanos, during time span of 1203032251 nanos.
[2025-07-18T15:21:44.030+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor read 36 records through 1 polls (polled  out 36 records), taking 590474501 nanos, during time span of 1202036667 nanos.
[2025-07-18T15:21:44.064+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4795 bytes result sent to driver
[2025-07-18T15:21:44.065+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4925 bytes result sent to driver
[2025-07-18T15:21:44.065+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4877 bytes result sent to driver
[2025-07-18T15:21:44.081+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1800 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:21:44.082+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-18T15:21:44.083+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1791 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:21:44.084+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-18T15:21:44.084+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1784 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:21:44.084+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-18T15:21:44.091+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.880 s
[2025-07-18T15:21:44.091+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:21:44.091+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-18T15:21:44.092+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.821 s
[2025-07-18T15:21:44.092+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:21:44.093+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-18T15:21:44.093+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 1.802 s
[2025-07-18T15:21:44.093+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:21:44.093+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-18T15:21:44.094+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.902488 s
[2025-07-18T15:21:44.095+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.901282 s
[2025-07-18T15:21:44.097+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:21:44.097+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 1.898332 s
[2025-07-18T15:21:44.097+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:21:44.098+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committing epoch 0 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:21:44.098+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committing epoch 0 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:21:44.098+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:21:44.099+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committing epoch 0 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:21:44.130+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:21:44.131+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:21:44.131+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:21:44.343+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v2.metadata.json
[2025-07-18T15:21:44.344+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v2.metadata.json
[2025-07-18T15:21:44.358+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v2.metadata.json
[2025-07-18T15:21:44.377+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SnapshotProducer: Committed snapshot 1289787612813796791 (FastAppend)
[2025-07-18T15:21:44.386+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SnapshotProducer: Committed snapshot 9172509098898173112 (FastAppend)
[2025-07-18T15:21:44.396+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SnapshotProducer: Committed snapshot 894368021723052490 (FastAppend)
[2025-07-18T15:21:44.463+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=894368021723052490, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.321557125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=36}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=36}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=4467}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=4467}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:21:44.463+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=1289787612813796791, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.321527251S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=36}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=36}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=4872}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=4872}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:21:44.463+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committed in 330 ms
[2025-07-18T15:21:44.464+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=9172509098898173112, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.321569917S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=36}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=36}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3952}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=3952}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:21:44.464+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committed in 330 ms
[2025-07-18T15:21:44.465+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO SparkWrite: Committed in 331 ms
[2025-07-18T15:21:44.465+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:21:44.466+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:21:44.466+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:21:44.472+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/0 using temp file file:/tmp/checkpoints/reservations/commits/.0.6fb4bf83-c0be-4a90-80c0-8e5821038586.tmp
[2025-07-18T15:21:44.472+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/0 using temp file file:/tmp/checkpoints/feedback/commits/.0.5840782f-4c8f-4746-b644-f5cdd310ce3b.tmp
[2025-07-18T15:21:44.472+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/0 using temp file file:/tmp/checkpoints/checkins/commits/.0.fec23560-8ca5-466d-bce9-24c3be27c260.tmp
[2025-07-18T15:21:44.491+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.0.6fb4bf83-c0be-4a90-80c0-8e5821038586.tmp to file:/tmp/checkpoints/reservations/commits/0
[2025-07-18T15:21:44.492+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.0.5840782f-4c8f-4746-b644-f5cdd310ce3b.tmp to file:/tmp/checkpoints/feedback/commits/0
[2025-07-18T15:21:44.493+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.0.fec23560-8ca5-466d-bce9-24c3be27c260.tmp to file:/tmp/checkpoints/checkins/commits/0
[2025-07-18T15:21:44.508+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:21:44.509+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:21:44.509+0000] {subprocess.py:93} INFO -   "runId" : "2a9734b5-dec9-4b1e-a3f3-fa1e30f97415",
[2025-07-18T15:21:44.509+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:21:44.509+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:21:41.042Z",
[2025-07-18T15:21:44.510+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T15:21:44.510+0000] {subprocess.py:93} INFO -   "numInputRows" : 36,
[2025-07-18T15:21:44.510+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:21:44.511+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 10.434782608695652,
[2025-07-18T15:21:44.511+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:21:44.511+0000] {subprocess.py:93} INFO -     "addBatch" : 2719,
[2025-07-18T15:21:44.511+0000] {subprocess.py:93} INFO -     "commitOffsets" : 28,
[2025-07-18T15:21:44.512+0000] {subprocess.py:93} INFO -     "getBatch" : 14,
[2025-07-18T15:21:44.512+0000] {subprocess.py:93} INFO -     "latestOffset" : 331,
[2025-07-18T15:21:44.512+0000] {subprocess.py:93} INFO -     "queryPlanning" : 313,
[2025-07-18T15:21:44.512+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3450,
[2025-07-18T15:21:44.512+0000] {subprocess.py:93} INFO -     "walCommit" : 36
[2025-07-18T15:21:44.513+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:21:44.513+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:21:44.513+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:21:44.513+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:21:44.513+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T15:21:44.514+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:21:44.514+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:21:44.514+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:21:44.514+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:21:44.514+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:21:44.514+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:21:44.514+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -     "numInputRows" : 36,
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 10.434782608695652,
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:21:44.515+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -     "numOutputRows" : 36
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "runId" : "635bc90e-09bd-4ae9-a3d8-8039735dac8d",
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:21:40.922Z",
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "numInputRows" : 36,
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 10.084033613445378,
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -     "addBatch" : 2720,
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -     "commitOffsets" : 29,
[2025-07-18T15:21:44.516+0000] {subprocess.py:93} INFO -     "getBatch" : 11,
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "latestOffset" : 452,
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "queryPlanning" : 313,
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3570,
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "walCommit" : 39
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:21:44.517+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     "numInputRows" : 36,
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 10.084033613445378,
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -     "numOutputRows" : 36
[2025-07-18T15:21:44.518+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO - 25/07/18 15:21:44 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "runId" : "05e9c6f0-552d-440a-916d-062a56f88daa",
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:21:40.799Z",
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "numInputRows" : 36,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 9.753454348415064,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -     "addBatch" : 2719,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -     "commitOffsets" : 26,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -     "getBatch" : 14,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -     "latestOffset" : 562,
[2025-07-18T15:21:44.519+0000] {subprocess.py:93} INFO -     "queryPlanning" : 313,
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3691,
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -     "walCommit" : 36
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:21:44.520+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     "numInputRows" : 36,
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 9.753454348415064,
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -     "numOutputRows" : 36
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:21:44.521+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:21:54.503+0000] {subprocess.py:93} INFO - 25/07/18 15:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:21:54.506+0000] {subprocess.py:93} INFO - 25/07/18 15:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:21:54.506+0000] {subprocess.py:93} INFO - 25/07/18 15:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:21:56.376+0000] {subprocess.py:93} INFO - 25/07/18 15:21:56 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 77cb57a6bd53:43611 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:21:56.383+0000] {subprocess.py:93} INFO - 25/07/18 15:21:56 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 77cb57a6bd53:43611 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:21:56.385+0000] {subprocess.py:93} INFO - 25/07/18 15:21:56 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 77cb57a6bd53:43611 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:21:56.388+0000] {subprocess.py:93} INFO - 25/07/18 15:21:56 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 77cb57a6bd53:43611 in memory (size: 29.3 KiB, free: 434.3 MiB)
[2025-07-18T15:21:56.390+0000] {subprocess.py:93} INFO - 25/07/18 15:21:56 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 77cb57a6bd53:43611 in memory (size: 29.4 KiB, free: 434.4 MiB)
[2025-07-18T15:21:56.393+0000] {subprocess.py:93} INFO - 25/07/18 15:21:56 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 77cb57a6bd53:43611 in memory (size: 29.3 KiB, free: 434.4 MiB)
[2025-07-18T15:22:02.554+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/1 using temp file file:/tmp/checkpoints/reservations/offsets/.1.8e3734f0-e85f-45e5-a2ee-27f1f9635396.tmp
[2025-07-18T15:22:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.1.8e3734f0-e85f-45e5-a2ee-27f1f9635396.tmp to file:/tmp/checkpoints/reservations/offsets/1
[2025-07-18T15:22:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752852122549,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:22:02.579+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.579+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.579+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.582+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.583+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.590+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.590+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.590+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.593+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.594+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.610+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.610+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.610+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.613+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.614+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.625+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T15:22:02.633+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T15:22:02.634+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 77cb57a6bd53:43611 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:22:02.634+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkContext: Created broadcast 6 from start at <unknown>:0
[2025-07-18T15:22:02.636+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:22:02.637+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:22:02.637+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:22:02.638+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Final stage: ResultStage 3 (start at <unknown>:0)
[2025-07-18T15:22:02.638+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:22:02.639+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:22:02.639+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:22:02.641+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:22:02.643+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T15:22:02.645+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 77cb57a6bd53:43611 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:22:02.649+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:22:02.649+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:22:02.650+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-07-18T15:22:02.651+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:22:02.651+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-07-18T15:22:02.665+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:22:02.668+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=36 untilOffset=37, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=1 taskId=3 partitionId=0
[2025-07-18T15:22:02.671+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to offset 36 for partition reservations-0
[2025-07-18T15:22:02.676+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:22:02.744+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:02.745+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:22:02.745+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=38, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:02.746+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T15:22:02.763+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T15:22:02.764+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 74409958 nanos, during time span of 92617625 nanos.
[2025-07-18T15:22:02.770+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4782 bytes result sent to driver
[2025-07-18T15:22:02.775+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 125 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:22:02.776+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-07-18T15:22:02.776+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: ResultStage 3 (start at <unknown>:0) finished in 0.135 s
[2025-07-18T15:22:02.776+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:22:02.776+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-07-18T15:22:02.776+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.142121 s
[2025-07-18T15:22:02.776+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:22:02.777+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Committing epoch 1 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:22:02.811+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.894+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v3.metadata.json
[2025-07-18T15:22:02.921+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SnapshotProducer: Committed snapshot 4152877608899107949 (FastAppend)
[2025-07-18T15:22:02.945+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=4152877608899107949, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.140855208S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=37}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2990}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=6942}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:22:02.946+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Committed in 141 ms
[2025-07-18T15:22:02.946+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:22:02.949+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/1 using temp file file:/tmp/checkpoints/reservations/commits/.1.220eda24-cdad-41dd-817d-4fcddf16d62b.tmp
[2025-07-18T15:22:02.961+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.1.220eda24-cdad-41dd-817d-4fcddf16d62b.tmp to file:/tmp/checkpoints/reservations/commits/1
[2025-07-18T15:22:02.963+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:22:02.963+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:22:02.963+0000] {subprocess.py:93} INFO -   "runId" : "05e9c6f0-552d-440a-916d-062a56f88daa",
[2025-07-18T15:22:02.963+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:22:02.963+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:22:02.548Z",
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.4213075060532687,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -     "addBatch" : 358,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -     "commitOffsets" : 18,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -     "queryPlanning" : 17,
[2025-07-18T15:22:02.964+0000] {subprocess.py:93} INFO -     "triggerExecution" : 413,
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -     "walCommit" : 17
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:02.965+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:02.966+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.4213075060532687,
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:22:02.967+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:22:02.968+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:22:02.968+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:22:02.968+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:22:02.968+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:22:02.969+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/2 using temp file file:/tmp/checkpoints/reservations/offsets/.2.438c0850-92ba-4a7a-a9c0-7cc0e8cc4a1d.tmp
[2025-07-18T15:22:02.979+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.2.438c0850-92ba-4a7a-a9c0-7cc0e8cc4a1d.tmp to file:/tmp/checkpoints/reservations/offsets/2
[2025-07-18T15:22:02.979+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752852122964,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:22:02.985+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.986+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.987+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.987+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.988+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.993+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.993+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.993+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:02.996+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:02.996+0000] {subprocess.py:93} INFO - 25/07/18 15:22:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.003+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:03.004+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:03.004+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:03.006+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.007+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.016+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:22:03.021+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.022+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 77cb57a6bd53:43611 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.022+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 8 from start at <unknown>:0
[2025-07-18T15:22:03.022+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:22:03.022+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:22:03.023+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:22:03.023+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
[2025-07-18T15:22:03.025+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:22:03.026+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:22:03.026+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:22:03.026+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 77cb57a6bd53:43611 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.027+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:22:03.027+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 77cb57a6bd53:43611 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 77cb57a6bd53:43611 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:22:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:22:03.030+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:22:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-18T15:22:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:22:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-07-18T15:22:03.037+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:22:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=37 untilOffset=39, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=2 taskId=4 partitionId=0
[2025-07-18T15:22:03.048+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to offset 37 for partition reservations-0
[2025-07-18T15:22:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:22:03.058+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.059+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:22:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to offset 38 for partition reservations-0
[2025-07-18T15:22:03.061+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:22:03.163+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/1 using temp file file:/tmp/checkpoints/checkins/offsets/.1.7afd3c67-cd25-42f9-a134-13b805b581b4.tmp
[2025-07-18T15:22:03.175+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.1.7afd3c67-cd25-42f9-a134-13b805b581b4.tmp to file:/tmp/checkpoints/checkins/offsets/1
[2025-07-18T15:22:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752852123159,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:22:03.182+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.182+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.182+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.185+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.185+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.193+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.193+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.203+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.214+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:22:03.218+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.221+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 77cb57a6bd53:43611 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.222+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 10 from start at <unknown>:0
[2025-07-18T15:22:03.222+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:22:03.223+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:22:03.223+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:22:03.224+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Final stage: ResultStage 5 (start at <unknown>:0)
[2025-07-18T15:22:03.224+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:22:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:22:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:22:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 77cb57a6bd53:43611 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:22:03.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:22:03.230+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-07-18T15:22:03.230+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:22:03.231+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-07-18T15:22:03.242+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:22:03.243+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=36 untilOffset=37, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=1 taskId=5 partitionId=0
[2025-07-18T15:22:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to offset 36 for partition checkins-0
[2025-07-18T15:22:03.249+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:22:03.350+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.350+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:22:03.350+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=38, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.352+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T15:22:03.365+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T15:22:03.366+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 103923458 nanos, during time span of 118902959 nanos.
[2025-07-18T15:22:03.372+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4913 bytes result sent to driver
[2025-07-18T15:22:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 143 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:22:03.374+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-07-18T15:22:03.375+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: ResultStage 5 (start at <unknown>:0) finished in 0.151 s
[2025-07-18T15:22:03.375+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:22:03.376+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-07-18T15:22:03.376+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.153670 s
[2025-07-18T15:22:03.377+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:22:03.377+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committing epoch 1 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:22:03.390+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v3.metadata.json
[2025-07-18T15:22:03.511+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SnapshotProducer: Committed snapshot 2351040969931048312 (FastAppend)
[2025-07-18T15:22:03.535+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2351040969931048312, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.143989958S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=37}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2916}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=7383}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:22:03.535+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committed in 144 ms
[2025-07-18T15:22:03.535+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:22:03.539+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/1 using temp file file:/tmp/checkpoints/checkins/commits/.1.250d6aae-e2ec-4237-a60b-30b367596565.tmp
[2025-07-18T15:22:03.556+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.1.250d6aae-e2ec-4237-a60b-30b367596565.tmp to file:/tmp/checkpoints/checkins/commits/1
[2025-07-18T15:22:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:22:03.557+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:22:03.557+0000] {subprocess.py:93} INFO -   "runId" : "635bc90e-09bd-4ae9-a3d8-8039735dac8d",
[2025-07-18T15:22:03.557+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:22:03.557+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:22:03.158Z",
[2025-07-18T15:22:03.558+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T15:22:03.558+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:22:03.558+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:22:03.558+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.5188916876574305,
[2025-07-18T15:22:03.558+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:22:03.559+0000] {subprocess.py:93} INFO -     "addBatch" : 347,
[2025-07-18T15:22:03.559+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:22:03.559+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:22:03.559+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:22:03.559+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T15:22:03.559+0000] {subprocess.py:93} INFO -     "triggerExecution" : 397,
[2025-07-18T15:22:03.560+0000] {subprocess.py:93} INFO -     "walCommit" : 15
[2025-07-18T15:22:03.560+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:22:03.560+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:22:03.560+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:22:03.560+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:22:03.561+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:22:03.561+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:22:03.561+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:22:03.561+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:03.561+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:03.562+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:22:03.562+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:22:03.562+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:03.562+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:03.562+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:03.562+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:22:03.563+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:22:03.563+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:03.563+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:03.564+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:03.564+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:22:03.564+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:22:03.564+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.5188916876574305,
[2025-07-18T15:22:03.564+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:22:03.564+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:22:03.564+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:22:03.565+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:22:03.565+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:22:03.565+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:22:03.566+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.567+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/2 using temp file file:/tmp/checkpoints/checkins/offsets/.2.3bb8630b-ae3f-4d9a-a6e6-bdf98a99a10d.tmp
[2025-07-18T15:22:03.567+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T15:22:03.584+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T15:22:03.586+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 522515791 nanos, during time span of 543525959 nanos.
[2025-07-18T15:22:03.590+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4773 bytes result sent to driver
[2025-07-18T15:22:03.593+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.2.3bb8630b-ae3f-4d9a-a6e6-bdf98a99a10d.tmp to file:/tmp/checkpoints/checkins/offsets/2
[2025-07-18T15:22:03.594+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752852123558,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:22:03.595+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 77cb57a6bd53:43611 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.595+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 561 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:22:03.595+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-18T15:22:03.596+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.571 s
[2025-07-18T15:22:03.596+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:22:03.596+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-18T15:22:03.597+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.572893 s
[2025-07-18T15:22:03.597+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:22:03.597+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committing epoch 2 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:22:03.608+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 77cb57a6bd53:43611 in memory (size: 12.3 KiB, free: 434.4 MiB)
[2025-07-18T15:22:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.612+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.616+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:22:03.634+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.635+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.635+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.641+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.646+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.647+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.647+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:03.648+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.649+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.659+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:22:03.664+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.667+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 77cb57a6bd53:43611 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.668+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 12 from start at <unknown>:0
[2025-07-18T15:22:03.669+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:22:03.670+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:22:03.670+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 77cb57a6bd53:43611 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.671+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:22:03.671+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
[2025-07-18T15:22:03.671+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:22:03.672+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:22:03.672+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:22:03.672+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 28.0 KiB, free 434.3 MiB)
[2025-07-18T15:22:03.672+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.672+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 77cb57a6bd53:43611 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.673+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:22:03.674+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:22:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-18T15:22:03.676+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:22:03.676+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-07-18T15:22:03.677+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:22:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=37 untilOffset=39, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=2 taskId=6 partitionId=0
[2025-07-18T15:22:03.681+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to offset 37 for partition checkins-0
[2025-07-18T15:22:03.683+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:22:03.685+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.686+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:22:03.686+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.688+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to offset 38 for partition checkins-0
[2025-07-18T15:22:03.688+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:22:03.737+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v4.metadata.json
[2025-07-18T15:22:03.771+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SnapshotProducer: Committed snapshot 1020184874958535889 (FastAppend)
[2025-07-18T15:22:03.772+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/1 using temp file file:/tmp/checkpoints/feedback/offsets/.1.3723d945-3a01-410e-a72d-bf27074f498c.tmp
[2025-07-18T15:22:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.1.3723d945-3a01-410e-a72d-bf27074f498c.tmp to file:/tmp/checkpoints/feedback/offsets/1
[2025-07-18T15:22:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752852123765,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:22:03.794+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.794+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.794+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.796+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=1020184874958535889, sequenceNumber=3, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.178179458S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=3}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=39}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2994}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=9936}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:22:03.796+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committed in 178 ms
[2025-07-18T15:22:03.796+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:22:03.797+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.799+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/2 using temp file file:/tmp/checkpoints/reservations/commits/.2.6538f01c-84cd-4d71-b387-c4b411dc4325.tmp
[2025-07-18T15:22:03.801+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.805+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.805+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.805+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.809+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.809+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.814+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.2.6538f01c-84cd-4d71-b387-c4b411dc4325.tmp to file:/tmp/checkpoints/reservations/commits/2
[2025-07-18T15:22:03.816+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:22:03.817+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:22:03.818+0000] {subprocess.py:93} INFO -   "runId" : "05e9c6f0-552d-440a-916d-062a56f88daa",
[2025-07-18T15:22:03.818+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:22:03.818+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:22:02.962Z",
[2025-07-18T15:22:03.818+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T15:22:03.819+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:22:03.819+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.830917874396135,
[2025-07-18T15:22:03.819+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.3501762632197414,
[2025-07-18T15:22:03.819+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:22:03.819+0000] {subprocess.py:93} INFO -     "addBatch" : 805,
[2025-07-18T15:22:03.820+0000] {subprocess.py:93} INFO -     "commitOffsets" : 19,
[2025-07-18T15:22:03.820+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:22:03.820+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:22:03.820+0000] {subprocess.py:93} INFO -     "queryPlanning" : 11,
[2025-07-18T15:22:03.820+0000] {subprocess.py:93} INFO -     "triggerExecution" : 851,
[2025-07-18T15:22:03.820+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T15:22:03.820+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:22:03.821+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:22:03.821+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:22:03.821+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:22:03.821+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:22:03.821+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:22:03.822+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:03.822+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:03.822+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:03.822+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:22:03.823+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:22:03.823+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:03.824+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.830917874396135,
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.3501762632197414,
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:22:03.825+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:22:03.826+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:22:03.826+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:22:03.826+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:22:03.826+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:22:03.827+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:22:03.827+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.827+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.828+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:03.828+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.829+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:03.829+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.837+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.837+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 77cb57a6bd53:43611 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.837+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 14 from start at <unknown>:0
[2025-07-18T15:22:03.838+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:22:03.840+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 77cb57a6bd53:43611 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.840+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 77cb57a6bd53:43611 (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:22:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:22:03.842+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:22:03.842+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-18T15:22:03.842+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:22:03.842+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-07-18T15:22:03.853+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:22:03.855+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=36 untilOffset=37, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=1 taskId=7 partitionId=0
[2025-07-18T15:22:03.866+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to offset 36 for partition feedback-0
[2025-07-18T15:22:03.869+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:22:03.960+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.960+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:22:03.961+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=38, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:03.962+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T15:22:03.973+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T15:22:03.974+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 102301917 nanos, during time span of 115139042 nanos.
[2025-07-18T15:22:03.978+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4856 bytes result sent to driver
[2025-07-18T15:22:03.979+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 140 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:22:03.979+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-18T15:22:03.980+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.144 s
[2025-07-18T15:22:03.980+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:22:03.980+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-18T15:22:03.981+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 0.145382 s
[2025-07-18T15:22:03.981+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:22:03.981+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committing epoch 1 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:22:03.991+0000] {subprocess.py:93} INFO - 25/07/18 15:22:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.058+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v3.metadata.json
[2025-07-18T15:22:04.078+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SnapshotProducer: Committed snapshot 4452184769310153985 (FastAppend)
[2025-07-18T15:22:04.092+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=4452184769310153985, sequenceNumber=2, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.101197708S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=2}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=37}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2935}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=7807}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:22:04.092+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Committed in 101 ms
[2025-07-18T15:22:04.093+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:22:04.095+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/1 using temp file file:/tmp/checkpoints/feedback/commits/.1.d8ccd7d8-15c1-4f67-9d42-a5a4e80bf527.tmp
[2025-07-18T15:22:04.105+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.1.d8ccd7d8-15c1-4f67-9d42-a5a4e80bf527.tmp to file:/tmp/checkpoints/feedback/commits/1
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "runId" : "2a9734b5-dec9-4b1e-a3f3-fa1e30f97415",
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:22:03.764Z",
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.941176470588235,
[2025-07-18T15:22:04.106+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "addBatch" : 290,
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "commitOffsets" : 13,
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "queryPlanning" : 13,
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "triggerExecution" : 340,
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "walCommit" : 21
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:22:04.107+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -         "0" : 36
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:04.108+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.941176470588235,
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:22:04.109+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:22:04.110+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:22:04.110+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:22:04.110+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:22:04.110+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:22:04.110+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:22:04.110+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/2 using temp file file:/tmp/checkpoints/feedback/offsets/.2.0ae80462-239c-4ded-9044-c593c5b334cb.tmp
[2025-07-18T15:22:04.120+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.2.0ae80462-239c-4ded-9044-c593c5b334cb.tmp to file:/tmp/checkpoints/feedback/offsets/2
[2025-07-18T15:22:04.120+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752852124107,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:22:04.126+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.126+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.128+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.130+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.144+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.145+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.145+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.146+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.151+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.151+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.152+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.152+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.154+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.162+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:22:04.168+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:22:04.169+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 77cb57a6bd53:43611 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.169+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 77cb57a6bd53:43611 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.170+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkContext: Created broadcast 16 from start at <unknown>:0
[2025-07-18T15:22:04.171+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:22:04.172+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:22:04.172+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:22:04.173+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
[2025-07-18T15:22:04.173+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:22:04.174+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:22:04.177+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:22:04.177+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 77cb57a6bd53:43611 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.177+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T15:22:04.177+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2025-07-18T15:22:04.178+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 77cb57a6bd53:43611 (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.178+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:22:04.178+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:22:04.178+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-18T15:22:04.183+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:22:04.183+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-07-18T15:22:04.186+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:22:04.186+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=37 untilOffset=38, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=2 taskId=8 partitionId=0
[2025-07-18T15:22:04.188+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to offset 37 for partition feedback-0
[2025-07-18T15:22:04.189+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:22:04.199+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:04.201+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:22:04.201+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:04.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:22:04.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:04.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:04.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T15:22:04.202+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T15:22:04.219+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T15:22:04.219+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 9147917 nanos, during time span of 30823917 nanos.
[2025-07-18T15:22:04.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4847 bytes result sent to driver
[2025-07-18T15:22:04.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T15:22:04.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor read 2 records through 2 polls (polled  out 2 records), taking 514655126 nanos, during time span of 543931751 nanos.
[2025-07-18T15:22:04.227+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4869 bytes result sent to driver
[2025-07-18T15:22:04.227+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 47 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:22:04.227+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-18T15:22:04.228+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 558 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:22:04.230+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-18T15:22:04.231+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.059 s
[2025-07-18T15:22:04.231+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:22:04.231+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-18T15:22:04.232+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.563 s
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.060584 s
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Committing epoch 2 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.565270 s
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Committing epoch 2 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:22:04.247+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.254+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:22:04.339+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v4.metadata.json
[2025-07-18T15:22:04.339+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v4.metadata.json
[2025-07-18T15:22:04.358+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SnapshotProducer: Committed snapshot 170348445761229176 (FastAppend)
[2025-07-18T15:22:04.380+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=170348445761229176, sequenceNumber=3, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.126892S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=3}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=39}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2927}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=10310}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:22:04.381+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Committed in 127 ms
[2025-07-18T15:22:04.381+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:22:04.386+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/2 using temp file file:/tmp/checkpoints/checkins/commits/.2.78108a54-a0f4-49f4-907a-a91f41824a6b.tmp
[2025-07-18T15:22:04.405+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.2.78108a54-a0f4-49f4-907a-a91f41824a6b.tmp to file:/tmp/checkpoints/checkins/commits/2
[2025-07-18T15:22:04.405+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:22:04.405+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:22:04.406+0000] {subprocess.py:93} INFO -   "runId" : "635bc90e-09bd-4ae9-a3d8-8039735dac8d",
[2025-07-18T15:22:04.407+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:22:04.407+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:22:03.556Z",
[2025-07-18T15:22:04.407+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T15:22:04.408+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:22:04.408+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 5.025125628140703,
[2025-07-18T15:22:04.408+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.366863905325444,
[2025-07-18T15:22:04.408+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:22:04.408+0000] {subprocess.py:93} INFO -     "addBatch" : 762,
[2025-07-18T15:22:04.409+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:22:04.409+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:22:04.410+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:22:04.410+0000] {subprocess.py:93} INFO -     "queryPlanning" : 26,
[2025-07-18T15:22:04.411+0000] {subprocess.py:93} INFO -     "triggerExecution" : 845,
[2025-07-18T15:22:04.411+0000] {subprocess.py:93} INFO -     "walCommit" : 32
[2025-07-18T15:22:04.412+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:22:04.415+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:22:04.418+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:22:04.418+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:22:04.418+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:22:04.418+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:22:04.418+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:04.418+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.419+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.419+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:22:04.419+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:22:04.419+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:22:04.419+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.421+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.421+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:22:04.421+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:22:04.422+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:22:04.422+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.422+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.422+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:22:04.422+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 5.025125628140703,
[2025-07-18T15:22:04.422+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.366863905325444,
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:22:04.423+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:22:04.424+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SnapshotProducer: Committed snapshot 6587016825006995578 (FastAppend)
[2025-07-18T15:22:04.429+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=6587016825006995578, sequenceNumber=3, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.181695334S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=3}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=38}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2863}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=10670}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:22:04.430+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Committed in 182 ms
[2025-07-18T15:22:04.430+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:22:04.434+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/2 using temp file file:/tmp/checkpoints/feedback/commits/.2.8cc351a9-0d13-4e17-80e9-82570266cc5d.tmp
[2025-07-18T15:22:04.446+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.2.8cc351a9-0d13-4e17-80e9-82570266cc5d.tmp to file:/tmp/checkpoints/feedback/commits/2
[2025-07-18T15:22:04.446+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "runId" : "2a9734b5-dec9-4b1e-a3f3-fa1e30f97415",
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:22:04.105Z",
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.9325513196480935,
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.941176470588235,
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -     "addBatch" : 299,
[2025-07-18T15:22:04.447+0000] {subprocess.py:93} INFO -     "commitOffsets" : 17,
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -     "queryPlanning" : 9,
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -     "triggerExecution" : 340,
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -     "walCommit" : 12
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:04.448+0000] {subprocess.py:93} INFO -         "0" : 37
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -         "0" : 38
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -         "0" : 38
[2025-07-18T15:22:04.449+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.9325513196480935,
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.941176470588235,
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:22:04.450+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:22:04.451+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:22:04.451+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:22:04.451+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:22:04.451+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:22:04.451+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/3 using temp file file:/tmp/checkpoints/feedback/offsets/.3.165bef41-2fe0-481a-a560-ee4687cdf693.tmp
[2025-07-18T15:22:04.464+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.3.165bef41-2fe0-481a-a560-ee4687cdf693.tmp to file:/tmp/checkpoints/feedback/offsets/3
[2025-07-18T15:22:04.464+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752852124447,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:22:04.469+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.469+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.469+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.469+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.470+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.474+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.474+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.475+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.476+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.476+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.482+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.482+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.482+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:04.483+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.484+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:22:04.492+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:22:04.503+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:22:04.503+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 77cb57a6bd53:43611 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.503+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 77cb57a6bd53:43611 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.503+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkContext: Created broadcast 18 from start at <unknown>:0
[2025-07-18T15:22:04.503+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:22:04.503+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:22:04.503+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:22:04.504+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Final stage: ResultStage 9 (start at <unknown>:0)
[2025-07-18T15:22:04.504+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:22:04.504+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:22:04.504+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 77cb57a6bd53:43611 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.506+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 77cb57a6bd53:43611 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:22:04.508+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:22:04.512+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 27.5 KiB, free 434.3 MiB)
[2025-07-18T15:22:04.513+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 77cb57a6bd53:43611 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T15:22:04.513+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.3 MiB)
[2025-07-18T15:22:04.513+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 77cb57a6bd53:43611 (size: 12.1 KiB, free: 434.4 MiB)
[2025-07-18T15:22:04.513+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:22:04.514+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:22:04.514+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-18T15:22:04.516+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:22:04.517+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2025-07-18T15:22:04.526+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:22:04.527+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=38 untilOffset=39, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=3 taskId=9 partitionId=0
[2025-07-18T15:22:04.529+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to offset 38 for partition feedback-0
[2025-07-18T15:22:04.529+0000] {subprocess.py:93} INFO - 25/07/18 15:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:22:05.031+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:05.031+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:22:05.032+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=39, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:22:05.034+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T15:22:05.044+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T15:22:05.045+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 503237708 nanos, during time span of 516182542 nanos.
[2025-07-18T15:22:05.053+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4852 bytes result sent to driver
[2025-07-18T15:22:05.055+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 535 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:22:05.055+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-18T15:22:05.056+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO DAGScheduler: ResultStage 9 (start at <unknown>:0) finished in 0.542 s
[2025-07-18T15:22:05.056+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:22:05.058+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-18T15:22:05.058+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.552918 s
[2025-07-18T15:22:05.058+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:22:05.059+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO SparkWrite: Committing epoch 3 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:22:05.068+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:22:05.170+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v5.metadata.json
[2025-07-18T15:22:05.199+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO SnapshotProducer: Committed snapshot 6915571666506501082 (FastAppend)
[2025-07-18T15:22:05.224+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=6915571666506501082, sequenceNumber=4, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.154490792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=4}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=39}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2755}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=13425}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:22:05.225+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO SparkWrite: Committed in 155 ms
[2025-07-18T15:22:05.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:22:05.226+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/3 using temp file file:/tmp/checkpoints/feedback/commits/.3.68835369-fe2b-4c8c-b946-3e615a7aabbd.tmp
[2025-07-18T15:22:05.258+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.3.68835369-fe2b-4c8c-b946-3e615a7aabbd.tmp to file:/tmp/checkpoints/feedback/commits/3
[2025-07-18T15:22:05.258+0000] {subprocess.py:93} INFO - 25/07/18 15:22:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:22:05.259+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:22:05.259+0000] {subprocess.py:93} INFO -   "runId" : "2a9734b5-dec9-4b1e-a3f3-fa1e30f97415",
[2025-07-18T15:22:05.259+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:22:05.259+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:22:04.446Z",
[2025-07-18T15:22:05.259+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T15:22:05.259+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.9325513196480935,
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.2391573729863692,
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -     "addBatch" : 749,
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -     "commitOffsets" : 34,
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:22:05.260+0000] {subprocess.py:93} INFO -     "queryPlanning" : 9,
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -     "triggerExecution" : 807,
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -         "0" : 38
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:05.261+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.9325513196480935,
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.2391573729863692,
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:22:05.262+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:22:05.263+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:22:05.263+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:22:05.263+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:22:05.263+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:22:09.426+0000] {subprocess.py:93} INFO - 25/07/18 15:22:09 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 77cb57a6bd53:43611 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:22:09.435+0000] {subprocess.py:93} INFO - 25/07/18 15:22:09 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 77cb57a6bd53:43611 in memory (size: 12.1 KiB, free: 434.4 MiB)
[2025-07-18T15:22:13.832+0000] {subprocess.py:93} INFO - 25/07/18 15:22:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:14.408+0000] {subprocess.py:93} INFO - 25/07/18 15:22:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:15.258+0000] {subprocess.py:93} INFO - 25/07/18 15:22:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:23.828+0000] {subprocess.py:93} INFO - 25/07/18 15:22:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:24.410+0000] {subprocess.py:93} INFO - 25/07/18 15:22:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:25.262+0000] {subprocess.py:93} INFO - 25/07/18 15:22:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:33.839+0000] {subprocess.py:93} INFO - 25/07/18 15:22:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:34.411+0000] {subprocess.py:93} INFO - 25/07/18 15:22:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:35.271+0000] {subprocess.py:93} INFO - 25/07/18 15:22:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:43.842+0000] {subprocess.py:93} INFO - 25/07/18 15:22:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:44.417+0000] {subprocess.py:93} INFO - 25/07/18 15:22:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:45.272+0000] {subprocess.py:93} INFO - 25/07/18 15:22:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:53.851+0000] {subprocess.py:93} INFO - 25/07/18 15:22:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:54.423+0000] {subprocess.py:93} INFO - 25/07/18 15:22:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:22:55.278+0000] {subprocess.py:93} INFO - 25/07/18 15:22:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:03.860+0000] {subprocess.py:93} INFO - 25/07/18 15:23:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:04.426+0000] {subprocess.py:93} INFO - 25/07/18 15:23:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:05.288+0000] {subprocess.py:93} INFO - 25/07/18 15:23:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:13.855+0000] {subprocess.py:93} INFO - 25/07/18 15:23:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:14.440+0000] {subprocess.py:93} INFO - 25/07/18 15:23:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:15.287+0000] {subprocess.py:93} INFO - 25/07/18 15:23:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:23.860+0000] {subprocess.py:93} INFO - 25/07/18 15:23:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:24.447+0000] {subprocess.py:93} INFO - 25/07/18 15:23:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:25.288+0000] {subprocess.py:93} INFO - 25/07/18 15:23:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:33.862+0000] {subprocess.py:93} INFO - 25/07/18 15:23:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:34.446+0000] {subprocess.py:93} INFO - 25/07/18 15:23:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:35.290+0000] {subprocess.py:93} INFO - 25/07/18 15:23:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:43.862+0000] {subprocess.py:93} INFO - 25/07/18 15:23:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:44.461+0000] {subprocess.py:93} INFO - 25/07/18 15:23:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:45.298+0000] {subprocess.py:93} INFO - 25/07/18 15:23:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:53.863+0000] {subprocess.py:93} INFO - 25/07/18 15:23:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:54.461+0000] {subprocess.py:93} INFO - 25/07/18 15:23:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:23:55.305+0000] {subprocess.py:93} INFO - 25/07/18 15:23:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:02.730+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/3 using temp file file:/tmp/checkpoints/reservations/offsets/.3.1bbd1e79-8128-4b5f-aaeb-755b17007a3e.tmp
[2025-07-18T15:24:02.758+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.3.1bbd1e79-8128-4b5f-aaeb-755b17007a3e.tmp to file:/tmp/checkpoints/reservations/offsets/3
[2025-07-18T15:24:02.758+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752852242717,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:24:02.779+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.779+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.780+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.780+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:02.784+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:02.792+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.792+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.793+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.799+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:02.800+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:02.818+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.819+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.819+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:02.820+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:02.842+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:02.862+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T15:24:02.870+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.3 MiB)
[2025-07-18T15:24:02.874+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 77cb57a6bd53:43611 (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T15:24:02.878+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkContext: Created broadcast 20 from start at <unknown>:0
[2025-07-18T15:24:02.879+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:24:02.884+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:24:02.888+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:24:02.891+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
[2025-07-18T15:24:02.893+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:24:02.920+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:24:02.922+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:24:02.925+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:24:02.925+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T15:24:02.925+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 77cb57a6bd53:43611 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:24:02.926+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:24:02.926+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:24:02.926+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-18T15:24:02.927+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:24:02.927+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2025-07-18T15:24:02.963+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:24:02.970+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=39 untilOffset=40, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=3 taskId=10 partitionId=0
[2025-07-18T15:24:02.974+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to offset 39 for partition reservations-0
[2025-07-18T15:24:02.989+0000] {subprocess.py:93} INFO - 25/07/18 15:24:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:24:03.125+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:03.129+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:24:03.130+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:03.131+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T15:24:03.172+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T15:24:03.173+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor read 1 records through 1 polls (polled  out 2 records), taking 153527084 nanos, during time span of 194821833 nanos.
[2025-07-18T15:24:03.174+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 4740 bytes result sent to driver
[2025-07-18T15:24:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 259 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:24:03.178+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-18T15:24:03.179+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.297 s
[2025-07-18T15:24:03.182+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:24:03.182+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-18T15:24:03.185+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.316974 s
[2025-07-18T15:24:03.186+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:24:03.186+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Committing epoch 3 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:24:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.353+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/3 using temp file file:/tmp/checkpoints/checkins/offsets/.3.d92bc6d8-2ce8-48bf-8892-bd37a42be6be.tmp
[2025-07-18T15:24:03.410+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 WARN CheckpointFileManager: Failed to rename temp file file:/tmp/checkpoints/checkins/offsets/.3.d92bc6d8-2ce8-48bf-8892-bd37a42be6be.tmp to file:/tmp/checkpoints/checkins/offsets/3 because file exists
[2025-07-18T15:24:03.411+0000] {subprocess.py:93} INFO - org.apache.hadoop.fs.FileAlreadyExistsException: rename destination file:/tmp/checkpoints/checkins/offsets/3 already exists.
[2025-07-18T15:24:03.411+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1600)
[2025-07-18T15:24:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)
[2025-07-18T15:24:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)
[2025-07-18T15:24:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:24:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)
[2025-07-18T15:24:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:24:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)
[2025-07-18T15:24:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)
[2025-07-18T15:24:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)
[2025-07-18T15:24:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:204)
[2025-07-18T15:24:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237)
[2025-07-18T15:24:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130)
[2025-07-18T15:24:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:764)
[2025-07-18T15:24:03.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:536)
[2025-07-18T15:24:03.414+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:24:03.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:24:03.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:24:03.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:535)
[2025-07-18T15:24:03.419+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T15:24:03.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T15:24:03.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T15:24:03.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T15:24:03.419+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:24:03.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:24:03.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:24:03.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T15:24:03.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T15:24:03.422+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T15:24:03.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T15:24:03.425+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:24:03.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T15:24:03.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T15:24:03.427+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T15:24:03.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T15:24:03.430+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v5.metadata.json
[2025-07-18T15:24:03.430+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 ERROR MicroBatchExecution: Query [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 635bc90e-09bd-4ae9-a3d8-8039735dac8d] terminated with error
[2025-07-18T15:24:03.431+0000] {subprocess.py:93} INFO - org.apache.spark.SparkConcurrentModificationException: Multiple streaming queries are concurrently using file:/tmp/checkpoints/checkins/offsets.
[2025-07-18T15:24:03.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.multiStreamingQueriesUsingPathConcurrentlyError(QueryExecutionErrors.scala:1858)
[2025-07-18T15:24:03.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:210)
[2025-07-18T15:24:03.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237)
[2025-07-18T15:24:03.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130)
[2025-07-18T15:24:03.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:764)
[2025-07-18T15:24:03.438+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:536)
[2025-07-18T15:24:03.441+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:24:03.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:24:03.446+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:24:03.446+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:535)
[2025-07-18T15:24:03.447+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T15:24:03.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T15:24:03.450+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T15:24:03.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T15:24:03.453+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:24:03.458+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:24:03.459+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:24:03.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T15:24:03.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T15:24:03.462+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T15:24:03.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T15:24:03.467+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:24:03.469+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T15:24:03.469+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T15:24:03.470+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T15:24:03.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T15:24:03.471+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: rename destination file:/tmp/checkpoints/checkins/offsets/3 already exists.
[2025-07-18T15:24:03.472+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1600)
[2025-07-18T15:24:03.473+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)
[2025-07-18T15:24:03.473+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)
[2025-07-18T15:24:03.474+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:24:03.475+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)
[2025-07-18T15:24:03.475+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:24:03.476+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)
[2025-07-18T15:24:03.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)
[2025-07-18T15:24:03.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)
[2025-07-18T15:24:03.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:204)
[2025-07-18T15:24:03.480+0000] {subprocess.py:93} INFO - 	... 28 more
[2025-07-18T15:24:03.480+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-07-18T15:24:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:24:03.482+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:24:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:24:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MicroBatchExecution: Async log purge executor pool for query [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 635bc90e-09bd-4ae9-a3d8-8039735dac8d] has been shutdown
[2025-07-18T15:24:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SnapshotProducer: Committed snapshot 6762414654457540199 (FastAppend)
[2025-07-18T15:24:03.572+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6762414654457540199, sequenceNumber=4, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.3475175S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=4}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=40}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2969}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=12905}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852098472, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:24:03.573+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Committed in 348 ms
[2025-07-18T15:24:03.573+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:24:03.592+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/3 using temp file file:/tmp/checkpoints/reservations/commits/.3.733307f7-fefa-4823-93d4-79de52bb76f9.tmp
[2025-07-18T15:24:03.640+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-07-18T15:24:03.641+0000] {subprocess.py:93} INFO -   File "/home/iceberg/spark/stream_to_bronze.py", line 94, in <module>
[2025-07-18T15:24:03.645+0000] {subprocess.py:93} INFO -     spark.streams.awaitAnyTermination()
[2025-07-18T15:24:03.646+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 596, in awaitAnyTermination
[2025-07-18T15:24:03.647+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.3.733307f7-fefa-4823-93d4-79de52bb76f9.tmp to file:/tmp/checkpoints/reservations/commits/3
[2025-07-18T15:24:03.648+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-07-18T15:24:03.649+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-07-18T15:24:03.651+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.StreamingQueryException25/07/18 15:24:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:24:03.652+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:24:03.652+0000] {subprocess.py:93} INFO -   "runId" : "05e9c6f0-552d-440a-916d-062a56f88daa",
[2025-07-18T15:24:03.652+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:24:03.653+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:24:02.715Z",
[2025-07-18T15:24:03.653+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T15:24:03.653+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:24:03.654+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:24:03.654+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.0741138560687433,
[2025-07-18T15:24:03.655+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:24:03.655+0000] {subprocess.py:93} INFO -     "addBatch" : 787,
[2025-07-18T15:24:03.656+0000] {subprocess.py:93} INFO -     "commitOffsets" : 75,
[2025-07-18T15:24:03.656+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:24:03.656+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:24:03.657+0000] {subprocess.py:93} INFO -     "queryPlanning" : 25,
[2025-07-18T15:24:03.658+0000] {subprocess.py:93} INFO -     "triggerExecution" : 931,
[2025-07-18T15:24:03.658+0000] {subprocess.py:93} INFO -     "walCommit" : 41
[2025-07-18T15:24:03.658+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:24:03.659+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:24:03.659+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:24:03.660+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:24:03.660+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:24:03.660+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:24:03.660+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:24:03.661+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:03.661+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:03.662+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:24:03.662+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:24:03.662+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-07-18T15:24:03.663+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:03.663+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:03.663+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:24:03.664+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:24:03.666+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-07-18T15:24:03.666+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:03.667+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:03.667+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:24:03.667+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:24:03.668+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.0741138560687433,
[2025-07-18T15:24:03.669+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:24:03.670+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:24:03.671+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:24:03.671+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:24:03.671+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:24:03.672+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:24:03.672+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:24:03.672+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:24:03.673+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:24:03.673+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:24:03.674+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:24:03.676+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/4 using temp file file:/tmp/checkpoints/reservations/offsets/.4.c425c7c2-5bf0-49ee-b8d0-9d8875231b63.tmp
[2025-07-18T15:24:03.677+0000] {subprocess.py:93} INFO - : [STREAM_FAILED] Query [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 635bc90e-09bd-4ae9-a3d8-8039735dac8d] terminated with exception: Multiple streaming queries are concurrently using file:/tmp/checkpoints/checkins/offsets.
[2025-07-18T15:24:03.744+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.4.c425c7c2-5bf0-49ee-b8d0-9d8875231b63.tmp to file:/tmp/checkpoints/reservations/offsets/4
[2025-07-18T15:24:03.745+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752852243652,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:24:03.764+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.766+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.767+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.770+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:03.776+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:03.785+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.786+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.787+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:03.794+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.795+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.807+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:03.808+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:03.808+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:03.808+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-07-18T15:24:03.809+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2, groupId=spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-07-18T15:24:03.813+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:24:03.818+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:24:03.821+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 77cb57a6bd53:43611 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:24:03.822+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkContext: Created broadcast 22 from start at <unknown>:0
[2025-07-18T15:24:03.822+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:24:03.822+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:24:03.823+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:24:03.825+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
[2025-07-18T15:24:03.827+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:24:03.827+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:24:03.828+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:24:03.829+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T15:24:03.831+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T15:24:03.833+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 77cb57a6bd53:43611 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:24:03.833+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:24:03.834+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:24:03.834+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-18T15:24:03.834+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:24:03.835+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 ERROR Inbox: Ignoring error
[2025-07-18T15:24:03.835+0000] {subprocess.py:93} INFO - java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@799da9c7 rejected from java.util.concurrent.ThreadPoolExecutor@185abde2[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 11]
[2025-07-18T15:24:03.835+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)
[2025-07-18T15:24:03.836+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
[2025-07-18T15:24:03.836+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor.launchTask(Executor.scala:364)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T15:24:03.837+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T15:24:03.838+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
[2025-07-18T15:24:03.838+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:68)
[2025-07-18T15:24:03.838+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
[2025-07-18T15:24:03.838+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
[2025-07-18T15:24:03.838+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
[2025-07-18T15:24:03.840+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
[2025-07-18T15:24:03.840+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
[2025-07-18T15:24:03.840+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-07-18T15:24:03.841+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-07-18T15:24:03.841+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-07-18T15:24:03.843+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:24:03.843+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:24:03.844+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:24:03.844+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-3cde6346-63e4-42de-8340-16b1d5cf5ec0-858357054-executor-2 unregistered
[2025-07-18T15:24:03.844+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-07-18T15:24:03.845+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1, groupId=spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-07-18T15:24:03.846+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:24:03.846+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:24:03.847+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:24:03.847+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-40e334f2-423a-495c-baf5-42dbe6178a9b-603580605-executor-1 unregistered
[2025-07-18T15:24:03.848+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-07-18T15:24:03.850+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3, groupId=spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-07-18T15:24:03.851+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:24:03.851+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:24:03.852+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:24:03.852+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ee70d228-879f-44a4-9606-5cbd2204c84c-2028037020-executor-3 unregistered
[2025-07-18T15:24:03.852+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkContext: Invoking stop() from shutdown hook
[2025-07-18T15:24:03.853+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-07-18T15:24:03.860+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkUI: Stopped Spark web UI at http://77cb57a6bd53:4040
[2025-07-18T15:24:03.863+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) failed in 0.038 s due to Stage cancelled because SparkContext was shut down
[2025-07-18T15:24:03.863+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO DAGScheduler: Job 11 failed: start at <unknown>:0, took 0.041593 s
[2025-07-18T15:24:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is aborting.
[2025-07-18T15:24:03.867+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] failed to abort.
[2025-07-18T15:24:03.872+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 ERROR MicroBatchExecution: Query [id = 0314df7c-5598-4928-8d91-374ee67989d1, runId = 05e9c6f0-552d-440a-916d-062a56f88daa] terminated with error
[2025-07-18T15:24:03.872+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Writing job failed.
[2025-07-18T15:24:03.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobFailedError(QueryExecutionErrors.scala:903)
[2025-07-18T15:24:03.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:416)
[2025-07-18T15:24:03.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T15:24:03.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T15:24:03.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T15:24:03.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T15:24:03.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T15:24:03.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T15:24:03.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T15:24:03.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:24:03.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T15:24:03.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:24:03.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:24:03.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T15:24:03.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T15:24:03.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T15:24:03.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T15:24:03.878+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:24:03.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T15:24:03.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T15:24:03.879+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T15:24:03.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T15:24:03.879+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job 11 cancelled because SparkContext was shut down
[2025-07-18T15:24:03.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1259)
[2025-07-18T15:24:03.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1257)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1257)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3129)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3015)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3015)
[2025-07-18T15:24:03.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2258)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2211)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2025-07-18T15:24:03.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
[2025-07-18T15:24:03.882+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:24:03.882+0000] {subprocess.py:93} INFO - 	at scala.util.Try$.apply(Try.scala:213)
[2025-07-18T15:24:03.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
[2025-07-18T15:24:03.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
[2025-07-18T15:24:03.882+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[2025-07-18T15:24:03.882+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[2025-07-18T15:24:03.882+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-07-18T15:24:03.883+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-07-18T15:24:03.883+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-07-18T15:24:03.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
[2025-07-18T15:24:03.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2025-07-18T15:24:03.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)
[2025-07-18T15:24:03.884+0000] {subprocess.py:93} INFO - 	... 45 more
[2025-07-18T15:24:03.884+0000] {subprocess.py:93} INFO - 	Suppressed: java.lang.IllegalStateException: Shutdown in progress
[2025-07-18T15:24:03.884+0000] {subprocess.py:93} INFO - 		at java.base/java.lang.ApplicationShutdownHooks.add(Unknown Source)
[2025-07-18T15:24:03.885+0000] {subprocess.py:93} INFO - 		at java.base/java.lang.Runtime.addShutdownHook(Unknown Source)
[2025-07-18T15:24:03.885+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)
[2025-07-18T15:24:03.885+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)
[2025-07-18T15:24:03.886+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)
[2025-07-18T15:24:03.886+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)
[2025-07-18T15:24:03.886+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)
[2025-07-18T15:24:03.886+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.util.ThreadPools.newWorkerPool(ThreadPools.java:67)
[2025-07-18T15:24:03.887+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.hadoop.HadoopFileIO.executorService(HadoopFileIO.java:195)
[2025-07-18T15:24:03.887+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.hadoop.HadoopFileIO.deleteFiles(HadoopFileIO.java:170)
[2025-07-18T15:24:03.887+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkCleanupUtil.bulkDelete(SparkCleanupUtil.java:99)
[2025-07-18T15:24:03.887+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkCleanupUtil.deletePaths(SparkCleanupUtil.java:91)
[2025-07-18T15:24:03.887+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkCleanupUtil.deleteFiles(SparkCleanupUtil.java:85)
[2025-07-18T15:24:03.887+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkWrite.abort(SparkWrite.java:244)
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkWrite.access$800(SparkWrite.java:84)
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.abort(SparkWrite.java:545)
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 		at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.abort(MicroBatchWrite.scala:43)
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 		at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:411)
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 		... 45 more
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:24:03.888+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:24:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:24:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-07-18T15:24:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MicroBatchExecution: Async log purge executor pool for query [id = 0314df7c-5598-4928-8d91-374ee67989d1, runId = 05e9c6f0-552d-440a-916d-062a56f88daa] has been shutdown
[2025-07-18T15:24:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MemoryStore: MemoryStore cleared
[2025-07-18T15:24:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO BlockManager: BlockManager stopped
[2025-07-18T15:24:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-07-18T15:24:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-07-18T15:24:03.892+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO SparkContext: Successfully stopped SparkContext
[2025-07-18T15:24:03.893+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ShutdownHookManager: Shutdown hook called
[2025-07-18T15:24:03.893+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-a6e2c5b2-f0c5-448b-9272-9aa877de9afc
[2025-07-18T15:24:03.895+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-b7e08426-b2af-4948-abf7-305b5c547947
[2025-07-18T15:24:03.896+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-a6e2c5b2-f0c5-448b-9272-9aa877de9afc/pyspark-0705c32b-ff41-41ad-915f-70893858c202
[2025-07-18T15:24:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-07-18T15:24:03.903+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-07-18T15:24:03.903+0000] {subprocess.py:93} INFO - 25/07/18 15:24:03 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-07-18T15:24:04.288+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-07-18T15:24:04.301+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-07-18T15:24:04.303+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=restaurant_pipeline, task_id=stream_to_bronze, execution_date=20250718T152131, start_date=20250718T152136, end_date=20250718T152404
[2025-07-18T15:24:04.312+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 93 for task stream_to_bronze (Bash command failed. The command returned a non-zero exit code 1.; 240)
[2025-07-18T15:24:04.317+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-07-18T15:24:04.344+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
