[2025-07-18T16:16:05.413+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:14:00+00:00 [queued]>
[2025-07-18T16:16:05.425+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:14:00+00:00 [queued]>
[2025-07-18T16:16:05.425+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-07-18T16:16:05.436+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): stream_to_bronze> on 2025-07-18 16:14:00+00:00
[2025-07-18T16:16:05.442+0000] {standard_task_runner.py:57} INFO - Started process 1281 to run task
[2025-07-18T16:16:05.445+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'restaurant_pipeline', 'stream_to_bronze', 'scheduled__2025-07-18T16:14:00+00:00', '--job-id', '183', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_pipeline.py', '--cfg-path', '/tmp/tmpaeykoqph']
[2025-07-18T16:16:05.447+0000] {standard_task_runner.py:85} INFO - Job 183: Subtask stream_to_bronze
[2025-07-18T16:16:05.502+0000] {task_command.py:416} INFO - Running <TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:14:00+00:00 [running]> on host 9bcfb43e0ab7
[2025-07-18T16:16:05.576+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='moran' AIRFLOW_CTX_DAG_ID='restaurant_pipeline' AIRFLOW_CTX_TASK_ID='stream_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-07-18T16:14:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T16:14:00+00:00'
[2025-07-18T16:16:05.577+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-07-18T16:16:05.579+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', "docker exec -e AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T16:14:00+00:00' spark-iceberg spark-submit /home/iceberg/spark/stream_to_bronze.py"]
[2025-07-18T16:16:05.585+0000] {subprocess.py:86} INFO - Output:
[2025-07-18T16:16:07.234+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkContext: Running Spark version 3.5.6
[2025-07-18T16:16:07.235+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:16:07.235+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkContext: Java version 17.0.15
[2025-07-18T16:16:07.250+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO ResourceUtils: ==============================================================
[2025-07-18T16:16:07.250+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-18T16:16:07.250+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO ResourceUtils: ==============================================================
[2025-07-18T16:16:07.250+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkContext: Submitted application: StreamToBronze
[2025-07-18T16:16:07.261+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-18T16:16:07.266+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO ResourceProfile: Limiting resource is cpu
[2025-07-18T16:16:07.266+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-18T16:16:07.290+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SecurityManager: Changing view acls to: root,spark
[2025-07-18T16:16:07.291+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SecurityManager: Changing modify acls to: root,spark
[2025-07-18T16:16:07.291+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SecurityManager: Changing view acls groups to:
[2025-07-18T16:16:07.291+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SecurityManager: Changing modify acls groups to:
[2025-07-18T16:16:07.291+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
[2025-07-18T16:16:07.321+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-18T16:16:07.463+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Utils: Successfully started service 'sparkDriver' on port 43867.
[2025-07-18T16:16:07.482+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkEnv: Registering MapOutputTracker
[2025-07-18T16:16:07.511+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-18T16:16:07.526+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-18T16:16:07.529+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-18T16:16:07.538+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-18T16:16:07.582+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d1e9383c-d379-4c1b-9379-a41405316a6f
[2025-07-18T16:16:07.590+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-18T16:16:07.600+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-18T16:16:07.686+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-18T16:16:07.718+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-07-18T16:16:07.718+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2025-07-18T16:16:07.723+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Utils: Successfully started service 'SparkUI' on port 4042.
[2025-07-18T16:16:07.799+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Executor: Starting executor ID driver on host 77cb57a6bd53
[2025-07-18T16:16:07.800+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:16:07.800+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Executor: Java version 17.0.15
[2025-07-18T16:16:07.801+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-18T16:16:07.802+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1dafc29b for default.
[2025-07-18T16:16:07.817+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35301.
[2025-07-18T16:16:07.818+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO NettyBlockTransferService: Server created on 77cb57a6bd53:35301
[2025-07-18T16:16:07.819+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-18T16:16:07.829+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 77cb57a6bd53, 35301, None)
[2025-07-18T16:16:07.837+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO BlockManagerMasterEndpoint: Registering block manager 77cb57a6bd53:35301 with 434.4 MiB RAM, BlockManagerId(driver, 77cb57a6bd53, 35301, None)
[2025-07-18T16:16:07.838+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 77cb57a6bd53, 35301, None)
[2025-07-18T16:16:07.839+0000] {subprocess.py:93} INFO - 25/07/18 16:16:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 77cb57a6bd53, 35301, None)
[2025-07-18T16:16:08.094+0000] {subprocess.py:93} INFO - 25/07/18 16:16:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-18T16:16:08.097+0000] {subprocess.py:93} INFO - 25/07/18 16:16:08 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-07-18T16:16:09.796+0000] {subprocess.py:93} INFO - 25/07/18 16:16:09 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-07-18T16:16:09.814+0000] {subprocess.py:93} INFO - 25/07/18 16:16:09 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-07-18T16:16:09.814+0000] {subprocess.py:93} INFO - 25/07/18 16:16:09 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-07-18T16:16:10.956+0000] {subprocess.py:93} INFO - 25/07/18 16:16:10 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:10.970+0000] {subprocess.py:93} INFO - 25/07/18 16:16:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-07-18T16:16:11.049+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00 resolved to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00.
[2025-07-18T16:16:11.050+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:16:11.090+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/metadata using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/.metadata.638b637f-1247-4464-be5e-66c3b8b03cad.tmp
[2025-07-18T16:16:11.132+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/.metadata.638b637f-1247-4464-be5e-66c3b8b03cad.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/metadata
[2025-07-18T16:16:11.150+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Starting [id = 1ba8749c-1701-47e7-880c-fbc4a0b916d5, runId = 1469f87c-608e-4555-a6fe-7c0ce410c11f]. Use file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00 to store the query checkpoint.
[2025-07-18T16:16:11.159+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@50f245bb] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5ff1846f]
[2025-07-18T16:16:11.184+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:16:11.186+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:16:11.187+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:16:11.188+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:16:11.243+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:11.247+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00 resolved to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00.
[2025-07-18T16:16:11.248+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:16:11.257+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/metadata using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/.metadata.0f5407d5-f96d-4b9b-989a-0793091159af.tmp
[2025-07-18T16:16:11.271+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/.metadata.0f5407d5-f96d-4b9b-989a-0793091159af.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/metadata
[2025-07-18T16:16:11.279+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Starting [id = 14ee97d4-8b20-40a2-a8d4-01491461c78b, runId = 1c689d4b-5499-4b62-8616-1fa50e13ce42]. Use file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00 to store the query checkpoint.
[2025-07-18T16:16:11.280+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@20d44485] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2e13af37]
[2025-07-18T16:16:11.281+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:16:11.282+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:16:11.282+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:16:11.282+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:16:11.350+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:11.353+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00 resolved to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00.
[2025-07-18T16:16:11.354+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:16:11.360+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/metadata using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/.metadata.73d2613e-410e-47a5-ac85-2486b387eb8f.tmp
[2025-07-18T16:16:11.374+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/.metadata.73d2613e-410e-47a5-ac85-2486b387eb8f.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/metadata
[2025-07-18T16:16:11.378+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:16:11.379+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:16:11.379+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:16:11.379+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:16:11.379+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:16:11.379+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:16:11.379+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:16:11.380+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:16:11.380+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:16:11.380+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:16:11.380+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:16:11.380+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:16:11.380+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:16:11.381+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:16:11.381+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:16:11.381+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:16:11.381+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:16:11.381+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:16:11.382+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:16:11.382+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:16:11.383+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:16:11.383+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:16:11.384+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:16:11.384+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:16:11.384+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:16:11.384+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:16:11.385+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:16:11.385+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:16:11.385+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:16:11.385+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:16:11.386+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:16:11.386+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:16:11.386+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:16:11.386+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:16:11.387+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:16:11.387+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:16:11.387+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:16:11.387+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:16:11.388+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:16:11.388+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:16:11.388+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:16:11.388+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:16:11.388+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:16:11.389+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:16:11.389+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:16:11.390+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:16:11.390+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:16:11.390+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:16:11.390+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:16:11.390+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:16:11.391+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:16:11.391+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:16:11.391+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:16:11.391+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:16:11.391+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:16:11.391+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:16:11.391+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:16:11.392+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:16:11.393+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:16:11.393+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:16:11.393+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:16:11.393+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:16:11.393+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:16:11.393+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:16:11.393+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:16:11.394+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:16:11.395+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:16:11.396+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:16:11.396+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:16:11.396+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:16:11.396+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:16:11.396+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:16:11.396+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:16:11.396+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:16:11.397+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:16:11.397+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:16:11.397+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:16:11.397+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:16:11.398+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:16:11.398+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:16:11.398+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:16:11.398+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:16:11.398+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:16:11.398+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:16:11.398+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:16:11.399+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:16:11.399+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:16:11.399+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:16:11.399+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:16:11.399+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:16:11.400+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:16:11.400+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:16:11.400+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:16:11.400+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:16:11.400+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:16:11.400+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:16:11.400+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:16:11.401+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:16:11.401+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:16:11.401+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:16:11.401+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:16:11.402+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:16:11.402+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:16:11.402+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:16:11.402+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:16:11.403+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:16:11.403+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:16:11.403+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:16:11.404+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:16:11.405+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:16:11.405+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:16:11.405+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:16:11.405+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:16:11.407+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Starting [id = adf70716-4e16-4ce5-aa2c-d7a1cbea446a, runId = c457c2da-f6de-43bf-bc48-5cb636f17f98]. Use file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00 to store the query checkpoint.
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3482375c] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3a45ce64]
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:16:11.408+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:16:11.409+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:16:11.410+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:16:11.411+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:16:11.412+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:16:11.412+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:16:11.412+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:16:11.412+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:16:11.412+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:16:11.412+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:16:11.412+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:16:11.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:16:11.414+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:16:11.414+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:16:11.414+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:16:11.414+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:16:11.414+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:16:11.414+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:16:11.415+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:16:11.415+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:16:11.415+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:16:11.415+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:16:11.415+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:16:11.415+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:16:11.416+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:16:11.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:16:11.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:16:11.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:16:11.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:16:11.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:16:11.416+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:16:11.417+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:16:11.417+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:16:11.417+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:16:11.417+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:16:11.417+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:16:11.417+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:16:11.417+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:16:11.418+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:16:11.425+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:16:11.426+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:16:11.426+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:16:11.427+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:16:11.428+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:16:11.428+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka startTimeMs: 1752855371424
[2025-07-18T16:16:11.428+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:16:11.428+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:16:11.428+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka startTimeMs: 1752855371424
[2025-07-18T16:16:11.429+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:16:11.429+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:16:11.429+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO AppInfoParser: Kafka startTimeMs: 1752855371424
[2025-07-18T16:16:11.617+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/sources/0/.0.4200c1b5-6617-45fb-88e9-11358c4b0200.tmp
[2025-07-18T16:16:11.620+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/sources/0/.0.df3a0c67-5708-484b-bb43-af9cbaccf3a3.tmp
[2025-07-18T16:16:11.621+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/sources/0/.0.3a62b4a7-06b0-4740-9aac-718eb42d147b.tmp
[2025-07-18T16:16:11.633+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/sources/0/.0.4200c1b5-6617-45fb-88e9-11358c4b0200.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/sources/0/0
[2025-07-18T16:16:11.633+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaMicroBatchStream: Initial offsets: {"checkins":{"0":0}}
[2025-07-18T16:16:11.638+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/sources/0/.0.df3a0c67-5708-484b-bb43-af9cbaccf3a3.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/sources/0/0
[2025-07-18T16:16:11.639+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaMicroBatchStream: Initial offsets: {"feedback":{"0":0}}
[2025-07-18T16:16:11.639+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/sources/0/.0.3a62b4a7-06b0-4740-9aac-718eb42d147b.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/sources/0/0
[2025-07-18T16:16:11.639+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaMicroBatchStream: Initial offsets: {"reservations":{"0":0}}
[2025-07-18T16:16:11.650+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.0.75173cf1-88e8-4f9a-8d51-44bc23d77c40.tmp
[2025-07-18T16:16:11.651+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.0.60a188bb-9949-4cb9-95c0-49a7a1d9087f.tmp
[2025-07-18T16:16:11.651+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.0.ea695e60-4a71-4a5a-b819-c4fdaf6e0437.tmp
[2025-07-18T16:16:11.678+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.0.75173cf1-88e8-4f9a-8d51-44bc23d77c40.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/0
[2025-07-18T16:16:11.678+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855371642,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:11.678+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.0.60a188bb-9949-4cb9-95c0-49a7a1d9087f.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/0
[2025-07-18T16:16:11.679+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855371642,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:11.680+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.0.ea695e60-4a71-4a5a-b819-c4fdaf6e0437.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/0
[2025-07-18T16:16:11.680+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855371642,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:11.864+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:11.865+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:11.865+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:11.865+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:11.865+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:11.865+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:11.867+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:11.867+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:11.868+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:11.950+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:11.951+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:11.951+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:11.997+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:11.997+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:11.997+0000] {subprocess.py:93} INFO - 25/07/18 16:16:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.025+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:12.026+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:12.027+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:12.029+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:12.030+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:12.030+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:12.030+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.032+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.033+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:12.033+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:12.034+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:12.036+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.037+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.042+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.043+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.074+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:12.075+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:12.075+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:12.079+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:12.080+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.082+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:12.084+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:12.084+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.085+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.087+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:12.088+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:12.090+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:12.091+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.095+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.099+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:12.276+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 113.746833 ms
[2025-07-18T16:16:12.277+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 113.780667 ms
[2025-07-18T16:16:12.277+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 113.859209 ms
[2025-07-18T16:16:12.358+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:16:12.358+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:16:12.359+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:16:12.397+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:16:12.397+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:16:12.398+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:16:12.398+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:12.398+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:16:12.399+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:16:12.400+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Created broadcast 2 from start at <unknown>:0
[2025-07-18T16:16:12.400+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Created broadcast 1 from start at <unknown>:0
[2025-07-18T16:16:12.400+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Created broadcast 0 from start at <unknown>:0
[2025-07-18T16:16:12.411+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:12.412+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:12.412+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:12.420+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:12.421+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:12.421+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:12.428+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:12.429+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-07-18T16:16:12.429+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:12.429+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:12.431+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:12.475+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T16:16:12.475+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T16:16:12.476+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 77cb57a6bd53:35301 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:12.476+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:12.490+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:12.491+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-18T16:16:12.501+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:12.501+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-07-18T16:16:12.502+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:12.502+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:12.502+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:12.504+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:16:12.505+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:16:12.506+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 77cb57a6bd53:35301 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:16:12.506+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:12.507+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:12.507+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-18T16:16:12.513+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:16:12.518+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:12.519+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
[2025-07-18T16:16:12.519+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:12.520+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:12.521+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:12.522+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:16:12.523+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T16:16:12.524+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:16:12.524+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-18T16:16:12.525+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-18T16:16:12.526+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 77cb57a6bd53:35301 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:16:12.527+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:12.528+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:12.528+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-18T16:16:12.529+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:16:12.529+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-18T16:16:12.627+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 21.724375 ms
[2025-07-18T16:16:12.629+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 21.724334 ms
[2025-07-18T16:16:12.630+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 22.661708 ms
[2025-07-18T16:16:12.640+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 14.204375 ms
[2025-07-18T16:16:12.640+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 14.335625 ms
[2025-07-18T16:16:12.642+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 14.761458 ms
[2025-07-18T16:16:12.689+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:12.690+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:12.691+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:12.859+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=0 untilOffset=126, for query queryId=14ee97d4-8b20-40a2-a8d4-01491461c78b batchId=0 taskId=2 partitionId=0
[2025-07-18T16:16:12.860+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=0 untilOffset=126, for query queryId=adf70716-4e16-4ce5-aa2c-d7a1cbea446a batchId=0 taskId=1 partitionId=0
[2025-07-18T16:16:12.864+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=0 untilOffset=126, for query queryId=1ba8749c-1701-47e7-880c-fbc4a0b916d5 batchId=0 taskId=0 partitionId=0
[2025-07-18T16:16:12.885+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 8.158167 ms
[2025-07-18T16:16:12.904+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO CodeGenerator: Code generated in 9.700083 ms
[2025-07-18T16:16:12.917+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:16:12.917+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:16:12.917+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:16:12.918+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:16:12.918+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:16:12.918+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:16:12.918+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:16:12.919+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:16:12.919+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2
[2025-07-18T16:16:12.919+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:16:12.919+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:16:12.919+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:16:12.920+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:16:12.920+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:16:12.920+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:16:12.920+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:16:12.920+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:16:12.920+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor
[2025-07-18T16:16:12.921+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:16:12.921+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:16:12.921+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:16:12.921+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:16:12.921+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:16:12.921+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:16:12.922+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:16:12.923+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:16:12.923+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:16:12.923+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:16:12.923+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:16:12.923+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:16:12.923+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:16:12.924+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:16:12.924+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:16:12.924+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:16:12.925+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:16:12.925+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:16:12.925+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:16:12.925+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:16:12.926+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:16:12.926+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:16:12.926+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:16:12.926+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:16:12.926+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:16:12.926+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:16:12.927+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:16:12.927+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:16:12.927+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:16:12.927+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:16:12.927+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:16:12.927+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:16:12.928+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:16:12.928+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:16:12.929+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:16:12.929+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:16:12.929+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:16:12.929+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:16:12.929+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:16:12.930+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:16:12.930+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:16:12.930+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:16:12.930+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:16:12.930+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:16:12.931+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:16:12.931+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:16:12.931+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:16:12.931+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:16:12.931+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:16:12.931+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:16:12.932+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:16:12.933+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:16:12.934+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor
[2025-07-18T16:16:12.935+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:16:12.936+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:16:12.937+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:16:12.937+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:16:12.937+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:16:12.937+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:16:12.938+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:16:12.938+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:16:12.938+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:16:12.938+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:16:12.938+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:16:12.938+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:16:12.939+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:16:12.940+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:16:12.940+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:16:12.940+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:16:12.940+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:16:12.940+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:16:12.940+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:16:12.940+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:16:12.941+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:16:12.941+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:16:12.942+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:16:12.942+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:16:12.942+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:16:12.943+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:16:12.943+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:16:12.944+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:16:12.944+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:16:12.945+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:16:12.945+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:16:12.945+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:16:12.945+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:16:12.945+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:16:12.945+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:16:12.945+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:16:12.946+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:16:12.946+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:16:12.946+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:16:12.947+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:16:12.947+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:16:12.947+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:16:12.947+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:16:12.947+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:16:12.948+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:16:12.948+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:16:12.948+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:16:12.948+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:16:12.948+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:16:12.949+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:16:12.949+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:16:12.949+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:16:12.949+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:16:12.949+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:16:12.950+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:16:12.950+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:16:12.951+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:16:12.951+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:16:12.951+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:16:12.952+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:16:12.952+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:16:12.952+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:16:12.952+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:16:12.953+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:16:12.953+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:16:12.953+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:16:12.953+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:16:12.953+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:16:12.954+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:16:12.954+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3
[2025-07-18T16:16:12.954+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:16:12.954+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:16:12.954+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:16:12.954+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:16:12.954+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:16:12.955+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:16:12.955+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:16:12.955+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:16:12.955+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor
[2025-07-18T16:16:12.956+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:16:12.956+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:16:12.956+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:16:12.956+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:16:12.957+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:16:12.958+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:16:12.959+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:16:12.960+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:16:12.960+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:16:12.960+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:16:12.960+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:16:12.961+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:16:12.962+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:16:12.962+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:16:12.962+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:16:12.963+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:16:12.963+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:16:12.963+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:16:12.964+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:16:12.964+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:16:12.965+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:16:12.965+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:16:12.965+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:16:12.966+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:16:12.966+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:16:12.967+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:16:12.967+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:16:12.968+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:16:12.968+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:16:12.969+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:16:12.969+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:16:12.970+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:16:12.970+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:16:12.970+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:16:12.971+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:16:12.971+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:16:12.971+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:16:12.971+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:16:12.971+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:16:12.971+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:16:12.972+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:16:12.972+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:16:12.972+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:16:12.972+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:16:12.972+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:16:12.972+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:16:12.973+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:16:12.973+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:16:12.973+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:16:12.973+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:16:12.974+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:16:12.974+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:16:12.974+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:16:12.974+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:16:12.975+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:16:12.976+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:16:12.977+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:16:12.978+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:16:12.978+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:16:12.978+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:16:12.979+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:16:12.979+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:16:12.980+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:16:12.980+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:16:12.980+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:16:12.980+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:16:12.980+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:16:12.981+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:16:12.981+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:16:12.981+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:16:12.981+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:16:12.982+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka startTimeMs: 1752855372950
[2025-07-18T16:16:12.982+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:16:12.983+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:16:12.983+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka startTimeMs: 1752855372951
[2025-07-18T16:16:12.983+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:16:12.983+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO AppInfoParser: Kafka startTimeMs: 1752855372950
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Assigned to partition(s): reservations-0
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Assigned to partition(s): checkins-0
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Assigned to partition(s): feedback-0
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to offset 0 for partition reservations-0
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to offset 0 for partition feedback-0
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to offset 0 for partition checkins-0
[2025-07-18T16:16:12.984+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:16:12.985+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:16:12.985+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:16:13.001+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:16:13.002+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:16:13.002+0000] {subprocess.py:93} INFO - 25/07/18 16:16:12 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:16:13.513+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:13.516+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:16:13.517+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:13.518+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:16:13.518+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:13.519+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:16:13.519+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:13.520+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:13.520+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:13.636+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:16:13.636+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:16:13.637+0000] {subprocess.py:93} INFO - 25/07/18 16:16:13 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:16:14.092+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:16:14.094+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:16:14.095+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:16:14.095+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor read 126 records through 1 polls (polled  out 126 records), taking 559479501 nanos, during time span of 1127346084 nanos.
[2025-07-18T16:16:14.096+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor read 126 records through 1 polls (polled  out 126 records), taking 556317208 nanos, during time span of 1127708167 nanos.
[2025-07-18T16:16:14.096+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor read 126 records through 1 polls (polled  out 126 records), taking 556143125 nanos, during time span of 1129669209 nanos.
[2025-07-18T16:16:14.148+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4795 bytes result sent to driver
[2025-07-18T16:16:14.150+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4934 bytes result sent to driver
[2025-07-18T16:16:14.153+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4869 bytes result sent to driver
[2025-07-18T16:16:14.219+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1683 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:14.228+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-18T16:16:14.232+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1720 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:14.233+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-18T16:16:14.237+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1703 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:14.239+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-18T16:16:14.250+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.748 s
[2025-07-18T16:16:14.254+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:14.256+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-18T16:16:14.258+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.818 s
[2025-07-18T16:16:14.258+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:14.258+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-18T16:16:14.259+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.835969 s
[2025-07-18T16:16:14.262+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.836900 s
[2025-07-18T16:16:14.263+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 1.739 s
[2025-07-18T16:16:14.264+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:14.264+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-18T16:16:14.265+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 1.840605 s
[2025-07-18T16:16:14.265+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:14.265+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SparkWrite: Committing epoch 0 for query adf70716-4e16-4ce5-aa2c-d7a1cbea446a in append mode
[2025-07-18T16:16:14.266+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:14.267+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SparkWrite: Committing epoch 0 for query 1ba8749c-1701-47e7-880c-fbc4a0b916d5 in append mode
[2025-07-18T16:16:14.268+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:14.268+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SparkWrite: Committing epoch 0 for query 14ee97d4-8b20-40a2-a8d4-01491461c78b in append mode
[2025-07-18T16:16:14.319+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:14.319+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:14.321+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:14.902+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v77.metadata.json
[2025-07-18T16:16:14.905+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v79.metadata.json
[2025-07-18T16:16:14.905+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v68.metadata.json
[2025-07-18T16:16:14.975+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SnapshotProducer: Committed snapshot 5827522053273136645 (FastAppend)
[2025-07-18T16:16:14.985+0000] {subprocess.py:93} INFO - 25/07/18 16:16:14 INFO SnapshotProducer: Committed snapshot 2264928466705410709 (FastAppend)
[2025-07-18T16:16:15.040+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO SnapshotProducer: Committed snapshot 554407422218728885 (FastAppend)
[2025-07-18T16:16:15.047+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=5827522053273136645, sequenceNumber=76, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.71342175S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=76}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=126}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=594}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=7500}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=239895}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:15.047+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO SparkWrite: Committed in 727 ms
[2025-07-18T16:16:15.047+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:16:15.055+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=2264928466705410709, sequenceNumber=78, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.729222583S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=78}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=126}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=594}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=8985}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=249957}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:15.056+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO SparkWrite: Committed in 731 ms
[2025-07-18T16:16:15.057+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:16:15.081+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.0.3a0e23db-2399-4658-a1a7-eff861dbd89a.tmp
[2025-07-18T16:16:15.084+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.0.527b90c9-279f-40fe-b098-efd357751a02.tmp
[2025-07-18T16:16:15.121+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=554407422218728885, sequenceNumber=67, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.787602708S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=67}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=126}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=593}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=5904}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=212005}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:15.124+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO SparkWrite: Committed in 790 ms
[2025-07-18T16:16:15.124+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:16:15.146+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.0.5f281f02-5627-4729-a82a-65ee0c96cf81.tmp
[2025-07-18T16:16:15.147+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.0.3a0e23db-2399-4658-a1a7-eff861dbd89a.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/0
[2025-07-18T16:16:15.156+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.0.527b90c9-279f-40fe-b098-efd357751a02.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/0
[2025-07-18T16:16:15.189+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.0.5f281f02-5627-4729-a82a-65ee0c96cf81.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/0
[2025-07-18T16:16:15.203+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:15.204+0000] {subprocess.py:93} INFO -   "id" : "adf70716-4e16-4ce5-aa2c-d7a1cbea446a",
[2025-07-18T16:16:15.205+0000] {subprocess.py:93} INFO -   "runId" : "c457c2da-f6de-43bf-bc48-5cb636f17f98",
[2025-07-18T16:16:15.205+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:15.205+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:11.383Z",
[2025-07-18T16:16:15.206+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:16:15.206+0000] {subprocess.py:93} INFO -   "numInputRows" : 126,
[2025-07-18T16:16:15.206+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:16:15.206+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 33.38632750397456,
[2025-07-18T16:16:15.206+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -     "addBatch" : 3043,
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -     "commitOffsets" : 100,
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -     "getBatch" : 9,
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -     "latestOffset" : 257,
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -     "queryPlanning" : 320,
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3774,
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -     "walCommit" : 32
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:15.207+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:15.208+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:15.208+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:16:15.208+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:16:15.208+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:15.208+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:15.209+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:15.209+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:15.209+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:15.210+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:15.210+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:15.210+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:15.210+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:15.211+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:15.211+0000] {subprocess.py:93} INFO -     "numInputRows" : 126,
[2025-07-18T16:16:15.211+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:16:15.211+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 33.38632750397456,
[2025-07-18T16:16:15.212+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:15.212+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:15.212+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:15.212+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:15.212+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:15.212+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:15.213+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:15.213+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:16:15.213+0000] {subprocess.py:93} INFO -     "numOutputRows" : 126
[2025-07-18T16:16:15.213+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:15.213+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:15.214+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:15.214+0000] {subprocess.py:93} INFO -   "id" : "1ba8749c-1701-47e7-880c-fbc4a0b916d5",
[2025-07-18T16:16:15.214+0000] {subprocess.py:93} INFO -   "runId" : "1469f87c-608e-4555-a6fe-7c0ce410c11f",
[2025-07-18T16:16:15.214+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:11.177Z",
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -   "numInputRows" : 126,
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 31.413612565445025,
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -     "addBatch" : 3103,
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -     "commitOffsets" : 76,
[2025-07-18T16:16:15.215+0000] {subprocess.py:93} INFO -     "getBatch" : 6,
[2025-07-18T16:16:15.216+0000] {subprocess.py:93} INFO -     "latestOffset" : 453,
[2025-07-18T16:16:15.217+0000] {subprocess.py:93} INFO -     "queryPlanning" : 320,
[2025-07-18T16:16:15.217+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4010,
[2025-07-18T16:16:15.217+0000] {subprocess.py:93} INFO -     "walCommit" : 36
[2025-07-18T16:16:15.218+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:15.218+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:15.218+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:15.220+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:16:15.221+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:16:15.221+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:15.222+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:15.222+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:15.223+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:15.224+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:15.224+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:15.224+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:15.225+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:15.225+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:15.225+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:15.226+0000] {subprocess.py:93} INFO -     "numInputRows" : 126,
[2025-07-18T16:16:15.226+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:16:15.227+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 31.413612565445025,
[2025-07-18T16:16:15.228+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:15.229+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:15.229+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:15.230+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:15.230+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:15.231+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:15.231+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:15.232+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:16:15.234+0000] {subprocess.py:93} INFO -     "numOutputRows" : 126
[2025-07-18T16:16:15.234+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:15.246+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:15.246+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:15.246+0000] {subprocess.py:93} INFO -   "id" : "14ee97d4-8b20-40a2-a8d4-01491461c78b",
[2025-07-18T16:16:15.247+0000] {subprocess.py:93} INFO -   "runId" : "1c689d4b-5499-4b62-8616-1fa50e13ce42",
[2025-07-18T16:16:15.247+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:15.247+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:11.279Z",
[2025-07-18T16:16:15.248+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:16:15.249+0000] {subprocess.py:93} INFO -   "numInputRows" : 126,
[2025-07-18T16:16:15.250+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:16:15.254+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 32.57497414684592,
[2025-07-18T16:16:15.261+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:15.262+0000] {subprocess.py:93} INFO -     "addBatch" : 3041,
[2025-07-18T16:16:15.262+0000] {subprocess.py:93} INFO -     "commitOffsets" : 85,
[2025-07-18T16:16:15.263+0000] {subprocess.py:93} INFO -     "getBatch" : 6,
[2025-07-18T16:16:15.263+0000] {subprocess.py:93} INFO -     "latestOffset" : 361,
[2025-07-18T16:16:15.263+0000] {subprocess.py:93} INFO -     "queryPlanning" : 320,
[2025-07-18T16:16:15.264+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3868,
[2025-07-18T16:16:15.264+0000] {subprocess.py:93} INFO -     "walCommit" : 33
[2025-07-18T16:16:15.264+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:15.264+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:15.265+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:15.265+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:16:15.265+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:16:15.265+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:15.266+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:15.266+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:15.266+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:15.267+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:15.267+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:15.267+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:15.267+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:15.268+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:15.268+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:15.269+0000] {subprocess.py:93} INFO -     "numInputRows" : 126,
[2025-07-18T16:16:15.269+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:16:15.269+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 32.57497414684592,
[2025-07-18T16:16:15.270+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:15.270+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:15.271+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:15.276+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:15.277+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:15.277+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:15.278+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:15.279+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:16:15.283+0000] {subprocess.py:93} INFO -     "numOutputRows" : 126
[2025-07-18T16:16:15.284+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:15.284+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:17.562+0000] {subprocess.py:93} INFO - 25/07/18 16:16:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:16:17.565+0000] {subprocess.py:93} INFO - 25/07/18 16:16:17 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 77cb57a6bd53:35301 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:17.566+0000] {subprocess.py:93} INFO - 25/07/18 16:16:17 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 77cb57a6bd53:35301 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:16:17.569+0000] {subprocess.py:93} INFO - 25/07/18 16:16:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:17.572+0000] {subprocess.py:93} INFO - 25/07/18 16:16:17 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 77cb57a6bd53:35301 in memory (size: 12.3 KiB, free: 434.4 MiB)
[2025-07-18T16:16:17.575+0000] {subprocess.py:93} INFO - 25/07/18 16:16:17 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:16:25.181+0000] {subprocess.py:93} INFO - 25/07/18 16:16:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:25.182+0000] {subprocess.py:93} INFO - 25/07/18 16:16:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:25.190+0000] {subprocess.py:93} INFO - 25/07/18 16:16:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:35.187+0000] {subprocess.py:93} INFO - 25/07/18 16:16:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:35.188+0000] {subprocess.py:93} INFO - 25/07/18 16:16:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:35.199+0000] {subprocess.py:93} INFO - 25/07/18 16:16:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:45.202+0000] {subprocess.py:93} INFO - 25/07/18 16:16:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:45.204+0000] {subprocess.py:93} INFO - 25/07/18 16:16:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:45.204+0000] {subprocess.py:93} INFO - 25/07/18 16:16:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:55.207+0000] {subprocess.py:93} INFO - 25/07/18 16:16:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:55.209+0000] {subprocess.py:93} INFO - 25/07/18 16:16:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:55.209+0000] {subprocess.py:93} INFO - 25/07/18 16:16:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:05.204+0000] {subprocess.py:93} INFO - 25/07/18 16:17:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:05.206+0000] {subprocess.py:93} INFO - 25/07/18 16:17:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:05.219+0000] {subprocess.py:93} INFO - 25/07/18 16:17:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:15.205+0000] {subprocess.py:93} INFO - 25/07/18 16:17:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:15.206+0000] {subprocess.py:93} INFO - 25/07/18 16:17:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:15.229+0000] {subprocess.py:93} INFO - 25/07/18 16:17:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:25.216+0000] {subprocess.py:93} INFO - 25/07/18 16:17:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:25.218+0000] {subprocess.py:93} INFO - 25/07/18 16:17:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:25.238+0000] {subprocess.py:93} INFO - 25/07/18 16:17:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:35.220+0000] {subprocess.py:93} INFO - 25/07/18 16:17:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:35.222+0000] {subprocess.py:93} INFO - 25/07/18 16:17:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:35.244+0000] {subprocess.py:93} INFO - 25/07/18 16:17:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:45.224+0000] {subprocess.py:93} INFO - 25/07/18 16:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:45.227+0000] {subprocess.py:93} INFO - 25/07/18 16:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:45.247+0000] {subprocess.py:93} INFO - 25/07/18 16:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:55.222+0000] {subprocess.py:93} INFO - 25/07/18 16:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:55.227+0000] {subprocess.py:93} INFO - 25/07/18 16:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:55.248+0000] {subprocess.py:93} INFO - 25/07/18 16:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:02.183+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.1.6afeb1d0-2564-4744-934c-4bce84bc23ac.tmp
[2025-07-18T16:18:02.224+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.1.6afeb1d0-2564-4744-934c-4bce84bc23ac.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/1
[2025-07-18T16:18:02.225+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855482164,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:02.282+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.285+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.289+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.290+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.295+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.332+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.339+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.340+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.343+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.373+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.413+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.415+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.416+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.473+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.478+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.542+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:18:02.576+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:18:02.581+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:02.586+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkContext: Created broadcast 6 from start at <unknown>:0
[2025-07-18T16:18:02.587+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:02.588+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:02.588+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:02.592+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Final stage: ResultStage 3 (start at <unknown>:0)
[2025-07-18T16:18:02.605+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:02.608+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:02.610+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:02.614+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:18:02.654+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:18:02.698+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 77cb57a6bd53:35301 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:02.700+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:02.703+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:02.704+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-07-18T16:18:02.711+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:18:02.736+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-07-18T16:18:02.801+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.1.e212bbc5-3f29-496e-ab3d-912667bf9497.tmp
[2025-07-18T16:18:02.803+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:02.811+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=126 untilOffset=127, for query queryId=1ba8749c-1701-47e7-880c-fbc4a0b916d5 batchId=1 taskId=3 partitionId=0
[2025-07-18T16:18:02.899+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to offset 126 for partition reservations-0
[2025-07-18T16:18:02.908+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:18:02.923+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.1.e212bbc5-3f29-496e-ab3d-912667bf9497.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/1
[2025-07-18T16:18:02.926+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855482781,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:02.939+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.940+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.940+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.941+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.948+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.978+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.982+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.982+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.992+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.025+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.093+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.098+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.105+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.119+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.153+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:18:03.159+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.160+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.160+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 8 from start at <unknown>:0
[2025-07-18T16:18:03.160+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:03.162+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:03.163+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:03.163+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
[2025-07-18T16:18:03.164+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:03.164+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:03.173+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:03.175+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.212+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.213+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 77cb57a6bd53:35301 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.218+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:03.221+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:03.222+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-18T16:18:03.226+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:18:03.232+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-07-18T16:18:03.241+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:03.243+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=126 untilOffset=127, for query queryId=14ee97d4-8b20-40a2-a8d4-01491461c78b batchId=1 taskId=4 partitionId=0
[2025-07-18T16:18:03.270+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to offset 126 for partition checkins-0
[2025-07-18T16:18:03.276+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:18:03.416+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.419+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:18:03.419+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.430+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:18:03.439+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.1.cd927d2d-092c-4f0d-9773-d9a999e459f8.tmp
[2025-07-18T16:18:03.520+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:18:03.521+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor read 1 records through 1 polls (polled  out 3 records), taking 515318417 nanos, during time span of 623282542 nanos.
[2025-07-18T16:18:03.524+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4745 bytes result sent to driver
[2025-07-18T16:18:03.530+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 818 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:03.531+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-07-18T16:18:03.531+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: ResultStage 3 (start at <unknown>:0) finished in 0.920 s
[2025-07-18T16:18:03.531+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:03.532+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-07-18T16:18:03.533+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.950303 s
[2025-07-18T16:18:03.533+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:03.538+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing epoch 1 for query 1ba8749c-1701-47e7-880c-fbc4a0b916d5 in append mode
[2025-07-18T16:18:03.550+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.1.cd927d2d-092c-4f0d-9773-d9a999e459f8.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/1
[2025-07-18T16:18:03.560+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855483400,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:03.572+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.573+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.575+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.576+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.584+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:03.593+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.689+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.691+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.692+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.701+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.723+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.782+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.783+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:18:03.786+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.786+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.786+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.786+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.786+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:18:03.801+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.805+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.833+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.837+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:03.839+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.840+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 10 from start at <unknown>:0
[2025-07-18T16:18:03.848+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:03.867+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:18:03.873+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor read 1 records through 1 polls (polled  out 3 records), taking 517647042 nanos, during time span of 578122625 nanos.
[2025-07-18T16:18:03.875+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:03.878+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4864 bytes result sent to driver
[2025-07-18T16:18:03.879+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:03.879+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Final stage: ResultStage 5 (start at <unknown>:0)
[2025-07-18T16:18:03.880+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:03.880+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:03.880+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:03.880+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:03.882+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 626 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:03.882+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-18T16:18:03.883+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:18:03.883+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 77cb57a6bd53:35301 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.883+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:03.883+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:03.884+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-07-18T16:18:03.885+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:18:03.885+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.698 s
[2025-07-18T16:18:03.885+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:03.887+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-18T16:18:03.888+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-07-18T16:18:03.888+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.716309 s
[2025-07-18T16:18:03.888+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:03.888+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing epoch 1 for query 14ee97d4-8b20-40a2-a8d4-01491461c78b in append mode
[2025-07-18T16:18:03.901+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:03.903+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=126 untilOffset=127, for query queryId=adf70716-4e16-4ce5-aa2c-d7a1cbea446a batchId=1 taskId=5 partitionId=0
[2025-07-18T16:18:03.913+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to offset 126 for partition feedback-0
[2025-07-18T16:18:03.936+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:18:03.999+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.029+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 WARN Tasks: Retrying task after failure: Version 70 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v70.metadata.json
[2025-07-18T16:18:04.031+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 70 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v70.metadata.json
[2025-07-18T16:18:04.048+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:18:04.049+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:18:04.052+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:18:04.052+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:18:04.053+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:18:04.053+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:18:04.053+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:18:04.055+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:18:04.055+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:18:04.056+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:18:04.057+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:18:04.059+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:18:04.059+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:18:04.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:18:04.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:18:04.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:18:04.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:18:04.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:18:04.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:18:04.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:18:04.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:18:04.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:18:04.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:18:04.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:18:04.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:18:04.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:18:04.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:04.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:04.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:04.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:04.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:04.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:18:04.080+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:18:04.081+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:18:04.082+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:04.082+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:04.083+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:04.084+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:04.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:04.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:18:04.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:04.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:04.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:04.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:18:04.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:18:04.089+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:04.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:04.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:04.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:04.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:18:04.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:18:04.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:18:04.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:18:04.098+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:04.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:04.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:18:04.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:18:04.101+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:04.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:18:04.103+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:18:04.293+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 WARN Tasks: Retrying task after failure: Version 79 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v79.metadata.json
[2025-07-18T16:18:04.293+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 79 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v79.metadata.json
[2025-07-18T16:18:04.293+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:18:04.294+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:18:04.294+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:18:04.296+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:18:04.297+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:18:04.298+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:18:04.298+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:18:04.299+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:18:04.299+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:18:04.299+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:18:04.300+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:18:04.300+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:18:04.300+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:18:04.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:18:04.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:18:04.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:18:04.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:18:04.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:18:04.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:18:04.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:18:04.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:18:04.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:18:04.308+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:18:04.309+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:18:04.310+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:18:04.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:18:04.316+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:04.318+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:04.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:04.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:04.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:04.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:18:04.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:18:04.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:18:04.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:04.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:04.331+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:04.332+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:04.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:04.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:18:04.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:04.337+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:04.338+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:04.341+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:18:04.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:18:04.346+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:04.348+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:04.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:04.351+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:04.353+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:18:04.360+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:18:04.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:18:04.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:18:04.372+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:04.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:04.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:18:04.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:18:04.386+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:04.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:18:04.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:18:04.432+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:04.433+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:18:04.450+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:04.479+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:18:04.516+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v71.metadata.json
[2025-07-18T16:18:04.582+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:18:04.586+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor read 1 records through 1 polls (polled  out 3 records), taking 521231501 nanos, during time span of 668358750 nanos.
[2025-07-18T16:18:04.587+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4819 bytes result sent to driver
[2025-07-18T16:18:04.595+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 724 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:04.600+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-07-18T16:18:04.600+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: ResultStage 5 (start at <unknown>:0) finished in 0.744 s
[2025-07-18T16:18:04.601+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:04.602+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-07-18T16:18:04.603+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.756957 s
[2025-07-18T16:18:04.607+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:04.608+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committing epoch 1 for query adf70716-4e16-4ce5-aa2c-d7a1cbea446a in append mode
[2025-07-18T16:18:04.648+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SnapshotProducer: Committed snapshot 8988217303155816039 (FastAppend)
[2025-07-18T16:18:04.654+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.739+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v80.metadata.json
[2025-07-18T16:18:04.808+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=8988217303155816039, sequenceNumber=70, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.220427209S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=70}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=596}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3004}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=221017}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:04.810+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committed in 1222 ms
[2025-07-18T16:18:04.810+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:18:04.833+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/1 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.1.4ca7dc4d-863f-4af4-8e0e-07754924b385.tmp
[2025-07-18T16:18:04.848+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SnapshotProducer: Committed snapshot 8995210634136153981 (FastAppend)
[2025-07-18T16:18:04.898+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.1.4ca7dc4d-863f-4af4-8e0e-07754924b385.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/1
[2025-07-18T16:18:04.900+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:04.901+0000] {subprocess.py:93} INFO -   "id" : "1ba8749c-1701-47e7-880c-fbc4a0b916d5",
[2025-07-18T16:18:04.902+0000] {subprocess.py:93} INFO -   "runId" : "1469f87c-608e-4555-a6fe-7c0ce410c11f",
[2025-07-18T16:18:04.902+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:04.903+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:02.161Z",
[2025-07-18T16:18:04.905+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:18:04.907+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:18:04.908+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:18:04.910+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.3656307129798903,
[2025-07-18T16:18:04.911+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:04.912+0000] {subprocess.py:93} INFO -     "addBatch" : 2501,
[2025-07-18T16:18:04.916+0000] {subprocess.py:93} INFO -     "commitOffsets" : 92,
[2025-07-18T16:18:04.917+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:18:04.918+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:18:04.923+0000] {subprocess.py:93} INFO -     "queryPlanning" : 59,
[2025-07-18T16:18:04.925+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2735,
[2025-07-18T16:18:04.932+0000] {subprocess.py:93} INFO -     "walCommit" : 73
[2025-07-18T16:18:04.933+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:04.933+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:04.933+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:04.933+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:18:04.934+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:04.937+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:04.938+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:18:04.939+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.940+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.942+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:04.944+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:04.946+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:04.947+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.947+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.948+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:04.948+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:04.948+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:04.948+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.948+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.949+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:18:04.954+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:18:04.956+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.3656307129798903,
[2025-07-18T16:18:04.958+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:04.960+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:04.961+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:04.962+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:04.964+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:04.965+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:04.967+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:04.968+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:18:04.971+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:18:04.972+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:04.972+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:04.973+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 WARN Tasks: Retrying task after failure: Version 81 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v81.metadata.json
[2025-07-18T16:18:04.973+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 81 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v81.metadata.json
[2025-07-18T16:18:04.974+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:18:04.977+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:18:04.979+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:18:04.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:18:04.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:18:04.981+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:18:04.981+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:18:04.983+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:18:04.984+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:18:04.984+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:18:04.987+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:18:04.989+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:18:04.990+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:18:04.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:18:04.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:18:04.992+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:18:04.993+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:18:04.994+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:18:04.994+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:18:04.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:18:04.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:18:04.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:18:04.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:18:05.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:18:05.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:18:05.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:18:05.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:18:05.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:18:05.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:18:05.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:18:05.034+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.035+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.035+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:18:05.035+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:18:05.036+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:18:05.036+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:18:05.037+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.037+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.038+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:18:05.038+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:18:05.039+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.040+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:18:05.042+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:18:05.043+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.2.c0ac39bb-eac3-477a-ac75-b707f3f1c9da.tmp
[2025-07-18T16:18:05.043+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=8995210634136153981, sequenceNumber=79, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.98067375S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=79}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=597}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2896}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=248583}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:05.044+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committed in 980 ms
[2025-07-18T16:18:05.046+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:18:05.047+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/1 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.1.014cc874-1b92-4665-bd98-0030e0943cae.tmp
[2025-07-18T16:18:05.049+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.2.c0ac39bb-eac3-477a-ac75-b707f3f1c9da.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/2
[2025-07-18T16:18:05.050+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855484920,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:05.069+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.1.014cc874-1b92-4665-bd98-0030e0943cae.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/1
[2025-07-18T16:18:05.074+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:05.075+0000] {subprocess.py:93} INFO -   "id" : "14ee97d4-8b20-40a2-a8d4-01491461c78b",
[2025-07-18T16:18:05.075+0000] {subprocess.py:93} INFO -   "runId" : "1c689d4b-5499-4b62-8616-1fa50e13ce42",
[2025-07-18T16:18:05.076+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:05.076+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:02.771Z",
[2025-07-18T16:18:05.077+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:18:05.079+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:18:05.080+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 45.45454545454546,
[2025-07-18T16:18:05.080+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.43802014892685065,
[2025-07-18T16:18:05.081+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:05.082+0000] {subprocess.py:93} INFO -     "addBatch" : 2025,
[2025-07-18T16:18:05.082+0000] {subprocess.py:93} INFO -     "commitOffsets" : 76,
[2025-07-18T16:18:05.082+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:05.083+0000] {subprocess.py:93} INFO -     "latestOffset" : 10,
[2025-07-18T16:18:05.083+0000] {subprocess.py:93} INFO -     "queryPlanning" : 25,
[2025-07-18T16:18:05.084+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2281,
[2025-07-18T16:18:05.086+0000] {subprocess.py:93} INFO -     "walCommit" : 144
[2025-07-18T16:18:05.087+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:05.088+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:05.088+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:05.088+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:18:05.089+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:05.090+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:05.091+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:18:05.091+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.091+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.093+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:05.095+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:05.096+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:05.097+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.098+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.099+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:05.099+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:05.099+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:05.099+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.099+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.100+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:18:05.100+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 45.45454545454546,
[2025-07-18T16:18:05.101+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.43802014892685065,
[2025-07-18T16:18:05.101+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:05.104+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:05.105+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:05.105+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:05.107+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:05.108+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:05.108+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:05.108+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:18:05.108+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:18:05.109+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:05.109+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:05.110+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.110+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.110+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.110+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.110+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.2.4c7a3975-f17a-4806-829e-f78941f11fc8.tmp
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.112+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.112+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.118+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.137+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.2.4c7a3975-f17a-4806-829e-f78941f11fc8.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/2
[2025-07-18T16:18:05.139+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855485060,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:05.156+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.163+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.164+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.176+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.178+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:18:05.181+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T16:18:05.185+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:18:05.193+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkContext: Created broadcast 12 from start at <unknown>:0
[2025-07-18T16:18:05.193+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:05.197+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:05.199+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.203+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:05.206+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
[2025-07-18T16:18:05.210+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:05.219+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:05.221+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:05.222+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T16:18:05.223+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T16:18:05.225+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 77cb57a6bd53:35301 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:18:05.226+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:05.231+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:05.232+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-18T16:18:05.232+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:18:05.233+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-07-18T16:18:05.236+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.237+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.238+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.238+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:05.238+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=127 untilOffset=129, for query queryId=1ba8749c-1701-47e7-880c-fbc4a0b916d5 batchId=2 taskId=6 partitionId=0
[2025-07-18T16:18:05.238+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.242+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.260+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:18:05.260+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.261+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.261+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.272+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.274+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.302+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:18:05.302+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 56326875 nanos.
[2025-07-18T16:18:05.304+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4734 bytes result sent to driver
[2025-07-18T16:18:05.309+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 100 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:05.310+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-18T16:18:05.313+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.118 s
[2025-07-18T16:18:05.314+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:05.315+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-07-18T16:18:05.316+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:18:05.316+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.127519 s
[2025-07-18T16:18:05.316+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:05.316+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing epoch 2 for query 1ba8749c-1701-47e7-880c-fbc4a0b916d5 in append mode
[2025-07-18T16:18:05.319+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T16:18:05.329+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:18:05.330+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkContext: Created broadcast 14 from start at <unknown>:0
[2025-07-18T16:18:05.331+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:05.331+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:05.334+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:05.334+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
[2025-07-18T16:18:05.334+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:05.335+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:05.335+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:05.347+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T16:18:05.353+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T16:18:05.354+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 77cb57a6bd53:35301 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:18:05.360+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:05.360+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:05.361+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-18T16:18:05.366+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:18:05.373+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:05.382+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-07-18T16:18:05.397+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:05.405+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=127 untilOffset=129, for query queryId=14ee97d4-8b20-40a2-a8d4-01491461c78b batchId=2 taskId=7 partitionId=0
[2025-07-18T16:18:05.423+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v82.metadata.json
[2025-07-18T16:18:05.446+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:18:05.479+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:18:05.486+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 40309166 nanos.
[2025-07-18T16:18:05.489+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4850 bytes result sent to driver
[2025-07-18T16:18:05.490+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 122 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:05.491+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-18T16:18:05.492+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.150 s
[2025-07-18T16:18:05.493+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:05.498+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-18T16:18:05.498+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 0.161310 s
[2025-07-18T16:18:05.499+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:05.499+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing epoch 2 for query 14ee97d4-8b20-40a2-a8d4-01491461c78b in append mode
[2025-07-18T16:18:05.561+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.577+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SnapshotProducer: Committed snapshot 2890128153026012557 (FastAppend)
[2025-07-18T16:18:05.691+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 WARN Tasks: Retrying task after failure: Version 73 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v73.metadata.json
[2025-07-18T16:18:05.693+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 73 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v73.metadata.json
[2025-07-18T16:18:05.698+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:18:05.704+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:18:05.705+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:18:05.709+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:18:05.719+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:18:05.721+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:18:05.722+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:18:05.723+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:18:05.727+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:18:05.728+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:18:05.729+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:18:05.729+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:18:05.730+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:18:05.732+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:18:05.733+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:18:05.738+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:18:05.740+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:18:05.744+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:18:05.745+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:18:05.746+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:18:05.748+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:18:05.749+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:18:05.750+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:18:05.752+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:18:05.752+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:18:05.752+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:18:05.753+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.753+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.754+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.754+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.758+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.759+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:18:05.760+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:18:05.760+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:18:05.760+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.760+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.761+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.762+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.763+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.765+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:18:05.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:18:05.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:18:05.768+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:18:05.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:18:05.768+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:18:05.769+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:18:05.769+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=2890128153026012557, sequenceNumber=81, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.064738584S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=81}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=597}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=258804}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:05.769+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committed in 1065 ms
[2025-07-18T16:18:05.769+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:18:05.770+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/1 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.1.b0dc2c37-2a61-419d-848f-17da8d559265.tmp
[2025-07-18T16:18:05.800+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.1.b0dc2c37-2a61-419d-848f-17da8d559265.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/1
[2025-07-18T16:18:05.805+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:05.806+0000] {subprocess.py:93} INFO -   "id" : "adf70716-4e16-4ce5-aa2c-d7a1cbea446a",
[2025-07-18T16:18:05.806+0000] {subprocess.py:93} INFO -   "runId" : "c457c2da-f6de-43bf-bc48-5cb636f17f98",
[2025-07-18T16:18:05.808+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:05.808+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:03.391Z",
[2025-07-18T16:18:05.809+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:18:05.809+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:18:05.809+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 31.25,
[2025-07-18T16:18:05.809+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.41511000415110005,
[2025-07-18T16:18:05.809+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:05.809+0000] {subprocess.py:93} INFO -     "addBatch" : 2116,
[2025-07-18T16:18:05.809+0000] {subprocess.py:93} INFO -     "commitOffsets" : 81,
[2025-07-18T16:18:05.810+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:05.810+0000] {subprocess.py:93} INFO -     "latestOffset" : 9,
[2025-07-18T16:18:05.810+0000] {subprocess.py:93} INFO -     "queryPlanning" : 42,
[2025-07-18T16:18:05.810+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2408,
[2025-07-18T16:18:05.810+0000] {subprocess.py:93} INFO -     "walCommit" : 158
[2025-07-18T16:18:05.810+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:05.811+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:05.812+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:05.815+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:18:05.816+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:05.817+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:05.818+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:18:05.819+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.819+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.821+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:05.821+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:05.822+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:05.823+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.823+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.823+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:05.824+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:05.824+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:05.825+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.825+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.826+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:18:05.826+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 31.25,
[2025-07-18T16:18:05.827+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.41511000415110005,
[2025-07-18T16:18:05.827+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:05.828+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:05.828+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:05.829+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:05.829+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:05.831+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:05.831+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:05.832+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:18:05.832+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:18:05.832+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:05.833+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:05.845+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:18:05.854+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 WARN Tasks: Retrying task after failure: Version 82 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v82.metadata.json
[2025-07-18T16:18:05.854+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 82 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v82.metadata.json
[2025-07-18T16:18:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:18:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:18:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:18:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:18:05.858+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:18:05.858+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:18:05.859+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:18:05.860+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:18:05.861+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:18:05.861+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:18:05.862+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:18:05.862+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:18:05.862+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:18:05.863+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:18:05.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:18:05.866+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:18:05.867+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:18:05.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:18:05.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:18:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:18:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:18:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:18:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:18:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:18:05.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:18:05.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:18:05.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:18:05.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:18:05.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:18:05.884+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.888+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.890+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.892+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.892+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.893+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.893+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.894+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:18:05.894+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.898+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.898+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.900+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:18:05.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:18:05.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:18:05.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:18:05.903+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.903+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.903+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:18:05.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:18:05.905+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:18:05.909+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:18:05.909+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.2.b34d854d-c4a7-44e4-8172-28e3921c45b0.tmp
[2025-07-18T16:18:05.910+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 77cb57a6bd53:35301 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:18:05.910+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 77cb57a6bd53:35301 in memory (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:18:05.911+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.911+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 77cb57a6bd53:35301 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.914+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.2.b34d854d-c4a7-44e4-8172-28e3921c45b0.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/2
[2025-07-18T16:18:05.915+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855485840,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:05.916+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.923+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 77cb57a6bd53:35301 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.925+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.925+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.925+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.927+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.930+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 77cb57a6bd53:35301 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.935+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.936+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.937+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.940+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.947+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.947+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.968+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.970+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.972+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.972+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:05.973+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:06.012+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:06.014+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:18:06.015+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:06.016+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkContext: Created broadcast 16 from start at <unknown>:0
[2025-07-18T16:18:06.017+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:06.018+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:06.020+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:06.021+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
[2025-07-18T16:18:06.022+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:06.022+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:06.023+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:06.023+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:18:06.023+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.2 MiB)
[2025-07-18T16:18:06.024+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 77cb57a6bd53:35301 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:18:06.025+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:06.025+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:06.025+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-18T16:18:06.026+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:18:06.031+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-07-18T16:18:06.041+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:06.041+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=127 untilOffset=129, for query queryId=adf70716-4e16-4ce5-aa2c-d7a1cbea446a batchId=2 taskId=8 partitionId=0
[2025-07-18T16:18:06.048+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v74.metadata.json
[2025-07-18T16:18:06.059+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:18:06.082+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:18:06.083+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 35940291 nanos.
[2025-07-18T16:18:06.084+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4772 bytes result sent to driver
[2025-07-18T16:18:06.085+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 60 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:06.085+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-18T16:18:06.086+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.070 s
[2025-07-18T16:18:06.087+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:06.087+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-18T16:18:06.088+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.071885 s
[2025-07-18T16:18:06.088+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkWrite: Committing epoch 2 for query adf70716-4e16-4ce5-aa2c-d7a1cbea446a in append mode
[2025-07-18T16:18:06.122+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v83.metadata.json
[2025-07-18T16:18:06.127+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:06.131+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SnapshotProducer: Committed snapshot 194696519363548834 (FastAppend)
[2025-07-18T16:18:06.177+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SnapshotProducer: Committed snapshot 3713089131417569903 (FastAppend)
[2025-07-18T16:18:06.188+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=194696519363548834, sequenceNumber=73, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.815187958S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=73}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=602}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3013}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=230056}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:06.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkWrite: Committed in 815 ms
[2025-07-18T16:18:06.190+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:18:06.200+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/2 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.2.a70448b7-0e17-4c0b-8d5b-1d8316eeb972.tmp
[2025-07-18T16:18:06.232+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.2.a70448b7-0e17-4c0b-8d5b-1d8316eeb972.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/2
[2025-07-18T16:18:06.236+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:06.237+0000] {subprocess.py:93} INFO -   "id" : "1ba8749c-1701-47e7-880c-fbc4a0b916d5",
[2025-07-18T16:18:06.238+0000] {subprocess.py:93} INFO -   "runId" : "1469f87c-608e-4555-a6fe-7c0ce410c11f",
[2025-07-18T16:18:06.238+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:06.238+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:04.899Z",
[2025-07-18T16:18:06.238+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:18:06.238+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.7304601899196493,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.5015015015015014,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -     "addBatch" : 1114,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -     "commitOffsets" : 45,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -     "latestOffset" : 21,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -     "queryPlanning" : 31,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1332,
[2025-07-18T16:18:06.239+0000] {subprocess.py:93} INFO -     "walCommit" : 119
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.240+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.241+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:06.241+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:06.241+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.241+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.241+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.242+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:06.243+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:06.244+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.245+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.245+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.245+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:18:06.245+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.7304601899196493,
[2025-07-18T16:18:06.245+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.5015015015015014,
[2025-07-18T16:18:06.245+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:06.245+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:06.246+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:06.246+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:06.246+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:06.246+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:06.247+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:06.247+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:18:06.247+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:18:06.247+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:06.247+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:06.247+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=3713089131417569903, sequenceNumber=82, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.6799635S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=82}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=603}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2979}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=257520}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:06.247+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkWrite: Committed in 680 ms
[2025-07-18T16:18:06.248+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:18:06.254+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/2 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.2.0b3b127f-8d4c-4f4b-b0e3-b065d846d2f1.tmp
[2025-07-18T16:18:06.269+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 WARN Tasks: Retrying task after failure: Version 84 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v84.metadata.json
[2025-07-18T16:18:06.271+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 84 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v84.metadata.json
[2025-07-18T16:18:06.272+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:18:06.272+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:18:06.274+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:18:06.276+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:18:06.277+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:18:06.278+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:18:06.282+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:18:06.282+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:18:06.282+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:18:06.282+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:18:06.282+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:18:06.282+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:18:06.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:06.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:06.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:18:06.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:18:06.287+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:06.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:06.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:06.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:06.289+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:18:06.289+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:18:06.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:18:06.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:18:06.290+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:06.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:06.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:18:06.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:18:06.290+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:06.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:18:06.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:18:06.291+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.2.0b3b127f-8d4c-4f4b-b0e3-b065d846d2f1.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/2
[2025-07-18T16:18:06.297+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:06.298+0000] {subprocess.py:93} INFO -   "id" : "14ee97d4-8b20-40a2-a8d4-01491461c78b",
[2025-07-18T16:18:06.299+0000] {subprocess.py:93} INFO -   "runId" : "1c689d4b-5499-4b62-8616-1fa50e13ce42",
[2025-07-18T16:18:06.299+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:06.299+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:05.055Z",
[2025-07-18T16:18:06.299+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:18:06.300+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:18:06.300+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.8756567425569177,
[2025-07-18T16:18:06.301+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.6233766233766234,
[2025-07-18T16:18:06.301+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:06.301+0000] {subprocess.py:93} INFO -     "addBatch" : 1056,
[2025-07-18T16:18:06.302+0000] {subprocess.py:93} INFO -     "commitOffsets" : 41,
[2025-07-18T16:18:06.303+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:06.303+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:18:06.304+0000] {subprocess.py:93} INFO -     "queryPlanning" : 58,
[2025-07-18T16:18:06.304+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1232,
[2025-07-18T16:18:06.304+0000] {subprocess.py:93} INFO -     "walCommit" : 72
[2025-07-18T16:18:06.305+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:06.305+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:06.306+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:06.306+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:18:06.306+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:06.307+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:06.307+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:06.307+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.307+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.308+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:06.311+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:06.311+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.312+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.312+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.313+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:06.313+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:06.313+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.8756567425569177,
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.6233766233766234,
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:06.314+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:06.315+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:06.315+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:06.315+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:06.315+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:18:06.315+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:18:06.316+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:06.316+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:06.451+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v85.metadata.json
[2025-07-18T16:18:06.514+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SnapshotProducer: Committed snapshot 6445817042677060942 (FastAppend)
[2025-07-18T16:18:06.583+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=6445817042677060942, sequenceNumber=84, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.447423375S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=84}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=603}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2922}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=267570}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:06.584+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO SparkWrite: Committed in 449 ms
[2025-07-18T16:18:06.585+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:18:06.588+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/2 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.2.2d37ca96-8141-415d-b4ae-3f4482c6fa8e.tmp
[2025-07-18T16:18:06.650+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.2.2d37ca96-8141-415d-b4ae-3f4482c6fa8e.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/2
[2025-07-18T16:18:06.651+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:06.652+0000] {subprocess.py:93} INFO -   "id" : "adf70716-4e16-4ce5-aa2c-d7a1cbea446a",
[2025-07-18T16:18:06.652+0000] {subprocess.py:93} INFO -   "runId" : "c457c2da-f6de-43bf-bc48-5cb636f17f98",
[2025-07-18T16:18:06.652+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:06.653+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:05.807Z",
[2025-07-18T16:18:06.653+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:18:06.653+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:18:06.653+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.8278145695364238,
[2025-07-18T16:18:06.653+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.375296912114014,
[2025-07-18T16:18:06.653+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:06.654+0000] {subprocess.py:93} INFO -     "addBatch" : 646,
[2025-07-18T16:18:06.654+0000] {subprocess.py:93} INFO -     "commitOffsets" : 73,
[2025-07-18T16:18:06.654+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:06.654+0000] {subprocess.py:93} INFO -     "latestOffset" : 32,
[2025-07-18T16:18:06.654+0000] {subprocess.py:93} INFO -     "queryPlanning" : 16,
[2025-07-18T16:18:06.654+0000] {subprocess.py:93} INFO -     "triggerExecution" : 842,
[2025-07-18T16:18:06.655+0000] {subprocess.py:93} INFO -     "walCommit" : 73
[2025-07-18T16:18:06.655+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:06.655+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:06.655+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:06.655+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:18:06.655+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:06.655+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:06.656+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.8278145695364238,
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.375296912114014,
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:06.657+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:06.658+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:06.658+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:06.658+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:06.658+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:18:06.658+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:18:06.658+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:06.658+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:16.272+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:16.299+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:16.682+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:19.571+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 77cb57a6bd53:35301 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:18:19.577+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:26.263+0000] {subprocess.py:93} INFO - 25/07/18 16:18:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:26.300+0000] {subprocess.py:93} INFO - 25/07/18 16:18:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:26.674+0000] {subprocess.py:93} INFO - 25/07/18 16:18:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:36.260+0000] {subprocess.py:93} INFO - 25/07/18 16:18:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:36.303+0000] {subprocess.py:93} INFO - 25/07/18 16:18:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:36.675+0000] {subprocess.py:93} INFO - 25/07/18 16:18:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:46.269+0000] {subprocess.py:93} INFO - 25/07/18 16:18:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:46.309+0000] {subprocess.py:93} INFO - 25/07/18 16:18:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:46.680+0000] {subprocess.py:93} INFO - 25/07/18 16:18:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:50.957+0000] {subprocess.py:93} INFO - 25/07/18 16:18:50 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:50.963+0000] {subprocess.py:93} INFO - 25/07/18 16:18:50 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:18:56.273+0000] {subprocess.py:93} INFO - 25/07/18 16:18:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:56.308+0000] {subprocess.py:93} INFO - 25/07/18 16:18:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:56.685+0000] {subprocess.py:93} INFO - 25/07/18 16:18:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:06.272+0000] {subprocess.py:93} INFO - 25/07/18 16:19:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:06.307+0000] {subprocess.py:93} INFO - 25/07/18 16:19:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:06.695+0000] {subprocess.py:93} INFO - 25/07/18 16:19:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:16.278+0000] {subprocess.py:93} INFO - 25/07/18 16:19:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:16.316+0000] {subprocess.py:93} INFO - 25/07/18 16:19:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:16.697+0000] {subprocess.py:93} INFO - 25/07/18 16:19:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:26.286+0000] {subprocess.py:93} INFO - 25/07/18 16:19:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:26.321+0000] {subprocess.py:93} INFO - 25/07/18 16:19:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:26.699+0000] {subprocess.py:93} INFO - 25/07/18 16:19:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:36.296+0000] {subprocess.py:93} INFO - 25/07/18 16:19:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:36.334+0000] {subprocess.py:93} INFO - 25/07/18 16:19:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:36.702+0000] {subprocess.py:93} INFO - 25/07/18 16:19:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:46.299+0000] {subprocess.py:93} INFO - 25/07/18 16:19:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:46.344+0000] {subprocess.py:93} INFO - 25/07/18 16:19:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:46.709+0000] {subprocess.py:93} INFO - 25/07/18 16:19:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:56.308+0000] {subprocess.py:93} INFO - 25/07/18 16:19:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:56.344+0000] {subprocess.py:93} INFO - 25/07/18 16:19:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:56.708+0000] {subprocess.py:93} INFO - 25/07/18 16:19:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:04.200+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/3 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.3.b1cfe5b0-7d83-4170-84cd-7d826b38cdb9.tmp
[2025-07-18T16:20:04.248+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.3.b1cfe5b0-7d83-4170-84cd-7d826b38cdb9.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/3
[2025-07-18T16:20:04.249+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855604183,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:04.297+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.301+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.304+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.306+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.307+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.320+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.321+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.323+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.325+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.332+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.354+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.357+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.363+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.370+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.397+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:20:04.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:04.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Created broadcast 18 from start at <unknown>:0
[2025-07-18T16:20:04.438+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:04.439+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:04.446+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:04.449+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Final stage: ResultStage 9 (start at <unknown>:0)
[2025-07-18T16:20:04.450+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:04.453+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:04.456+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:04.508+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.531+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.531+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 77cb57a6bd53:35301 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:04.539+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:04.539+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:04.540+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-18T16:20:04.545+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:20:04.560+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2025-07-18T16:20:04.650+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:04.652+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=129 untilOffset=130, for query queryId=1ba8749c-1701-47e7-880c-fbc4a0b916d5 batchId=3 taskId=9 partitionId=0
[2025-07-18T16:20:04.672+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to offset 129 for partition reservations-0
[2025-07-18T16:20:04.684+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:20:04.819+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/3 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.3.8396e5f2-5a96-4721-b660-e06fe3d7db11.tmp
[2025-07-18T16:20:04.902+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.3.8396e5f2-5a96-4721-b660-e06fe3d7db11.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/3
[2025-07-18T16:20:04.905+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855604794,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:04.919+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.920+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.921+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.926+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.939+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.966+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.968+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.969+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.976+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.979+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.998+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:05.015+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.020+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.059+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:20:05.077+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.081+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.084+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 20 from start at <unknown>:0
[2025-07-18T16:20:05.086+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:05.092+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:05.092+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:05.096+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.103+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.103+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 77cb57a6bd53:35301 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.103+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:05.103+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:05.103+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-18T16:20:05.111+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:05.119+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2025-07-18T16:20:05.132+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:05.138+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=129 untilOffset=130, for query queryId=14ee97d4-8b20-40a2-a8d4-01491461c78b batchId=3 taskId=10 partitionId=0
[2025-07-18T16:20:05.140+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to offset 129 for partition checkins-0
[2025-07-18T16:20:05.148+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:20:05.207+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.218+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:20:05.220+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.221+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.221+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:20:05.224+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.225+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T16:20:05.231+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T16:20:05.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T16:20:05.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor read 1 records through 1 polls (polled  out 3 records), taking 531003042 nanos, during time span of 645301542 nanos.
[2025-07-18T16:20:05.311+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4696 bytes result sent to driver
[2025-07-18T16:20:05.315+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 774 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:05.315+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-18T16:20:05.315+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: ResultStage 9 (start at <unknown>:0) finished in 0.858 s
[2025-07-18T16:20:05.316+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:05.316+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-18T16:20:05.316+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.875769 s
[2025-07-18T16:20:05.321+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:05.322+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing epoch 3 for query 1ba8749c-1701-47e7-880c-fbc4a0b916d5 in append mode
[2025-07-18T16:20:05.322+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T16:20:05.322+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor read 1 records through 1 polls (polled  out 2 records), taking 62817875 nanos, during time span of 182384292 nanos.
[2025-07-18T16:20:05.327+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 4854 bytes result sent to driver
[2025-07-18T16:20:05.333+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 220 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:05.334+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-18T16:20:05.339+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.250 s
[2025-07-18T16:20:05.340+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:05.340+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-18T16:20:05.341+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.255854 s
[2025-07-18T16:20:05.341+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:05.341+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing epoch 3 for query 14ee97d4-8b20-40a2-a8d4-01491461c78b in append mode
[2025-07-18T16:20:05.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:05.447+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:05.448+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/3 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.3.0dc2452b-791b-4a09-b394-1b9eeecc01c8.tmp
[2025-07-18T16:20:05.543+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.3.0dc2452b-791b-4a09-b394-1b9eeecc01c8.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/3
[2025-07-18T16:20:05.545+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855605414,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:05.576+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.581+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.583+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.593+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.596+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.620+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.623+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.629+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.630+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.672+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.677+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.677+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.705+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.705+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.788+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.802+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:05.805+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.805+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 22 from start at <unknown>:0
[2025-07-18T16:20:05.812+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:05.819+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:05.826+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 WARN Tasks: Retrying task after failure: Version 85 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v85.metadata.json
[2025-07-18T16:20:05.826+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 85 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v85.metadata.json
[2025-07-18T16:20:05.826+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:05.826+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:05.827+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:05.828+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:05.828+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:05.828+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:05.829+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:05.829+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:05.830+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:05.830+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:05.831+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:05.832+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:05.834+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:05.836+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:05.837+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:05.841+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:05.843+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:05.845+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:05.851+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:05.852+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:05.853+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:05.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:05.860+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:05.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:05.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:05.866+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:05.867+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:05.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:05.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:05.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:05.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:05.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:05.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:05.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:05.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:05.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:05.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:05.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:05.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:05.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:05.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:05.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:05.883+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:05.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:05.889+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:05.890+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:05.893+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:05.897+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:05.899+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:05.900+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:05.900+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:05.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:05.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:05.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:05.905+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:05.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:05.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:05.907+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:05.908+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
[2025-07-18T16:20:05.908+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:05.909+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:05.909+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:05.910+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:05.911+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:20:05.912+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 77cb57a6bd53:35301 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.913+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:05.916+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:05.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-18T16:20:05.922+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:20:05.924+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2025-07-18T16:20:05.925+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 WARN Tasks: Retrying task after failure: Version 76 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v76.metadata.json
[2025-07-18T16:20:05.925+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 76 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v76.metadata.json
[2025-07-18T16:20:05.925+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:05.925+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:05.926+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:05.926+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:05.927+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:05.928+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:05.930+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:05.931+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:05.931+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:05.932+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:05.932+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:05.933+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:05.934+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:05.936+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:05.937+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:05.938+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:05.938+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:05.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:05.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:05.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:05.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:05.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:05.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:05.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:05.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:05.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:05.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:05.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:05.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:05.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:05.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:05.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:05.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:05.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:05.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:05.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:05.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:05.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:05.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:05.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:05.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:05.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:05.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:05.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:05.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:05.947+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:05.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:05.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:05.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:05.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:05.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:05.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:05.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:05.951+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:05.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:05.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:05.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:05.957+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:05.957+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:05.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:05.959+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:05.960+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=129 untilOffset=130, for query queryId=adf70716-4e16-4ce5-aa2c-d7a1cbea446a batchId=3 taskId=11 partitionId=0
[2025-07-18T16:20:05.960+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to offset 129 for partition feedback-0
[2025-07-18T16:20:05.960+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:20:06.364+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 WARN Tasks: Retrying task after failure: Version 86 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v86.metadata.json
[2025-07-18T16:20:06.366+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 86 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v86.metadata.json
[2025-07-18T16:20:06.367+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:06.369+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:06.372+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:06.375+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:06.376+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:06.383+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:06.384+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:06.385+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:06.387+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:06.389+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:06.389+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:06.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:06.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:06.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:06.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:06.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:06.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:06.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:06.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:06.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:06.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:06.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:06.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:06.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:06.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:06.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:06.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:06.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:06.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:06.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:06.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:06.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:06.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:06.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:06.414+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:06.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:06.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:06.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:06.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:06.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:06.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:06.418+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:06.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:06.419+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:06.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:06.420+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v77.metadata.json
[2025-07-18T16:20:06.433+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:06.434+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:20:06.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:06.468+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T16:20:06.497+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SnapshotProducer: Committed snapshot 9045336331753692605 (FastAppend)
[2025-07-18T16:20:06.540+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T16:20:06.540+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor read 1 records through 1 polls (polled  out 3 records), taking 520898750 nanos, during time span of 623299542 nanos.
[2025-07-18T16:20:06.543+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 4810 bytes result sent to driver
[2025-07-18T16:20:06.554+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 696 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:06.554+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-18T16:20:06.591+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.748 s
[2025-07-18T16:20:06.600+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:06.601+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-07-18T16:20:06.606+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.783530 s
[2025-07-18T16:20:06.606+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:06.607+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committing epoch 3 for query adf70716-4e16-4ce5-aa2c-d7a1cbea446a in append mode
[2025-07-18T16:20:06.710+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=9045336331753692605, sequenceNumber=76, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.2664545S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=76}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=733}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2969}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=241951}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:06.710+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committed in 1267 ms
[2025-07-18T16:20:06.710+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:20:06.744+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:06.773+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/3 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.3.b7862734-4a73-423f-8503-78999a21a4df.tmp
[2025-07-18T16:20:06.904+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.3.b7862734-4a73-423f-8503-78999a21a4df.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/3
[2025-07-18T16:20:06.906+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:06.908+0000] {subprocess.py:93} INFO -   "id" : "1ba8749c-1701-47e7-880c-fbc4a0b916d5",
[2025-07-18T16:20:06.908+0000] {subprocess.py:93} INFO -   "runId" : "1469f87c-608e-4555-a6fe-7c0ce410c11f",
[2025-07-18T16:20:06.910+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:06.915+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:04.181Z",
[2025-07-18T16:20:06.915+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:20:06.918+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:06.919+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T16:20:06.920+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.3675119441381845,
[2025-07-18T16:20:06.923+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:06.923+0000] {subprocess.py:93} INFO -     "addBatch" : 2407,
[2025-07-18T16:20:06.924+0000] {subprocess.py:93} INFO -     "commitOffsets" : 190,
[2025-07-18T16:20:06.924+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:06.927+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:20:06.928+0000] {subprocess.py:93} INFO -     "queryPlanning" : 54,
[2025-07-18T16:20:06.928+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2721,
[2025-07-18T16:20:06.929+0000] {subprocess.py:93} INFO -     "walCommit" : 64
[2025-07-18T16:20:06.929+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:06.929+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:06.930+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:06.930+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:20:06.932+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:06.933+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:06.950+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:06.950+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.953+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.953+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:06.954+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:06.954+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:06.954+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.954+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.957+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:06.958+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:06.958+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:06.958+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.958+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.958+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:06.958+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T16:20:06.959+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.3675119441381845,
[2025-07-18T16:20:06.959+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:06.959+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:06.960+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:06.961+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:06.962+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:06.965+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:06.965+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:06.965+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:20:06.966+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:06.966+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:06.966+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:06.966+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/4 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.4.20496560-f2f1-4d34-9447-386514bb8659.tmp
[2025-07-18T16:20:07.011+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v87.metadata.json
[2025-07-18T16:20:07.019+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.4.20496560-f2f1-4d34-9447-386514bb8659.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/4
[2025-07-18T16:20:07.021+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855606908,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:07.054+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.054+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.054+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.058+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:07.071+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:07.122+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.123+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.127+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.141+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:07.183+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:07.209+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 WARN Tasks: Retrying task after failure: Version 87 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v87.metadata.json
[2025-07-18T16:20:07.210+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 87 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v87.metadata.json
[2025-07-18T16:20:07.213+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:07.220+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:07.225+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:07.229+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:07.230+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:07.230+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:07.230+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:07.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:07.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:07.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:07.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:07.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:07.235+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:07.242+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:07.244+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:07.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:07.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:07.253+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.253+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.256+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:07.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:07.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:07.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.277+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.278+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:07.287+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.289+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:07.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:07.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:07.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:07.304+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SnapshotProducer: Committed snapshot 5740830049385422214 (FastAppend)
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T16:20:07.310+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:20:07.310+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkContext: Created broadcast 24 from start at <unknown>:0
[2025-07-18T16:20:07.310+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:07.310+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:07.310+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:07.316+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Final stage: ResultStage 12 (start at <unknown>:0)
[2025-07-18T16:20:07.323+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:07.324+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:07.338+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:07.340+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T16:20:07.341+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T16:20:07.343+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 77cb57a6bd53:35301 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:20:07.345+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:07.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:07.353+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-07-18T16:20:07.365+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:20:07.369+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2025-07-18T16:20:07.431+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:07.431+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=130 untilOffset=132, for query queryId=1ba8749c-1701-47e7-880c-fbc4a0b916d5 batchId=4 taskId=12 partitionId=0
[2025-07-18T16:20:07.450+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T16:20:07.557+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=5740830049385422214, sequenceNumber=86, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT2.030320667S, count=1}, attempts=CounterResult{unit=COUNT, value=3}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=86}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=735}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2876}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=273735}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:07.559+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committed in 2032 ms
[2025-07-18T16:20:07.565+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:20:07.577+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T16:20:07.578+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 124325959 nanos.
[2025-07-18T16:20:07.586+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 4696 bytes result sent to driver
[2025-07-18T16:20:07.587+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 276 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:07.588+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-07-18T16:20:07.589+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: ResultStage 12 (start at <unknown>:0) finished in 0.290 s
[2025-07-18T16:20:07.592+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:07.593+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-07-18T16:20:07.595+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.305810 s
[2025-07-18T16:20:07.599+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:07.605+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committing epoch 4 for query 1ba8749c-1701-47e7-880c-fbc4a0b916d5 in append mode
[2025-07-18T16:20:07.610+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/3 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.3.aa31527a-394f-443d-a881-88e0aed5080b.tmp
[2025-07-18T16:20:07.675+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.3.aa31527a-394f-443d-a881-88e0aed5080b.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/3
[2025-07-18T16:20:07.677+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:07.677+0000] {subprocess.py:93} INFO -   "id" : "14ee97d4-8b20-40a2-a8d4-01491461c78b",
[2025-07-18T16:20:07.678+0000] {subprocess.py:93} INFO -   "runId" : "1c689d4b-5499-4b62-8616-1fa50e13ce42",
[2025-07-18T16:20:07.678+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:07.680+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:04.786Z",
[2025-07-18T16:20:07.682+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:20:07.683+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:07.683+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:20:07.683+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.34638032559750603,
[2025-07-18T16:20:07.683+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:07.685+0000] {subprocess.py:93} INFO -     "addBatch" : 2534,
[2025-07-18T16:20:07.688+0000] {subprocess.py:93} INFO -     "commitOffsets" : 199,
[2025-07-18T16:20:07.688+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:07.689+0000] {subprocess.py:93} INFO -     "latestOffset" : 8,
[2025-07-18T16:20:07.689+0000] {subprocess.py:93} INFO -     "queryPlanning" : 38,
[2025-07-18T16:20:07.689+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2887,
[2025-07-18T16:20:07.689+0000] {subprocess.py:93} INFO -     "walCommit" : 107
[2025-07-18T16:20:07.690+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:07.690+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:07.691+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:07.693+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:20:07.693+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:07.694+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:07.694+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:07.695+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:07.698+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:07.700+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:07.701+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:07.702+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:07.703+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:07.706+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:07.706+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:07.707+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:07.708+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:07.708+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:07.709+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:07.710+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:07.712+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:20:07.714+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.34638032559750603,
[2025-07-18T16:20:07.716+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:07.716+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:07.717+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:07.718+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:07.718+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:07.719+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:07.720+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:07.720+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:20:07.721+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:07.721+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:07.722+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:07.730+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.789+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/4 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.4.689092c9-7bfd-4be6-8885-9d168456046a.tmp
[2025-07-18T16:20:07.904+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v88.metadata.json
[2025-07-18T16:20:07.943+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.4.689092c9-7bfd-4be6-8885-9d168456046a.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/4
[2025-07-18T16:20:07.951+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855607703,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:08.033+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.034+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.038+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.104+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.106+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.156+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.157+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.158+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.159+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.160+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.184+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.186+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.187+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.202+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SnapshotProducer: Committed snapshot 8336135979636389882 (FastAppend)
[2025-07-18T16:20:08.216+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.229+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.243+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 WARN Tasks: Retrying task after failure: Version 79 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v79.metadata.json
[2025-07-18T16:20:08.243+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 79 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v79.metadata.json
[2025-07-18T16:20:08.243+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:08.243+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:08.243+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:08.244+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:08.244+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:08.244+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:08.244+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:08.245+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:08.246+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:08.247+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:08.247+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:08.247+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:08.248+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:08.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:08.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:08.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:08.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:08.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:08.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:08.251+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:08.252+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:08.255+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:08.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:08.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:08.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:08.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:08.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:08.262+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:08.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:08.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:08.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:08.269+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:08.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:08.270+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:08.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:08.307+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:20:08.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T16:20:08.319+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:20:08.319+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkContext: Created broadcast 26 from start at <unknown>:0
[2025-07-18T16:20:08.321+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:08.322+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:08.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:08.440+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
[2025-07-18T16:20:08.442+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:08.444+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:08.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:08.446+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T16:20:08.448+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T16:20:08.449+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 77cb57a6bd53:35301 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:20:08.452+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:08.453+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:08.453+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-07-18T16:20:08.454+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:08.455+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2025-07-18T16:20:08.456+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:08.457+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=130 untilOffset=132, for query queryId=14ee97d4-8b20-40a2-a8d4-01491461c78b batchId=4 taskId=13 partitionId=0
[2025-07-18T16:20:08.621+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to offset 131 for partition checkins-0
[2025-07-18T16:20:08.630+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:20:08.650+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=8336135979636389882, sequenceNumber=87, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.908898501S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=87}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=734}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2934}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=282551}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:08.651+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Committed in 1923 ms
[2025-07-18T16:20:08.651+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:20:08.713+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/3 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.3.ba7e67d4-98dd-41c9-96be-b08710a2b79b.tmp
[2025-07-18T16:20:08.818+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.3.ba7e67d4-98dd-41c9-96be-b08710a2b79b.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/3
[2025-07-18T16:20:08.820+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:08.820+0000] {subprocess.py:93} INFO -   "id" : "adf70716-4e16-4ce5-aa2c-d7a1cbea446a",
[2025-07-18T16:20:08.821+0000] {subprocess.py:93} INFO -   "runId" : "c457c2da-f6de-43bf-bc48-5cb636f17f98",
[2025-07-18T16:20:08.822+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:08.823+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:05.409Z",
[2025-07-18T16:20:08.824+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:20:08.825+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:08.828+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:20:08.830+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.2934272300469484,
[2025-07-18T16:20:08.831+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:08.831+0000] {subprocess.py:93} INFO -     "addBatch" : 3053,
[2025-07-18T16:20:08.832+0000] {subprocess.py:93} INFO -     "commitOffsets" : 166,
[2025-07-18T16:20:08.832+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:08.832+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:20:08.833+0000] {subprocess.py:93} INFO -     "queryPlanning" : 45,
[2025-07-18T16:20:08.833+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3408,
[2025-07-18T16:20:08.834+0000] {subprocess.py:93} INFO -     "walCommit" : 128
[2025-07-18T16:20:08.836+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:08.837+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:08.838+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:08.839+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:20:08.839+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:08.839+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.840+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:08.840+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.841+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.841+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:08.842+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.842+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:08.843+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.843+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.843+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:08.845+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.846+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:08.849+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.850+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.851+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:08.851+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:20:08.852+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.2934272300469484,
[2025-07-18T16:20:08.852+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:08.852+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:08.853+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:08.854+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:08.854+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:08.855+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:08.856+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:08.858+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:20:08.862+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:08.865+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:08.865+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:08.866+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/4 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.4.24c0f485-012d-48be-9443-5953d757b729.tmp
[2025-07-18T16:20:08.907+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.4.24c0f485-012d-48be-9443-5953d757b729.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/4
[2025-07-18T16:20:08.908+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855608825,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:08.945+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.949+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.955+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.956+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.956+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.968+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.971+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.971+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.973+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v80.metadata.json
[2025-07-18T16:20:08.995+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.998+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.019+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.019+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.020+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.059+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.071+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:20:09.072+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.086+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 77cb57a6bd53:35301 in memory (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:20:09.095+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.113+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 77cb57a6bd53:35301 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.122+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 77cb57a6bd53:35301 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.133+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:09.135+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:20:09.137+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:09.137+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T16:20:09.144+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.145+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:20:09.151+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 77cb57a6bd53:35301 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.154+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:09.171+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.172+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Created broadcast 28 from start at <unknown>:0
[2025-07-18T16:20:09.179+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:09.181+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:09.181+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:09.182+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Final stage: ResultStage 14 (start at <unknown>:0)
[2025-07-18T16:20:09.182+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:09.183+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:09.184+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T16:20:09.186+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor read 2 records through 1 polls (polled  out 1 records), taking 535662041 nanos, during time span of 573664542 nanos.
[2025-07-18T16:20:09.190+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:09.193+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:09.198+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 4905 bytes result sent to driver
[2025-07-18T16:20:09.200+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.1 MiB)
[2025-07-18T16:20:09.202+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 77cb57a6bd53:35301 (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.203+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 845 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:09.204+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-18T16:20:09.205+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:09.206+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:09.207+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2025-07-18T16:20:09.210+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.868 s
[2025-07-18T16:20:09.212+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:09.213+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-07-18T16:20:09.215+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.875963 s
[2025-07-18T16:20:09.217+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:09.219+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committing epoch 4 for query 14ee97d4-8b20-40a2-a8d4-01491461c78b in append mode
[2025-07-18T16:20:09.221+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:20:09.222+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2025-07-18T16:20:09.233+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SnapshotProducer: Committed snapshot 184664738713896651 (FastAppend)
[2025-07-18T16:20:09.240+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:09.249+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=130 untilOffset=132, for query queryId=adf70716-4e16-4ce5-aa2c-d7a1cbea446a batchId=4 taskId=14 partitionId=0
[2025-07-18T16:20:09.290+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.292+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T16:20:09.374+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T16:20:09.375+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 101651167 nanos.
[2025-07-18T16:20:09.411+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 4854 bytes result sent to driver
[2025-07-18T16:20:09.411+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 206 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:09.412+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-07-18T16:20:09.412+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: ResultStage 14 (start at <unknown>:0) finished in 0.235 s
[2025-07-18T16:20:09.413+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:09.413+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2025-07-18T16:20:09.413+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.240998 s
[2025-07-18T16:20:09.413+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:09.413+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committing epoch 4 for query adf70716-4e16-4ce5-aa2c-d7a1cbea446a in append mode
[2025-07-18T16:20:09.419+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=184664738713896651, sequenceNumber=79, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.686973668S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=79}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=738}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3050}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=251020}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:09.421+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committed in 1687 ms
[2025-07-18T16:20:09.422+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:20:09.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/4 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.4.712ad504-f7d3-4833-9ee5-79a31958c051.tmp
[2025-07-18T16:20:09.475+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.495+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.4.712ad504-f7d3-4833-9ee5-79a31958c051.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/4
[2025-07-18T16:20:09.503+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:09.503+0000] {subprocess.py:93} INFO -   "id" : "1ba8749c-1701-47e7-880c-fbc4a0b916d5",
[2025-07-18T16:20:09.504+0000] {subprocess.py:93} INFO -   "runId" : "1469f87c-608e-4555-a6fe-7c0ce410c11f",
[2025-07-18T16:20:09.504+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:09.505+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:06.905Z",
[2025-07-18T16:20:09.506+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:20:09.506+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:09.507+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.7342143906020557,
[2025-07-18T16:20:09.507+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.772499034376207,
[2025-07-18T16:20:09.507+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:09.507+0000] {subprocess.py:93} INFO -     "addBatch" : 2337,
[2025-07-18T16:20:09.508+0000] {subprocess.py:93} INFO -     "commitOffsets" : 76,
[2025-07-18T16:20:09.508+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:09.508+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:20:09.509+0000] {subprocess.py:93} INFO -     "queryPlanning" : 58,
[2025-07-18T16:20:09.509+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2589,
[2025-07-18T16:20:09.509+0000] {subprocess.py:93} INFO -     "walCommit" : 114
[2025-07-18T16:20:09.510+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:09.510+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:09.511+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:09.511+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:20:09.512+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:09.512+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:09.512+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.513+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:09.514+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:09.514+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:09.514+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.514+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.514+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:09.518+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.7342143906020557,
[2025-07-18T16:20:09.518+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.772499034376207,
[2025-07-18T16:20:09.518+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:09.518+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:09.520+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:09.520+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:09.520+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:09.522+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:09.522+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:09.524+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:20:09.525+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:09.526+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:09.526+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:09.609+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 77cb57a6bd53:35301 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.674+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 WARN Tasks: Retrying task after failure: Failed to commit changes using rename: s3a://warehouse/bronze/Checkins_raw/metadata/v90.metadata.json
[2025-07-18T16:20:09.676+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Failed to commit changes using rename: s3a://warehouse/bronze/Checkins_raw/metadata/v90.metadata.json
[2025-07-18T16:20:09.676+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:378)
[2025-07-18T16:20:09.676+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:09.676+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:09.677+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:09.677+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:09.682+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:09.683+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:09.684+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:09.684+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:09.685+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:09.685+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:09.686+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:09.687+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:09.688+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:09.689+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:09.689+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:09.690+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:09.691+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:09.691+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:09.691+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:09.692+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:09.693+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:09.694+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:09.694+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:09.694+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:09.695+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:09.696+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:09.696+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:09.697+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:09.698+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:09.698+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:09.700+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:09.700+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:09.701+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:09.702+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:09.702+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:09.703+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:09.704+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:09.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:09.706+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:09.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:09.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:09.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:09.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:09.709+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:09.709+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:09.709+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:09.710+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:09.711+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:09.711+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:09.711+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:09.712+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:09.712+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:09.713+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:09.713+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:09.714+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:09.714+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:09.715+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:09.715+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:09.716+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:09.716+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Failed to rename s3a://warehouse/bronze/Checkins_raw/metadata/6d12813e-ae37-484c-8c56-9fecb6b83c0c.metadata.json to s3a://warehouse/bronze/Checkins_raw/metadata/v90.metadata.json; destination file exists
[2025-07-18T16:20:09.717+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initiateRename(S3AFileSystem.java:1920)
[2025-07-18T16:20:09.718+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerRename(S3AFileSystem.java:1988)
[2025-07-18T16:20:09.719+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$rename$7(S3AFileSystem.java:1846)
[2025-07-18T16:20:09.719+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2025-07-18T16:20:09.720+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2025-07-18T16:20:09.721+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2025-07-18T16:20:09.721+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:1844)
[2025-07-18T16:20:09.722+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:368)
[2025-07-18T16:20:09.723+0000] {subprocess.py:93} INFO - 	... 59 more
[2025-07-18T16:20:09.933+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v91.metadata.json
[2025-07-18T16:20:10.293+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SnapshotProducer: Committed snapshot 2171961578535854355 (FastAppend)
[2025-07-18T16:20:10.392+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 WARN SnapshotProducer: Failed to load committed snapshot, skipping manifest clean-up
[2025-07-18T16:20:10.395+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 WARN SnapshotProducer: Failed to notify listeners
[2025-07-18T16:20:10.396+0000] {subprocess.py:93} INFO - java.lang.NullPointerException: Cannot invoke "org.apache.iceberg.Snapshot.sequenceNumber()" because "snapshot" is null
[2025-07-18T16:20:10.397+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.FastAppend.updateEvent(FastAppend.java:174)
[2025-07-18T16:20:10.398+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.notifyListeners(SnapshotProducer.java:449)
[2025-07-18T16:20:10.399+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:441)
[2025-07-18T16:20:10.399+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:10.399+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:10.399+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:10.400+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:10.400+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:10.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:10.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:10.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:10.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:10.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:10.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:10.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:10.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:10.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:10.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:10.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:10.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:10.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:10.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:10.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:10.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:10.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:10.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:10.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:10.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:10.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:10.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:10.408+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:10.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:10.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:10.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:10.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:10.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:10.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:10.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:10.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:10.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:10.415+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:10.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:10.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:10.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:10.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:10.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:10.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:10.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:10.420+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:10.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:10.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:10.422+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:10.422+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:10.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:10.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:10.423+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SparkWrite: Committed in 913 ms
[2025-07-18T16:20:10.423+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:20:10.424+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/4 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.4.aaa09fce-cbad-4136-9d66-b97c881e3f10.tmp
[2025-07-18T16:20:10.424+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v91.metadata.json
[2025-07-18T16:20:10.449+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.4.aaa09fce-cbad-4136-9d66-b97c881e3f10.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/4
[2025-07-18T16:20:10.464+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:10.465+0000] {subprocess.py:93} INFO -   "id" : "adf70716-4e16-4ce5-aa2c-d7a1cbea446a",
[2025-07-18T16:20:10.466+0000] {subprocess.py:93} INFO -   "runId" : "c457c2da-f6de-43bf-bc48-5cb636f17f98",
[2025-07-18T16:20:10.466+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:10.466+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:08.822Z",
[2025-07-18T16:20:10.467+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:20:10.469+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:10.471+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.5859947260474656,
[2025-07-18T16:20:10.472+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.2307692307692308,
[2025-07-18T16:20:10.472+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:10.473+0000] {subprocess.py:93} INFO -     "addBatch" : 1430,
[2025-07-18T16:20:10.473+0000] {subprocess.py:93} INFO -     "commitOffsets" : 58,
[2025-07-18T16:20:10.474+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:20:10.475+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:20:10.477+0000] {subprocess.py:93} INFO -     "queryPlanning" : 57,
[2025-07-18T16:20:10.478+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1625,
[2025-07-18T16:20:10.480+0000] {subprocess.py:93} INFO -     "walCommit" : 71
[2025-07-18T16:20:10.480+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:10.481+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:10.482+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:10.482+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:20:10.483+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:10.483+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:10.483+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:10.484+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.484+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.485+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:10.486+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:10.486+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:10.487+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.487+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.487+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:10.487+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:10.487+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:10.488+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.488+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.489+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:10.489+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.5859947260474656,
[2025-07-18T16:20:10.490+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.2307692307692308,
[2025-07-18T16:20:10.495+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:10.496+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:10.497+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:10.497+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:10.498+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:10.498+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:10.498+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:10.498+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:20:10.498+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:10.498+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:10.498+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:10.517+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SnapshotProducer: Committed snapshot 4787622061924900056 (FastAppend)
[2025-07-18T16:20:10.632+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4787622061924900056, sequenceNumber=90, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.338781001S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=90}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=742}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2894}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=285293}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:10.633+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SparkWrite: Committed in 1342 ms
[2025-07-18T16:20:10.633+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:20:10.664+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/4 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.4.d8508216-b889-44a1-89a6-b11f2038b03e.tmp
[2025-07-18T16:20:10.763+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.4.d8508216-b889-44a1-89a6-b11f2038b03e.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/4
[2025-07-18T16:20:10.779+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:10.781+0000] {subprocess.py:93} INFO -   "id" : "14ee97d4-8b20-40a2-a8d4-01491461c78b",
[2025-07-18T16:20:10.782+0000] {subprocess.py:93} INFO -   "runId" : "1c689d4b-5499-4b62-8616-1fa50e13ce42",
[2025-07-18T16:20:10.783+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:10.784+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:07.675Z",
[2025-07-18T16:20:10.785+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:20:10.787+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:10.788+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.6922810661128419,
[2025-07-18T16:20:10.789+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6472491909385114,
[2025-07-18T16:20:10.789+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:10.792+0000] {subprocess.py:93} INFO -     "addBatch" : 2497,
[2025-07-18T16:20:10.795+0000] {subprocess.py:93} INFO -     "commitOffsets" : 130,
[2025-07-18T16:20:10.800+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:10.805+0000] {subprocess.py:93} INFO -     "latestOffset" : 28,
[2025-07-18T16:20:10.806+0000] {subprocess.py:93} INFO -     "queryPlanning" : 199,
[2025-07-18T16:20:10.810+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3090,
[2025-07-18T16:20:10.811+0000] {subprocess.py:93} INFO -     "walCommit" : 235
[2025-07-18T16:20:10.814+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:10.814+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:10.815+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:10.815+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:20:10.816+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:10.816+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:10.816+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:10.816+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.817+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.817+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:10.818+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:10.818+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:10.818+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.818+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.818+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:10.818+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:10.822+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:10.824+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.825+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.825+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:10.835+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.6922810661128419,
[2025-07-18T16:20:10.841+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6472491909385114,
[2025-07-18T16:20:10.844+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:10.846+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:10.847+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:10.848+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:10.850+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:10.851+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:10.855+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:10.866+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:20:10.870+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:10.872+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:10.873+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:19.516+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:20.448+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:20.768+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:25.307+0000] {subprocess.py:93} INFO - 25/07/18 16:20:25 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 77cb57a6bd53:35301 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T16:20:25.313+0000] {subprocess.py:93} INFO - 25/07/18 16:20:25 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:29.503+0000] {subprocess.py:93} INFO - 25/07/18 16:20:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:30.455+0000] {subprocess.py:93} INFO - 25/07/18 16:20:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:30.770+0000] {subprocess.py:93} INFO - 25/07/18 16:20:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:39.515+0000] {subprocess.py:93} INFO - 25/07/18 16:20:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:40.461+0000] {subprocess.py:93} INFO - 25/07/18 16:20:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:40.782+0000] {subprocess.py:93} INFO - 25/07/18 16:20:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:49.521+0000] {subprocess.py:93} INFO - 25/07/18 16:20:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:50.467+0000] {subprocess.py:93} INFO - 25/07/18 16:20:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:50.782+0000] {subprocess.py:93} INFO - 25/07/18 16:20:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:58.166+0000] {subprocess.py:93} INFO - 25/07/18 16:20:58 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:20:58.173+0000] {subprocess.py:93} INFO - 25/07/18 16:20:58 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:59.537+0000] {subprocess.py:93} INFO - 25/07/18 16:20:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:00.466+0000] {subprocess.py:93} INFO - 25/07/18 16:21:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:00.790+0000] {subprocess.py:93} INFO - 25/07/18 16:21:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:09.540+0000] {subprocess.py:93} INFO - 25/07/18 16:21:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:10.478+0000] {subprocess.py:93} INFO - 25/07/18 16:21:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:10.800+0000] {subprocess.py:93} INFO - 25/07/18 16:21:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:11.601+0000] {subprocess.py:93} INFO - 25/07/18 16:21:11 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
[2025-07-18T16:21:11.601+0000] {subprocess.py:93} INFO - 25/07/18 16:21:11 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2025-07-18T16:21:11.602+0000] {subprocess.py:93} INFO - 25/07/18 16:21:11 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
[2025-07-18T16:21:19.543+0000] {subprocess.py:93} INFO - 25/07/18 16:21:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:20.481+0000] {subprocess.py:93} INFO - 25/07/18 16:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:20.807+0000] {subprocess.py:93} INFO - 25/07/18 16:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:29.544+0000] {subprocess.py:93} INFO - 25/07/18 16:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:30.481+0000] {subprocess.py:93} INFO - 25/07/18 16:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:30.820+0000] {subprocess.py:93} INFO - 25/07/18 16:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:39.552+0000] {subprocess.py:93} INFO - 25/07/18 16:21:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:40.480+0000] {subprocess.py:93} INFO - 25/07/18 16:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:40.821+0000] {subprocess.py:93} INFO - 25/07/18 16:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:49.554+0000] {subprocess.py:93} INFO - 25/07/18 16:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:50.486+0000] {subprocess.py:93} INFO - 25/07/18 16:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:50.828+0000] {subprocess.py:93} INFO - 25/07/18 16:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:59.555+0000] {subprocess.py:93} INFO - 25/07/18 16:21:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:00.516+0000] {subprocess.py:93} INFO - 25/07/18 16:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:00.839+0000] {subprocess.py:93} INFO - 25/07/18 16:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:02.919+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/5 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.5.d29d4f7b-172d-4497-a440-374d6e0b1195.tmp
[2025-07-18T16:22:02.992+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.5.d29d4f7b-172d-4497-a440-374d6e0b1195.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/5
[2025-07-18T16:22:02.998+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752855722865,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.113+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.114+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.115+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.117+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.119+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.157+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.160+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.162+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.164+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.166+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.187+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.189+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.190+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.190+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.191+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.289+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:22:03.298+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.306+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:03.307+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 30 from start at <unknown>:0
[2025-07-18T16:22:03.307+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:03.323+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:03.327+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:03.341+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
[2025-07-18T16:22:03.343+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:03.343+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:03.343+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:03.354+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.371+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.379+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 77cb57a6bd53:35301 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:03.380+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:03.380+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:03.381+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-07-18T16:22:03.382+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:22:03.387+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2025-07-18T16:22:03.463+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:03.479+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=132 untilOffset=133, for query queryId=1ba8749c-1701-47e7-880c-fbc4a0b916d5 batchId=5 taskId=15 partitionId=0
[2025-07-18T16:22:03.546+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/5 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.5.1debc580-0602-490e-93e6-b04743ee7a6e.tmp
[2025-07-18T16:22:03.558+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to offset 132 for partition reservations-0
[2025-07-18T16:22:03.584+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:22:03.630+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.5.1debc580-0602-490e-93e6-b04743ee7a6e.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/5
[2025-07-18T16:22:03.636+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752855723482,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.667+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.669+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.672+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.674+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.676+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.692+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.694+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.695+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.695+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.696+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.706+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.706+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.706+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.706+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.708+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.738+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.754+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.756+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.757+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 32 from start at <unknown>:0
[2025-07-18T16:22:03.761+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:03.762+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:03.762+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:03.762+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Final stage: ResultStage 16 (start at <unknown>:0)
[2025-07-18T16:22:03.763+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:03.763+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:03.763+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:03.764+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.791+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.808+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 77cb57a6bd53:35301 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.809+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:03.812+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:03.813+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-07-18T16:22:03.817+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:03.817+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2025-07-18T16:22:03.843+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:03.845+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=132 untilOffset=133, for query queryId=14ee97d4-8b20-40a2-a8d4-01491461c78b batchId=5 taskId=16 partitionId=0
[2025-07-18T16:22:03.931+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to offset 132 for partition checkins-0
[2025-07-18T16:22:03.956+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:22:04.090+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.091+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:22:04.091+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor-3, groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.100+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T16:22:04.130+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/5 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.5.3d9650e8-03e4-48e6-a1cf-90e922d1ac01.tmp
[2025-07-18T16:22:04.186+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T16:22:04.195+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor read 1 records through 1 polls (polled  out 3 records), taking 531036417 nanos, during time span of 642817084 nanos.
[2025-07-18T16:22:04.197+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.5.3d9650e8-03e4-48e6-a1cf-90e922d1ac01.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/5
[2025-07-18T16:22:04.197+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752855724101,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:04.222+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 4764 bytes result sent to driver
[2025-07-18T16:22:04.232+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 850 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.235+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.237+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.238+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.238+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.239+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.891 s
[2025-07-18T16:22:04.239+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.240+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2025-07-18T16:22:04.240+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.244+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.245+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.927932 s
[2025-07-18T16:22:04.246+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.249+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 5 for query 1ba8749c-1701-47e7-880c-fbc4a0b916d5 in append mode
[2025-07-18T16:22:04.252+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.253+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.255+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.259+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.266+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.290+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.291+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.291+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.298+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.301+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:04.341+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:04.342+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:04.345+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 34 from start at <unknown>:0
[2025-07-18T16:22:04.346+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:04.346+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:04.347+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:04.348+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
[2025-07-18T16:22:04.349+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:04.349+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:04.349+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:04.360+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:04.368+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:04.373+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:22:04.374+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 77cb57a6bd53:35301 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:04.379+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:04.379+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:04.380+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-07-18T16:22:04.382+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:22:04.383+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2025-07-18T16:22:04.407+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:04.411+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=132 untilOffset=133, for query queryId=adf70716-4e16-4ce5-aa2c-d7a1cbea446a batchId=5 taskId=17 partitionId=0
[2025-07-18T16:22:04.436+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to offset 132 for partition feedback-0
[2025-07-18T16:22:04.449+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.460+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:22:04.467+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor-2, groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.473+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:22:04.571+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T16:22:04.625+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.625+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:22:04.631+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.632+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T16:22:04.734+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T16:22:04.735+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor read 1 records through 1 polls (polled  out 3 records), taking 541551750 nanos, during time span of 806243834 nanos.
[2025-07-18T16:22:04.747+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T16:22:04.768+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor read 1 records through 1 polls (polled  out 2 records), taking 190539500 nanos, during time span of 311751792 nanos.
[2025-07-18T16:22:04.778+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 4802 bytes result sent to driver
[2025-07-18T16:22:04.790+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 4865 bytes result sent to driver
[2025-07-18T16:22:04.791+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 409 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.795+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.799+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.446 s
[2025-07-18T16:22:04.800+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.801+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-07-18T16:22:04.805+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.455167 s
[2025-07-18T16:22:04.815+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.820+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 5 for query adf70716-4e16-4ce5-aa2c-d7a1cbea446a in append mode
[2025-07-18T16:22:04.840+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 1037 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.842+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.848+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 16 (start at <unknown>:0) finished in 1.080 s
[2025-07-18T16:22:04.857+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.858+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-07-18T16:22:04.858+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 1.083782 s
[2025-07-18T16:22:04.860+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.861+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 5 for query 14ee97d4-8b20-40a2-a8d4-01491461c78b in append mode
[2025-07-18T16:22:04.960+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.997+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 WARN Tasks: Retrying task after failure: Version 85 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v85.metadata.json
[2025-07-18T16:22:04.998+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 85 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v85.metadata.json
[2025-07-18T16:22:04.998+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:04.998+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:04.999+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:04.999+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:04.999+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:04.999+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.000+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.001+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.002+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.007+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.011+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.012+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.012+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:05.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:05.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:05.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:05.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:05.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:05.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:05.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:05.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:05.015+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:05.015+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:05.016+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:05.016+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:05.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:05.019+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:05.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:05.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:05.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.025+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.025+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.026+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.026+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.026+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.026+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.026+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:05.027+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:05.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:05.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:05.032+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:05.033+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.033+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:05.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:05.034+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:05.035+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:05.049+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:05.484+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.729+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SnapshotProducer: Committed snapshot 860612789940272708 (FastAppend)
[2025-07-18T16:22:05.734+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json
[2025-07-18T16:22:05.769+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.904+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=860612789940272708, sequenceNumber=93, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.848832543S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=93}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=877}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2881}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=298742}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:05.905+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committed in 850 ms
[2025-07-18T16:22:05.905+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:05.906+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SnapshotProducer: Committed snapshot 2732006097858416147 (FastAppend)
[2025-07-18T16:22:05.914+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/5 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.5.e0898fab-5388-48c9-ad6b-eb6d722a8c66.tmp
[2025-07-18T16:22:05.975+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.5.e0898fab-5388-48c9-ad6b-eb6d722a8c66.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/5
[2025-07-18T16:22:05.982+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:05.982+0000] {subprocess.py:93} INFO -   "id" : "14ee97d4-8b20-40a2-a8d4-01491461c78b",
[2025-07-18T16:22:05.984+0000] {subprocess.py:93} INFO -   "runId" : "1c689d4b-5499-4b62-8616-1fa50e13ce42",
[2025-07-18T16:22:05.988+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:05.992+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:03.481Z",
[2025-07-18T16:22:05.995+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T16:22:05.997+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:05.999+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T16:22:06.003+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.4020908725371934,
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO -     "addBatch" : 2219,
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO -     "commitOffsets" : 71,
[2025-07-18T16:22:06.005+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:22:06.007+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:22:06.008+0000] {subprocess.py:93} INFO -     "queryPlanning" : 54,
[2025-07-18T16:22:06.011+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2487,
[2025-07-18T16:22:06.014+0000] {subprocess.py:93} INFO -     "walCommit" : 141
[2025-07-18T16:22:06.017+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:06.018+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:06.019+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:06.030+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:06.032+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:06.042+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:06.045+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:06.045+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.045+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:06.047+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:06.047+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.048+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:06.053+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.053+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.056+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.057+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:06.057+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.4020908725371934,
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/6 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.6.e02512dd-198e-4733-ab00-14d10a83c0d1.tmp
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SnapshotProducer: Committed snapshot 1018111481054436872 (FastAppend)
[2025-07-18T16:22:06.076+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=2732006097858416147, sequenceNumber=85, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.708748043S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=85}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=877}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2920}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=271945}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:06.077+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Committed in 1710 ms
[2025-07-18T16:22:06.081+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:06.180+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/.6.e02512dd-198e-4733-ab00-14d10a83c0d1.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/offsets/6
[2025-07-18T16:22:06.181+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752855725990,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:06.183+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/5 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.5.29ebd85b-12ca-4b59-8cff-bb265d5d136f.tmp
[2025-07-18T16:22:06.236+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=1018111481054436872, sequenceNumber=93, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.273380875S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=93}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=874}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2713}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=306594}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:06.239+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Committed in 1275 ms
[2025-07-18T16:22:06.244+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:06.282+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/5 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.5.ebb7ce0d-bfe3-422b-988e-365294592c5c.tmp
[2025-07-18T16:22:06.292+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.293+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.293+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.297+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.5.29ebd85b-12ca-4b59-8cff-bb265d5d136f.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/5
[2025-07-18T16:22:06.345+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:06.346+0000] {subprocess.py:93} INFO -   "id" : "1ba8749c-1701-47e7-880c-fbc4a0b916d5",
[2025-07-18T16:22:06.346+0000] {subprocess.py:93} INFO -   "runId" : "1469f87c-608e-4555-a6fe-7c0ce410c11f",
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:02.862Z",
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.29120559114735,
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -     "addBatch" : 2959,
[2025-07-18T16:22:06.347+0000] {subprocess.py:93} INFO -     "commitOffsets" : 212,
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -     "queryPlanning" : 114,
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3434,
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -     "walCommit" : 137
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:06.348+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.349+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.350+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.350+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:06.350+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:06.350+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.350+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.350+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.350+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:06.351+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:06.351+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.29120559114735,
[2025-07-18T16:22:06.352+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:06.353+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:06.354+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:06.354+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:06.354+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:06.355+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:06.355+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:06.355+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:06.356+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:06.356+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:06.356+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:06.384+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.385+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.390+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/6 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.6.c9d5fc95-8785-45ce-a766-bf6766663f88.tmp
[2025-07-18T16:22:06.416+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.5.ebb7ce0d-bfe3-422b-988e-365294592c5c.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/5
[2025-07-18T16:22:06.419+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.421+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.421+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.422+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:06.425+0000] {subprocess.py:93} INFO -   "id" : "adf70716-4e16-4ce5-aa2c-d7a1cbea446a",
[2025-07-18T16:22:06.424+0000] {subprocess.py:93} INFO -   "runId" : "c457c2da-f6de-43bf-bc48-5cb636f17f98",
[2025-07-18T16:22:06.427+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:06.429+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:04.096Z",
[2025-07-18T16:22:06.430+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T16:22:06.438+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:06.439+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 47.61904761904761,
[2025-07-18T16:22:06.442+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.43177892918825567,
[2025-07-18T16:22:06.444+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:06.447+0000] {subprocess.py:93} INFO -     "addBatch" : 1996,
[2025-07-18T16:22:06.448+0000] {subprocess.py:93} INFO -     "commitOffsets" : 172,
[2025-07-18T16:22:06.449+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:06.452+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:22:06.454+0000] {subprocess.py:93} INFO -     "queryPlanning" : 47,
[2025-07-18T16:22:06.455+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2316,
[2025-07-18T16:22:06.456+0000] {subprocess.py:93} INFO -     "walCommit" : 95
[2025-07-18T16:22:06.461+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:06.463+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:06.466+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:06.467+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:06.468+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:06.469+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:06.470+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:06.470+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.471+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.471+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:06.472+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:06.474+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.478+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:06.478+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.480+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.480+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:06.482+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 47.61904761904761,
[2025-07-18T16:22:06.483+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.43177892918825567,
[2025-07-18T16:22:06.485+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:06.487+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:06.488+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:06.489+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:06.490+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:06.492+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:06.496+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:06.499+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:06.500+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:06.501+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:06.502+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:06.502+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.503+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.503+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/6 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.6.b3705241-c655-4403-b840-3a697f7e2d26.tmp
[2025-07-18T16:22:06.504+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.504+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.504+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:06.505+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.505+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.506+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/.6.c9d5fc95-8785-45ce-a766-bf6766663f88.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/offsets/6
[2025-07-18T16:22:06.509+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752855726359,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:06.510+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:06.515+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.516+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.521+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.521+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.523+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T16:22:06.525+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.534+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:22:06.537+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkContext: Created broadcast 36 from start at <unknown>:0
[2025-07-18T16:22:06.537+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:06.545+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:06.549+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Got job 18 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:06.550+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
[2025-07-18T16:22:06.550+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:06.552+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:06.553+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:06.556+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/.6.b3705241-c655-4403-b840-3a697f7e2d26.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/offsets/6
[2025-07-18T16:22:06.558+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752855726422,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:06.562+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.563+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.567+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.568+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.574+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.592+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:06.595+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.0 MiB)
[2025-07-18T16:22:06.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 77cb57a6bd53:35301 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:22:06.597+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:06.597+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:06.597+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-07-18T16:22:06.601+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:06.602+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2025-07-18T16:22:06.611+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.614+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.629+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.637+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.638+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.639+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:06.644+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.645+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.645+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.651+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=133 untilOffset=135, for query queryId=14ee97d4-8b20-40a2-a8d4-01491461c78b batchId=6 taskId=18 partitionId=0
[2025-07-18T16:22:06.665+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.669+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.694+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T16:22:06.702+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.704+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.705+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.729+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.734+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.740+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.742+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.743+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:06.796+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.803+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:06.853+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:06.894+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T16:22:06.895+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-6218ebd5-fe5b-432f-b749-2c36ef33a230-1995106205-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 202610792 nanos.
[2025-07-18T16:22:06.928+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 4856 bytes result sent to driver
[2025-07-18T16:22:06.943+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 341 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:06.944+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.353 s
[2025-07-18T16:22:06.947+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-07-18T16:22:06.949+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:06.952+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-07-18T16:22:06.953+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO DAGScheduler: Job 18 finished: start at <unknown>:0, took 0.403593 s
[2025-07-18T16:22:06.954+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:06.954+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Committing epoch 6 for query 14ee97d4-8b20-40a2-a8d4-01491461c78b in append mode
[2025-07-18T16:22:06.964+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T16:22:06.992+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 77cb57a6bd53:35301 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:06.996+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkContext: Created broadcast 38 from start at <unknown>:0
[2025-07-18T16:22:07.000+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:07.001+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:07.006+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Got job 19 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:07.008+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Final stage: ResultStage 19 (start at <unknown>:0)
[2025-07-18T16:22:07.011+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:07.012+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:07.012+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:07.013+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T16:22:07.015+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 27.5 KiB, free 433.9 MiB)
[2025-07-18T16:22:07.023+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 433.9 MiB)
[2025-07-18T16:22:07.025+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 77cb57a6bd53:35301 (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:07.025+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 433.8 MiB)
[2025-07-18T16:22:07.026+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 77cb57a6bd53:35301 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:22:07.026+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 39 from start at <unknown>:0
[2025-07-18T16:22:07.026+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:07.027+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:07.027+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:07.029+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:07.029+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-07-18T16:22:07.037+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:22:07.041+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Got job 20 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:07.044+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
[2025-07-18T16:22:07.045+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:07.047+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:07.050+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:07.052+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2025-07-18T16:22:07.053+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 28.6 KiB, free 433.8 MiB)
[2025-07-18T16:22:07.053+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:07.053+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 433.8 MiB)
[2025-07-18T16:22:07.054+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 77cb57a6bd53:35301 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:07.054+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:07.054+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:07.054+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2025-07-18T16:22:07.055+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=133 untilOffset=135, for query queryId=adf70716-4e16-4ce5-aa2c-d7a1cbea446a batchId=6 taskId=19 partitionId=0
[2025-07-18T16:22:07.060+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:22:07.060+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2025-07-18T16:22:07.087+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to offset 134 for partition feedback-0
[2025-07-18T16:22:07.095+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:22:07.101+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:07.102+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.119+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=133 untilOffset=135, for query queryId=1ba8749c-1701-47e7-880c-fbc4a0b916d5 batchId=6 taskId=20 partitionId=0
[2025-07-18T16:22:07.157+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T16:22:07.219+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T16:22:07.221+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-ce289cf7-ac1a-4d19-bdcf-0a9ac315c6e4-1447608252-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 88638708 nanos.
[2025-07-18T16:22:07.222+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 4699 bytes result sent to driver
[2025-07-18T16:22:07.224+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 164 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:07.229+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-07-18T16:22:07.230+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.189 s
[2025-07-18T16:22:07.230+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:07.231+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2025-07-18T16:22:07.231+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Job 20 finished: start at <unknown>:0, took 0.202246 s
[2025-07-18T16:22:07.233+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:07.235+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committing epoch 6 for query 1ba8749c-1701-47e7-880c-fbc4a0b916d5 in append mode
[2025-07-18T16:22:07.325+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.453+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN Tasks: Retrying task after failure: Version 96 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.462+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 96 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:07.465+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:07.465+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:07.465+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:07.466+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:07.466+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:07.466+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:07.467+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:07.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:07.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:07.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:07.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:07.467+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:07.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:07.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:07.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:07.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:07.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:07.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:07.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:07.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:07.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.474+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:07.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:07.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:07.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:07.485+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:07.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:07.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:07.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:07.487+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:07.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:07.488+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:07.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:07.595+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:07.595+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:22:07.597+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor-1, groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:07.602+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T16:22:07.647+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN Tasks: Retrying task after failure: Version 88 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v88.metadata.json
[2025-07-18T16:22:07.648+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 88 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v88.metadata.json
[2025-07-18T16:22:07.648+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:07.649+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:07.650+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:07.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:07.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:07.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:07.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:07.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:07.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:07.654+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:07.654+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:07.655+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:07.655+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:07.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:07.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:07.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:07.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:07.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:07.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:07.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:07.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:07.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:07.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:07.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:07.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:07.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:07.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.668+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.669+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:07.670+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:07.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:07.672+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.673+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.676+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.678+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:07.680+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:07.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:07.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:07.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:07.683+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.686+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.687+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:07.687+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:07.687+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.688+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:07.689+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:07.776+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DataWritingSparkTask: Committed partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T16:22:07.777+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b1ab4d00-8557-47f4-8609-b06017095c29-469638174-executor read 2 records through 1 polls (polled  out 1 records), taking 511743334 nanos, during time span of 691292917 nanos.
[2025-07-18T16:22:07.864+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 4803 bytes result sent to driver
[2025-07-18T16:22:07.876+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 840 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:07.881+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-07-18T16:22:07.889+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: ResultStage 19 (start at <unknown>:0) finished in 0.871 s
[2025-07-18T16:22:07.902+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:07.908+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2025-07-18T16:22:07.909+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Job 19 finished: start at <unknown>:0, took 0.889985 s
[2025-07-18T16:22:07.911+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:07.911+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committing epoch 6 for query adf70716-4e16-4ce5-aa2c-d7a1cbea446a in append mode
[2025-07-18T16:22:08.056+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:08.145+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v97.metadata.json
[2025-07-18T16:22:08.250+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 77cb57a6bd53:35301 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.278+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.304+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 77cb57a6bd53:35301 in memory (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.331+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.341+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.345+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN Tasks: Retrying task after failure: Version 89 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v89.metadata.json
[2025-07-18T16:22:08.347+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 89 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v89.metadata.json
[2025-07-18T16:22:08.348+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:08.348+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:08.348+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:08.349+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:08.349+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:08.349+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:08.350+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:08.351+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:08.351+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:08.351+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:08.352+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:08.352+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:08.353+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:08.353+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:08.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:08.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:08.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:08.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:08.359+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:08.359+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:08.360+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:08.361+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:08.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:08.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:08.363+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:08.364+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:08.365+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.367+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:08.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:08.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:08.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.374+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:08.380+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:08.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:08.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:08.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:08.385+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:08.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:08.387+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:08.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:08.390+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SnapshotProducer: Committed snapshot 5249061434249918785 (FastAppend)
[2025-07-18T16:22:08.391+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 77cb57a6bd53:35301 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.392+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 77cb57a6bd53:35301 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.393+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 77cb57a6bd53:35301 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.398+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 77cb57a6bd53:35301 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.513+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN SnapshotProducer: Failed to load committed snapshot, skipping manifest clean-up
[2025-07-18T16:22:08.515+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN SnapshotProducer: Failed to notify listeners
[2025-07-18T16:22:08.515+0000] {subprocess.py:93} INFO - java.lang.NullPointerException: Cannot invoke "org.apache.iceberg.Snapshot.sequenceNumber()" because "snapshot" is null
[2025-07-18T16:22:08.515+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.FastAppend.updateEvent(FastAppend.java:174)
[2025-07-18T16:22:08.517+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.notifyListeners(SnapshotProducer.java:449)
[2025-07-18T16:22:08.517+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:441)
[2025-07-18T16:22:08.518+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:08.518+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:08.518+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:08.518+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:08.518+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:08.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:08.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:08.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:08.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:08.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:08.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:08.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:08.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:08.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:08.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:08.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:08.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:08.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:08.523+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:08.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:08.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:08.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:08.525+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:08.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:08.526+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:08.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:08.527+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committed in 1404 ms
[2025-07-18T16:22:08.527+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:08.540+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/6 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.6.70915dcc-f598-4366-8199-abdf89139359.tmp
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v98.metadata.json
[2025-07-18T16:22:08.689+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/.6.70915dcc-f598-4366-8199-abdf89139359.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:14:00+00:00/commits/6
[2025-07-18T16:22:08.697+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:08.698+0000] {subprocess.py:93} INFO -   "id" : "14ee97d4-8b20-40a2-a8d4-01491461c78b",
[2025-07-18T16:22:08.698+0000] {subprocess.py:93} INFO -   "runId" : "1c689d4b-5499-4b62-8616-1fa50e13ce42",
[2025-07-18T16:22:08.699+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:08.699+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:05.979Z",
[2025-07-18T16:22:08.699+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T16:22:08.700+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:08.700+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.8006405124099278,
[2025-07-18T16:22:08.701+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.7382798080472499,
[2025-07-18T16:22:08.701+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:08.701+0000] {subprocess.py:93} INFO -     "addBatch" : 2129,
[2025-07-18T16:22:08.702+0000] {subprocess.py:93} INFO -     "commitOffsets" : 169,
[2025-07-18T16:22:08.702+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:08.702+0000] {subprocess.py:93} INFO -     "latestOffset" : 11,
[2025-07-18T16:22:08.702+0000] {subprocess.py:93} INFO -     "queryPlanning" : 212,
[2025-07-18T16:22:08.703+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2709,
[2025-07-18T16:22:08.703+0000] {subprocess.py:93} INFO -     "walCommit" : 188
[2025-07-18T16:22:08.704+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:08.705+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:08.705+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:08.709+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:08.711+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:08.715+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:08.718+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:08.720+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.722+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.722+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:08.723+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:08.723+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:08.726+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.728+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.730+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:08.733+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:08.735+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:08.736+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.738+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.740+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:08.743+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.8006405124099278,
[2025-07-18T16:22:08.747+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.7382798080472499,
[2025-07-18T16:22:08.749+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:08.751+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:08.752+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:08.753+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:08.754+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:08.754+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:08.755+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:08.756+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:08.757+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:08.757+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:08.758+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:08.758+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SnapshotProducer: Committed snapshot 7147159456204485322 (FastAppend)
[2025-07-18T16:22:08.921+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7147159456204485322, sequenceNumber=97, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.86934775S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=97}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=879}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2894}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=317627}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:08.926+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committed in 875 ms
[2025-07-18T16:22:08.927+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:09.008+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/6 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.6.7801ff6f-9048-4343-8582-ef7ca76fa48c.tmp
[2025-07-18T16:22:09.137+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v90.metadata.json
[2025-07-18T16:22:09.276+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SnapshotProducer: Committed snapshot 6782305685965320637 (FastAppend)
[2025-07-18T16:22:09.278+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/.6.7801ff6f-9048-4343-8582-ef7ca76fa48c.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:14:00+00:00/commits/6
[2025-07-18T16:22:09.285+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:09.285+0000] {subprocess.py:93} INFO -   "id" : "adf70716-4e16-4ce5-aa2c-d7a1cbea446a",
[2025-07-18T16:22:09.286+0000] {subprocess.py:93} INFO -   "runId" : "c457c2da-f6de-43bf-bc48-5cb636f17f98",
[2025-07-18T16:22:09.286+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:09.286+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:06.417Z",
[2025-07-18T16:22:09.286+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T16:22:09.287+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:09.287+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.8616975441619991,
[2025-07-18T16:22:09.287+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.7000350017500875,
[2025-07-18T16:22:09.287+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:09.288+0000] {subprocess.py:93} INFO -     "addBatch" : 2332,
[2025-07-18T16:22:09.288+0000] {subprocess.py:93} INFO -     "commitOffsets" : 348,
[2025-07-18T16:22:09.288+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:09.289+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:22:09.289+0000] {subprocess.py:93} INFO -     "queryPlanning" : 31,
[2025-07-18T16:22:09.289+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2857,
[2025-07-18T16:22:09.290+0000] {subprocess.py:93} INFO -     "walCommit" : 133
[2025-07-18T16:22:09.290+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:09.291+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:09.291+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:09.291+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:09.292+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:09.299+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.300+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.8616975441619991,
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.7000350017500875,
[2025-07-18T16:22:09.302+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:09.303+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:09.303+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:09.304+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:09.306+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:09.306+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:09.306+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:09.307+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:09.307+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:09.307+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:09.307+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:09.527+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6782305685965320637, sequenceNumber=89, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT2.201531042S, count=1}, attempts=CounterResult{unit=COUNT, value=3}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=89}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=883}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3052}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=283889}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855367766, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:09.530+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SparkWrite: Committed in 2203 ms
[2025-07-18T16:22:09.532+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:09.553+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/6 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.6.fd4adbb2-15ca-4391-acb3-b342da608003.tmp
[2025-07-18T16:22:09.634+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/.6.fd4adbb2-15ca-4391-acb3-b342da608003.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:14:00+00:00/commits/6
[2025-07-18T16:22:09.639+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:09.641+0000] {subprocess.py:93} INFO -   "id" : "1ba8749c-1701-47e7-880c-fbc4a0b916d5",
[2025-07-18T16:22:09.642+0000] {subprocess.py:93} INFO -   "runId" : "1469f87c-608e-4555-a6fe-7c0ce410c11f",
[2025-07-18T16:22:09.644+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:09.645+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:06.343Z",
[2025-07-18T16:22:09.645+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T16:22:09.645+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:09.646+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.5745475438092502,
[2025-07-18T16:22:09.650+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.608457560085184,
[2025-07-18T16:22:09.653+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:09.653+0000] {subprocess.py:93} INFO -     "addBatch" : 2985,
[2025-07-18T16:22:09.654+0000] {subprocess.py:93} INFO -     "commitOffsets" : 103,
[2025-07-18T16:22:09.654+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:09.655+0000] {subprocess.py:93} INFO -     "latestOffset" : 16,
[2025-07-18T16:22:09.655+0000] {subprocess.py:93} INFO -     "queryPlanning" : 61,
[2025-07-18T16:22:09.659+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3287,
[2025-07-18T16:22:09.660+0000] {subprocess.py:93} INFO -     "walCommit" : 121
[2025-07-18T16:22:09.661+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:09.662+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:09.694+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:09.699+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:09.702+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:09.703+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:09.704+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:09.704+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.704+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.705+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:09.705+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:09.705+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:09.705+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.705+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.705+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:09.706+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:09.706+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:09.706+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.706+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.706+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:09.706+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.5745475438092502,
[2025-07-18T16:22:09.707+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.608457560085184,
[2025-07-18T16:22:09.707+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:09.708+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:09.708+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:09.708+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:09.708+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:09.708+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:09.712+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:09.714+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:09.714+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:09.715+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:09.718+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:18.696+0000] {subprocess.py:93} INFO - 25/07/18 16:22:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:19.289+0000] {subprocess.py:93} INFO - 25/07/18 16:22:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:19.647+0000] {subprocess.py:93} INFO - 25/07/18 16:22:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:28.870+0000] {subprocess.py:93} INFO - 25/07/18 16:22:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:29.288+0000] {subprocess.py:93} INFO - 25/07/18 16:22:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:29.650+0000] {subprocess.py:93} INFO - 25/07/18 16:22:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.766+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.840+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.841+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:56.076+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:57.065+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:59.046+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.077+0000] {subprocess.py:93} INFO - 25/07/18 16:23:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.336+0000] {subprocess.py:93} INFO - 25/07/18 16:23:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.342+0000] {subprocess.py:93} INFO - 25/07/18 16:23:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:12.640+0000] {subprocess.py:93} INFO - 25/07/18 16:23:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:12.912+0000] {subprocess.py:93} INFO - 25/07/18 16:23:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:13.841+0000] {subprocess.py:93} INFO - 25/07/18 16:23:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:22.547+0000] {subprocess.py:93} INFO - 25/07/18 16:23:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:22.847+0000] {subprocess.py:93} INFO - 25/07/18 16:23:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:23.791+0000] {subprocess.py:93} INFO - 25/07/18 16:23:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:32.591+0000] {subprocess.py:93} INFO - 25/07/18 16:23:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:32.987+0000] {subprocess.py:93} INFO - 25/07/18 16:23:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:35.403+0000] {subprocess.py:93} INFO - 25/07/18 16:23:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:51.013+0000] {subprocess.py:93} INFO - 25/07/18 16:23:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:55.081+0000] {subprocess.py:93} INFO - 25/07/18 16:23:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:57.553+0000] {subprocess.py:93} INFO - 25/07/18 16:23:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:01.140+0000] {subprocess.py:93} INFO - 25/07/18 16:23:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:06.829+0000] {subprocess.py:93} INFO - 25/07/18 16:23:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:11.234+0000] {subprocess.py:93} INFO - 25/07/18 16:24:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:06.943+0000] {subprocess.py:93} INFO - 25/07/18 16:24:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:09.218+0000] {subprocess.py:93} INFO - 25/07/18 16:24:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:10.628+0000] {subprocess.py:93} INFO - 25/07/18 16:24:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:32.537+0000] {subprocess.py:93} INFO - 25/07/18 16:25:19 INFO NetworkClient: [AdminClient clientId=adminclient-3] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:32.692+0000] {subprocess.py:93} INFO - 25/07/18 16:25:19 INFO NetworkClient: [AdminClient clientId=adminclient-2] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:33.069+0000] {subprocess.py:93} INFO - 25/07/18 16:25:22 INFO NetworkClient: [AdminClient clientId=adminclient-1] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:33.127+0000] {subprocess.py:93} INFO - 25/07/18 16:25:23 INFO NetworkClient: [AdminClient clientId=adminclient-1] Cancelled in-flight METADATA request with correlation id 88203 due to node 1 being disconnected (elapsed time since creation: 37708ms, elapsed time since send: 37708ms, request timeout: 30000ms)
[2025-07-18T16:25:33.662+0000] {subprocess.py:93} INFO - 25/07/18 16:25:23 INFO NetworkClient: [AdminClient clientId=adminclient-2] Cancelled in-flight METADATA request with correlation id 88463 due to node 1 being disconnected (elapsed time since creation: 27640ms, elapsed time since send: 27640ms, request timeout: 27547ms)
[2025-07-18T16:25:33.752+0000] {subprocess.py:93} INFO - 25/07/18 16:25:23 INFO NetworkClient: [AdminClient clientId=adminclient-3] Cancelled in-flight METADATA request with correlation id 88768 due to node 1 being disconnected (elapsed time since creation: 32260ms, elapsed time since send: 32260ms, request timeout: 30000ms)
[2025-07-18T16:25:33.959+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map()) by listener AppStatusListener took 2.804626709s.
[2025-07-18T16:25:33.966+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:33.983+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855910774, tries=12, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 12 attempt(s)
[2025-07-18T16:25:34.005+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.024+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.042+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.066+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.076+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.080+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.083+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.084+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.085+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.086+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.087+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.087+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.087+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.088+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.088+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.089+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.089+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.090+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.091+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.093+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.111+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.139+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.151+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.152+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.167+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.174+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.181+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.199+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.216+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.227+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855910774, tries=12, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 12 attempt(s)
[2025-07-18T16:25:34.228+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 88463 due to node 1 being disconnected
[2025-07-18T16:25:34.228+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.229+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855912721, tries=11, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 11 attempt(s)
[2025-07-18T16:25:34.234+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.241+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.250+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.314+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.315+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.318+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.321+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.328+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.339+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.340+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.344+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.365+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.368+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.372+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.374+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.386+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.396+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.412+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.432+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.439+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.449+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.461+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.465+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.466+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.469+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.482+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.484+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855912721, tries=11, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 11 attempt(s)
[2025-07-18T16:25:34.485+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 88768 due to node 1 being disconnected
[2025-07-18T16:25:34.485+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.487+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855920684, tries=13, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 13 attempt(s)
[2025-07-18T16:25:34.489+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.489+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.492+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.494+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.550+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.553+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.564+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.564+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.565+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.586+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.624+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.625+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.627+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.634+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.635+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.653+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.667+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.671+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.676+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.683+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.686+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.702+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.720+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.725+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.728+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.730+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.807+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.807+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.808+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.808+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.821+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.823+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.825+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.828+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855920684, tries=13, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 13 attempt(s)
[2025-07-18T16:25:34.829+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 88203 due to node 1 being disconnected
[2025-07-18T16:25:34.909+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
[2025-07-18T16:25:34.914+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-07-18T16:25:34.914+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-07-18T16:25:34.977+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:34.981+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:34.983+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:34.988+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:34.990+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:34.994+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:34.996+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:34.997+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:34.999+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.002+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.007+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.010+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.013+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.022+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.044+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.053+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.064+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.070+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.073+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.074+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.076+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.081+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.089+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.094+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.096+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.102+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.109+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.114+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.118+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.119+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.119+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.123+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.128+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.145+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.161+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.163+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.175+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.177+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.183+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.183+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.184+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.184+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.186+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.189+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.191+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.192+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.193+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.193+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.198+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.198+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.202+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.202+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.203+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.207+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.208+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.209+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.211+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.214+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.215+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.215+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.215+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.216+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.217+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.218+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.226+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.227+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.227+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.232+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.248+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.260+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.260+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.261+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.261+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.261+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.262+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.263+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.266+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.266+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.267+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.267+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.272+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.274+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.284+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.285+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.285+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.285+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.285+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.285+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.286+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.288+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.289+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.289+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.291+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.294+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.294+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.295+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.295+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.296+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.296+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.299+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.302+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.304+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.304+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.304+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.305+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.306+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.309+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.310+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.312+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.312+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.313+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.313+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.314+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.314+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.315+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.316+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.319+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.334+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.335+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.342+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.344+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.357+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.358+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.358+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.359+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.359+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.360+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.361+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.383+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.390+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.390+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.461+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.466+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.469+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.472+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.472+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.473+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.473+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.473+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.474+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.474+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.475+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.476+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.476+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.477+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.477+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.477+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.477+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.478+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.479+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.479+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.479+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.479+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.479+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.480+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.480+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.480+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.480+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.480+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.481+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.481+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.481+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.481+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.481+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.482+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.487+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.487+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.488+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.488+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.488+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.489+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.489+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.490+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.490+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935014
[2025-07-18T16:25:35.490+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.490+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.490+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.490+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935016
[2025-07-18T16:25:35.490+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.491+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.491+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.491+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935044
[2025-07-18T16:25:35.491+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:35.491+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:35.491+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:42.695+0000] {subprocess.py:93} INFO - 25/07/18 16:25:42 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 77cb57a6bd53:35301 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:25:42.808+0000] {subprocess.py:93} INFO - 25/07/18 16:25:42 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:25:42.839+0000] {subprocess.py:93} INFO - 25/07/18 16:25:42 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 77cb57a6bd53:35301 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:25:44.873+0000] {local_task_job_runner.py:294} WARNING - State of this instance has been externally set to scheduled. Terminating instance.
[2025-07-18T16:25:44.949+0000] {process_utils.py:131} INFO - Sending 15 to group 1281. PIDs of all processes in the group: [1282, 1281]
[2025-07-18T16:25:44.952+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 1281
[2025-07-18T16:25:44.959+0000] {taskinstance.py:1632} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-07-18T16:25:44.961+0000] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2025-07-18T16:25:45.657+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/subprocess.py", line 91, in run_command
    for raw_line in iter(self.sub_process.stdout.readline, b""):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1634, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2025-07-18T16:25:45.873+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=restaurant_pipeline, task_id=stream_to_bronze, execution_date=20250718T161400, start_date=20250718T161605, end_date=20250718T162545
[2025-07-18T16:25:46.371+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 183 for task stream_to_bronze (Task received SIGTERM signal; 1281)
[2025-07-18T16:25:46.495+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1282, status='terminated', started='16:16:05') (1282) terminated with exit code None
[2025-07-18T16:25:46.507+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1281, status='terminated', exitcode=1, started='16:16:05') (1281) terminated with exit code 1
