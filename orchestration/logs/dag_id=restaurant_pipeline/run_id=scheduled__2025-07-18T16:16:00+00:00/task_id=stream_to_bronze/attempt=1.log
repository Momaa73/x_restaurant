[2025-07-18T16:18:05.910+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:16:00+00:00 [queued]>
[2025-07-18T16:18:05.934+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:16:00+00:00 [queued]>
[2025-07-18T16:18:05.936+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-07-18T16:18:05.960+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): stream_to_bronze> on 2025-07-18 16:16:00+00:00
[2025-07-18T16:18:05.975+0000] {standard_task_runner.py:57} INFO - Started process 1318 to run task
[2025-07-18T16:18:05.980+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'restaurant_pipeline', 'stream_to_bronze', 'scheduled__2025-07-18T16:16:00+00:00', '--job-id', '186', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_pipeline.py', '--cfg-path', '/tmp/tmp6jmip16a']
[2025-07-18T16:18:05.983+0000] {standard_task_runner.py:85} INFO - Job 186: Subtask stream_to_bronze
[2025-07-18T16:18:06.050+0000] {task_command.py:416} INFO - Running <TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:16:00+00:00 [running]> on host 9bcfb43e0ab7
[2025-07-18T16:18:06.147+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='moran' AIRFLOW_CTX_DAG_ID='restaurant_pipeline' AIRFLOW_CTX_TASK_ID='stream_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-07-18T16:16:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T16:16:00+00:00'
[2025-07-18T16:18:06.149+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-07-18T16:18:06.150+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', "docker exec -e AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T16:16:00+00:00' spark-iceberg spark-submit /home/iceberg/spark/stream_to_bronze.py"]
[2025-07-18T16:18:06.160+0000] {subprocess.py:86} INFO - Output:
[2025-07-18T16:18:08.994+0000] {subprocess.py:93} INFO - 25/07/18 16:18:08 INFO SparkContext: Running Spark version 3.5.6
[2025-07-18T16:18:08.995+0000] {subprocess.py:93} INFO - 25/07/18 16:18:08 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:18:08.996+0000] {subprocess.py:93} INFO - 25/07/18 16:18:08 INFO SparkContext: Java version 17.0.15
[2025-07-18T16:18:09.014+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO ResourceUtils: ==============================================================
[2025-07-18T16:18:09.016+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-18T16:18:09.016+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO ResourceUtils: ==============================================================
[2025-07-18T16:18:09.016+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SparkContext: Submitted application: StreamToBronze
[2025-07-18T16:18:09.028+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-18T16:18:09.033+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO ResourceProfile: Limiting resource is cpu
[2025-07-18T16:18:09.034+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-18T16:18:09.074+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SecurityManager: Changing view acls to: root,spark
[2025-07-18T16:18:09.075+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SecurityManager: Changing modify acls to: root,spark
[2025-07-18T16:18:09.076+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SecurityManager: Changing view acls groups to:
[2025-07-18T16:18:09.076+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SecurityManager: Changing modify acls groups to:
[2025-07-18T16:18:09.077+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
[2025-07-18T16:18:09.120+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-18T16:18:09.334+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Utils: Successfully started service 'sparkDriver' on port 37467.
[2025-07-18T16:18:09.355+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SparkEnv: Registering MapOutputTracker
[2025-07-18T16:18:09.378+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-18T16:18:09.396+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-18T16:18:09.396+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-18T16:18:09.397+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-18T16:18:09.416+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-313b44d2-9303-43b3-a39e-490ac41935e3
[2025-07-18T16:18:09.426+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-18T16:18:09.439+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-18T16:18:09.532+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-18T16:18:09.594+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-07-18T16:18:09.595+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2025-07-18T16:18:09.596+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2025-07-18T16:18:09.605+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Utils: Successfully started service 'SparkUI' on port 4043.
[2025-07-18T16:18:09.683+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Executor: Starting executor ID driver on host 77cb57a6bd53
[2025-07-18T16:18:09.684+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:18:09.684+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Executor: Java version 17.0.15
[2025-07-18T16:18:09.688+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-18T16:18:09.689+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@460eff97 for default.
[2025-07-18T16:18:09.708+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34127.
[2025-07-18T16:18:09.708+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO NettyBlockTransferService: Server created on 77cb57a6bd53:34127
[2025-07-18T16:18:09.709+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-18T16:18:09.714+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 77cb57a6bd53, 34127, None)
[2025-07-18T16:18:09.717+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO BlockManagerMasterEndpoint: Registering block manager 77cb57a6bd53:34127 with 434.4 MiB RAM, BlockManagerId(driver, 77cb57a6bd53, 34127, None)
[2025-07-18T16:18:09.719+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 77cb57a6bd53, 34127, None)
[2025-07-18T16:18:09.719+0000] {subprocess.py:93} INFO - 25/07/18 16:18:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 77cb57a6bd53, 34127, None)
[2025-07-18T16:18:10.045+0000] {subprocess.py:93} INFO - 25/07/18 16:18:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-18T16:18:10.050+0000] {subprocess.py:93} INFO - 25/07/18 16:18:10 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-07-18T16:18:12.108+0000] {subprocess.py:93} INFO - 25/07/18 16:18:12 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-07-18T16:18:12.126+0000] {subprocess.py:93} INFO - 25/07/18 16:18:12 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-07-18T16:18:12.127+0000] {subprocess.py:93} INFO - 25/07/18 16:18:12 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-07-18T16:18:13.280+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:13.300+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-07-18T16:18:13.342+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00 resolved to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00.
[2025-07-18T16:18:13.342+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:18:13.379+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/metadata using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/.metadata.c53f184a-fded-4313-873d-e994f564bec5.tmp
[2025-07-18T16:18:13.420+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/.metadata.c53f184a-fded-4313-873d-e994f564bec5.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/metadata
[2025-07-18T16:18:13.441+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Starting [id = a705b2ed-ad57-41ce-9e7a-629c21aaaacc, runId = d921ac12-16c5-4345-97c8-12fba1de01ef]. Use file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00 to store the query checkpoint.
[2025-07-18T16:18:13.448+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5df65145] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7f9bfa8c]
[2025-07-18T16:18:13.475+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:18:13.476+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:18:13.476+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:18:13.481+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:18:13.521+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:13.526+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00 resolved to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00.
[2025-07-18T16:18:13.526+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:18:13.533+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/metadata using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/.metadata.9744b705-14ca-4d7f-96a6-9e8102052210.tmp
[2025-07-18T16:18:13.549+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/.metadata.9744b705-14ca-4d7f-96a6-9e8102052210.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/metadata
[2025-07-18T16:18:13.558+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Starting [id = 6a2789a7-abc5-4ced-8710-95de25958a28, runId = a27254af-cc69-42a2-89ac-9c81c4cb53cc]. Use file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00 to store the query checkpoint.
[2025-07-18T16:18:13.560+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7ee91c14] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@702efc61]
[2025-07-18T16:18:13.561+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:18:13.561+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:18:13.562+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:18:13.562+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:18:13.646+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:13.650+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00 resolved to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00.
[2025-07-18T16:18:13.651+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:18:13.660+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/metadata using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/.metadata.0cfc3bc8-202a-41fe-a5ca-200731d20022.tmp
[2025-07-18T16:18:13.675+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/.metadata.0cfc3bc8-202a-41fe-a5ca-200731d20022.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/metadata
[2025-07-18T16:18:13.684+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Starting [id = 3e609a08-6175-4a61-a969-0559365755cd, runId = adc07e88-e29c-4be8-a682-c48f3eee7501]. Use file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00 to store the query checkpoint.
[2025-07-18T16:18:13.685+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3dace227] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@336b0a3a]
[2025-07-18T16:18:13.687+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:18:13.687+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:18:13.688+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:18:13.688+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:18:13.703+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:18:13.703+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:18:13.703+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:18:13.703+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:18:13.704+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:18:13.704+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:18:13.705+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:18:13.705+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:18:13.705+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:18:13.706+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:18:13.706+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:18:13.706+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:18:13.706+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:18:13.707+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:18:13.708+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:18:13.708+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:18:13.708+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:18:13.708+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:18:13.708+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:18:13.708+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:18:13.708+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:18:13.709+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:18:13.709+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:18:13.709+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:18:13.709+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:18:13.709+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:18:13.709+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:18:13.709+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:18:13.710+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:18:13.711+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:18:13.712+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:18:13.713+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:18:13.714+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:18:13.715+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:18:13.716+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:18:13.717+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:18:13.717+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:18:13.717+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:18:13.717+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:18:13.717+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:18:13.717+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:18:13.718+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:18:13.719+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:18:13.720+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:18:13.720+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:18:13.723+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:18:13.724+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:18:13.725+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:18:13.726+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:18:13.727+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:18:13.728+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:18:13.728+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:18:13.728+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:18:13.728+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:18:13.728+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:18:13.728+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:18:13.728+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:18:13.729+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:18:13.729+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:18:13.730+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:18:13.730+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:18:13.730+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:18:13.731+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:18:13.732+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:18:13.733+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:18:13.733+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:18:13.733+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:18:13.733+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:18:13.733+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:18:13.733+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:18:13.734+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:18:13.734+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:18:13.734+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:18:13.734+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:18:13.754+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:18:13.755+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:18:13.758+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:18:13.758+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:18:13.758+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:18:13.758+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka startTimeMs: 1752855493753
[2025-07-18T16:18:13.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:18:13.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:18:13.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka startTimeMs: 1752855493753
[2025-07-18T16:18:13.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:18:13.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:18:13.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:13 INFO AppInfoParser: Kafka startTimeMs: 1752855493753
[2025-07-18T16:18:14.171+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/sources/0/.0.9f9140e7-2f62-4170-81ce-5203f8bb5629.tmp
[2025-07-18T16:18:14.173+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/sources/0/.0.45044914-2cf9-4541-898b-fefe54c08e63.tmp
[2025-07-18T16:18:14.174+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/sources/0/.0.7e08c935-0d41-4618-bb4a-336707b13ac2.tmp
[2025-07-18T16:18:14.195+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/sources/0/.0.7e08c935-0d41-4618-bb4a-336707b13ac2.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/sources/0/0
[2025-07-18T16:18:14.196+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO KafkaMicroBatchStream: Initial offsets: {"checkins":{"0":0}}
[2025-07-18T16:18:14.198+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/sources/0/.0.9f9140e7-2f62-4170-81ce-5203f8bb5629.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/sources/0/0
[2025-07-18T16:18:14.199+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO KafkaMicroBatchStream: Initial offsets: {"reservations":{"0":0}}
[2025-07-18T16:18:14.199+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/sources/0/.0.45044914-2cf9-4541-898b-fefe54c08e63.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/sources/0/0
[2025-07-18T16:18:14.199+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO KafkaMicroBatchStream: Initial offsets: {"feedback":{"0":0}}
[2025-07-18T16:18:14.225+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.0.77f76913-0dbf-45a7-9a31-91f709c21c83.tmp
[2025-07-18T16:18:14.226+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.0.31de65b5-5d1f-437d-aa4f-c754a362f731.tmp
[2025-07-18T16:18:14.228+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.0.64bc3197-704e-4030-bd27-1ca3e64061a2.tmp
[2025-07-18T16:18:14.262+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.0.31de65b5-5d1f-437d-aa4f-c754a362f731.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/0
[2025-07-18T16:18:14.263+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855494212,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:14.265+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.0.77f76913-0dbf-45a7-9a31-91f709c21c83.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/0
[2025-07-18T16:18:14.265+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855494209,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:14.268+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.0.64bc3197-704e-4030-bd27-1ca3e64061a2.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/0
[2025-07-18T16:18:14.268+0000] {subprocess.py:93} INFO - 25/07/18 16:18:14 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855494212,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:15.154+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:15.170+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:15.172+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:15.173+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:15.179+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:15.184+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:15.186+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:15.187+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:15.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:15.933+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:15.935+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:15.936+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.178+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.182+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.187+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.344+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:16.358+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:16.360+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:16.369+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:16.379+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:16.380+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:16.380+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.380+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:16.383+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:16.383+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:16.385+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.385+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.389+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.390+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.392+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.469+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:16.471+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:16.472+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:16.498+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:16.498+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:16.499+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:16.502+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:16.504+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:16.507+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:16.513+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.514+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.523+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.524+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.532+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:16.533+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:17.277+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 293.705042 ms
[2025-07-18T16:18:17.278+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 293.102958 ms
[2025-07-18T16:18:17.278+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 293.032292 ms
[2025-07-18T16:18:17.423+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:18:17.423+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:18:17.424+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:18:17.468+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:18:17.469+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:18:17.470+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:18:17.470+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 77cb57a6bd53:34127 (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:18:17.471+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 77cb57a6bd53:34127 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:17.471+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:17.471+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Created broadcast 1 from start at <unknown>:0
[2025-07-18T16:18:17.472+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Created broadcast 0 from start at <unknown>:0
[2025-07-18T16:18:17.472+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Created broadcast 2 from start at <unknown>:0
[2025-07-18T16:18:17.472+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:17.474+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:17.474+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:17.498+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:17.498+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:17.498+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:17.511+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:17.511+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-07-18T16:18:17.512+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:17.512+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:17.515+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:17.616+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:18:17.624+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.2 MiB)
[2025-07-18T16:18:17.624+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 77cb57a6bd53:34127 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:18:17.625+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:17.637+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:17.638+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-18T16:18:17.654+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:17.654+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-07-18T16:18:17.655+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:17.655+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:17.656+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:17.657+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:17.658+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:18:17.658+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 77cb57a6bd53:34127 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:17.659+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:17.659+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:17.659+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-18T16:18:17.668+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:18:17.673+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:17.674+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
[2025-07-18T16:18:17.675+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:17.675+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:17.676+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:17.676+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T16:18:17.683+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:17.684+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:18:17.685+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 77cb57a6bd53:34127 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:17.685+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:17.687+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:17.687+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-18T16:18:17.689+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-18T16:18:17.690+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-18T16:18:17.690+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T16:18:17.690+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-18T16:18:17.800+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 23.21775 ms
[2025-07-18T16:18:17.801+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 23.647834 ms
[2025-07-18T16:18:17.801+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 25.195167 ms
[2025-07-18T16:18:17.816+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 15.316375 ms
[2025-07-18T16:18:17.816+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 14.334584 ms
[2025-07-18T16:18:17.817+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodeGenerator: Code generated in 15.808792 ms
[2025-07-18T16:18:17.887+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:17.888+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:17.888+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:18.035+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=0 untilOffset=129, for query queryId=6a2789a7-abc5-4ced-8710-95de25958a28 batchId=0 taskId=1 partitionId=0
[2025-07-18T16:18:18.036+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=0 untilOffset=129, for query queryId=3e609a08-6175-4a61-a969-0559365755cd batchId=0 taskId=0 partitionId=0
[2025-07-18T16:18:18.041+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=0 untilOffset=129, for query queryId=a705b2ed-ad57-41ce-9e7a-629c21aaaacc batchId=0 taskId=2 partitionId=0
[2025-07-18T16:18:18.065+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO CodeGenerator: Code generated in 7.239833 ms
[2025-07-18T16:18:18.090+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO CodeGenerator: Code generated in 10.214208 ms
[2025-07-18T16:18:18.106+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:18:18.106+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:18:18.106+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:18:18.106+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:18:18.106+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:18:18.107+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:18:18.108+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:18:18.109+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:18:18.110+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:18:18.111+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:18:18.111+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:18:18.111+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:18:18.111+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:18:18.112+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:18:18.112+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:18:18.112+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:18:18.112+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:18:18.113+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:18:18.113+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:18:18.114+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:18:18.115+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:18:18.115+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:18:18.115+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:18:18.115+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:18:18.116+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:18:18.116+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:18:18.116+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:18:18.117+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:18:18.117+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:18:18.117+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:18:18.117+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:18:18.118+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:18:18.118+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:18:18.118+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:18:18.118+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:18:18.118+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:18:18.119+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:18:18.123+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:18:18.124+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:18:18.124+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:18:18.124+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:18:18.124+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:18:18.125+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:18:18.126+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:18:18.127+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:18:18.128+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:18:18.128+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:18:18.128+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:18:18.129+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:18:18.130+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:18:18.130+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:18:18.131+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:18:18.131+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:18:18.132+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:18:18.133+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2
[2025-07-18T16:18:18.133+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:18:18.133+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:18:18.134+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:18:18.134+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:18:18.135+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:18:18.135+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:18:18.136+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:18:18.136+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:18:18.136+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor
[2025-07-18T16:18:18.137+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:18:18.137+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:18:18.137+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:18:18.137+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:18:18.137+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:18:18.138+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:18:18.138+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:18:18.138+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:18:18.138+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:18:18.138+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:18:18.139+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:18:18.139+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:18:18.139+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:18:18.139+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:18:18.139+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:18:18.140+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:18:18.141+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:18:18.141+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:18:18.142+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:18:18.142+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:18:18.142+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:18:18.143+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:18:18.143+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:18:18.143+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:18:18.144+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:18:18.144+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:18:18.144+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:18:18.145+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:18:18.145+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:18:18.145+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:18:18.145+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:18:18.145+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:18:18.145+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:18:18.145+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:18:18.146+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:18:18.146+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:18:18.146+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:18:18.146+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:18:18.146+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:18:18.147+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:18:18.147+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:18:18.147+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:18:18.148+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:18:18.148+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:18:18.148+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:18:18.148+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:18:18.148+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:18:18.148+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:18:18.149+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:18:18.150+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:18:18.151+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:18:18.152+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:18:18.153+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:18:18.153+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3
[2025-07-18T16:18:18.153+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:18:18.153+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:18:18.153+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:18:18.153+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:18:18.154+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:18:18.154+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:18:18.155+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:18:18.156+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:18:18.156+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor
[2025-07-18T16:18:18.157+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:18:18.157+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:18:18.157+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:18:18.158+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:18:18.158+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:18:18.158+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:18:18.158+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:18:18.159+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:18:18.159+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:18:18.159+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:18:18.159+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:18:18.160+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:18:18.160+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:18:18.161+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:18:18.161+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:18:18.161+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:18:18.161+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:18:18.161+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:18:18.161+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:18:18.162+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:18:18.162+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:18:18.162+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:18:18.163+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:18:18.163+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:18:18.164+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:18:18.165+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:18:18.165+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:18:18.166+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:18:18.166+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:18:18.167+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:18:18.167+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:18:18.168+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:18:18.169+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:18:18.170+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:18:18.170+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:18:18.171+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:18:18.172+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:18:18.173+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:18:18.174+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:18:18.175+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:18:18.175+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:18:18.176+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:18:18.177+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:18:18.178+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:18:18.178+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:18:18.179+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:18:18.179+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:18:18.179+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:18:18.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:18:18.180+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:18:18.180+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:18:18.180+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:18:18.181+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:18:18.181+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:18:18.182+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:18:18.182+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:18:18.182+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:18:18.183+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:18:18.183+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:18:18.183+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:18:18.183+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:18:18.183+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:18:18.183+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:18:18.183+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:18:18.184+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:18:18.184+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:18:18.184+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:18:18.184+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:18:18.185+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:18:18.185+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:18:18.185+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:18:18.185+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:18:18.185+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:18:18.185+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:18:18.186+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:18:18.187+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:18:18.187+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:18:18.187+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:18:18.187+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka startTimeMs: 1752855498154
[2025-07-18T16:18:18.187+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:18:18.188+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:18:18.188+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka startTimeMs: 1752855498156
[2025-07-18T16:18:18.188+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Assigned to partition(s): checkins-0
[2025-07-18T16:18:18.188+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Assigned to partition(s): feedback-0
[2025-07-18T16:18:18.188+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:18:18.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:18:18.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO AppInfoParser: Kafka startTimeMs: 1752855498160
[2025-07-18T16:18:18.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Assigned to partition(s): reservations-0
[2025-07-18T16:18:18.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to offset 0 for partition checkins-0
[2025-07-18T16:18:18.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to offset 0 for partition feedback-0
[2025-07-18T16:18:18.189+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to offset 0 for partition reservations-0
[2025-07-18T16:18:18.190+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:18:18.190+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:18:18.190+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:18:18.221+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:18:18.222+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:18:18.223+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:18:18.726+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:18.727+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:18.729+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:18.730+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:18:18.730+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:18:18.731+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:18:18.731+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:18.732+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:18.732+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:18.867+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:18:18.867+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:18:18.867+0000] {subprocess.py:93} INFO - 25/07/18 16:18:18 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:18:19.191+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:18:19.191+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:18:19.192+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor read 129 records through 1 polls (polled  out 129 records), taking 561562042 nanos, during time span of 1025862543 nanos.
[2025-07-18T16:18:19.192+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor read 129 records through 1 polls (polled  out 129 records), taking 561620542 nanos, during time span of 1025852542 nanos.
[2025-07-18T16:18:19.192+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:18:19.192+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor read 129 records through 1 polls (polled  out 129 records), taking 559718084 nanos, during time span of 1026799917 nanos.
[2025-07-18T16:18:19.210+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4950 bytes result sent to driver
[2025-07-18T16:18:19.210+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 5027 bytes result sent to driver
[2025-07-18T16:18:19.211+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4939 bytes result sent to driver
[2025-07-18T16:18:19.222+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1536 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:19.227+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-18T16:18:19.227+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1537 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:19.228+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-18T16:18:19.228+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1563 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:19.228+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-18T16:18:19.231+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.575 s
[2025-07-18T16:18:19.232+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:19.232+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-18T16:18:19.235+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.735975 s
[2025-07-18T16:18:19.236+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:19.237+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committing epoch 0 for query 6a2789a7-abc5-4ced-8710-95de25958a28 in append mode
[2025-07-18T16:18:19.237+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 1.561 s
[2025-07-18T16:18:19.237+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:19.237+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-18T16:18:19.239+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.714 s
[2025-07-18T16:18:19.243+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:19.244+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-18T16:18:19.244+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.740681 s
[2025-07-18T16:18:19.244+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:19.244+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committing epoch 0 for query a705b2ed-ad57-41ce-9e7a-629c21aaaacc in append mode
[2025-07-18T16:18:19.245+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 1.744465 s
[2025-07-18T16:18:19.247+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:19.247+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committing epoch 0 for query 3e609a08-6175-4a61-a969-0559365755cd in append mode
[2025-07-18T16:18:19.277+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:19.278+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:19.278+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:19.445+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 77cb57a6bd53:34127 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:19.621+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v86.metadata.json
[2025-07-18T16:18:19.621+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v84.metadata.json
[2025-07-18T16:18:19.622+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v75.metadata.json
[2025-07-18T16:18:19.662+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SnapshotProducer: Committed snapshot 3721736550623092548 (FastAppend)
[2025-07-18T16:18:19.666+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SnapshotProducer: Committed snapshot 6916966687533744006 (FastAppend)
[2025-07-18T16:18:19.727+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SnapshotProducer: Committed snapshot 9009730796838618064 (FastAppend)
[2025-07-18T16:18:19.842+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=3721736550623092548, sequenceNumber=85, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.551977459S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=85}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=129}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=732}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=9113}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=276683}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:19.843+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=9009730796838618064, sequenceNumber=83, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.553447959S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=83}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=129}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=732}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=7587}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=265107}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:19.843+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committed in 564 ms
[2025-07-18T16:18:19.844+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committed in 564 ms
[2025-07-18T16:18:19.844+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:18:19.844+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6916966687533744006, sequenceNumber=74, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.552214167S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=74}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=129}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=731}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=5957}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=236013}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:19.844+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:18:19.844+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO SparkWrite: Committed in 566 ms
[2025-07-18T16:18:19.845+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:18:19.861+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.0.91f7e127-e358-4320-a6e3-b4cc8379fc0a.tmp
[2025-07-18T16:18:19.864+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.0.caf94580-5a8d-4542-9b93-6234f39b9eb0.tmp
[2025-07-18T16:18:19.866+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.0.815492e8-4b83-40f0-8301-44391894bc89.tmp
[2025-07-18T16:18:19.893+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.0.815492e8-4b83-40f0-8301-44391894bc89.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/0
[2025-07-18T16:18:19.896+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.0.91f7e127-e358-4320-a6e3-b4cc8379fc0a.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/0
[2025-07-18T16:18:19.899+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.0.caf94580-5a8d-4542-9b93-6234f39b9eb0.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/0
[2025-07-18T16:18:19.952+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:19.953+0000] {subprocess.py:93} INFO -   "id" : "3e609a08-6175-4a61-a969-0559365755cd",
[2025-07-18T16:18:19.954+0000] {subprocess.py:93} INFO -   "runId" : "adc07e88-e29c-4be8-a682-c48f3eee7501",
[2025-07-18T16:18:19.958+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:19.960+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:13.684Z",
[2025-07-18T16:18:19.961+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:18:19.964+0000] {subprocess.py:93} INFO -   "numInputRows" : 129,
[2025-07-18T16:18:19.965+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:18:19.965+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 20.769602318467236,
[2025-07-18T16:18:19.966+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:19.966+0000] {subprocess.py:93} INFO -     "addBatch" : 3542,
[2025-07-18T16:18:19.967+0000] {subprocess.py:93} INFO -     "commitOffsets" : 48,
[2025-07-18T16:18:19.967+0000] {subprocess.py:93} INFO -     "getBatch" : 15,
[2025-07-18T16:18:19.967+0000] {subprocess.py:93} INFO -     "latestOffset" : 520,
[2025-07-18T16:18:19.968+0000] {subprocess.py:93} INFO -     "queryPlanning" : 2021,
[2025-07-18T16:18:19.969+0000] {subprocess.py:93} INFO -     "triggerExecution" : 6211,
[2025-07-18T16:18:19.969+0000] {subprocess.py:93} INFO -     "walCommit" : 49
[2025-07-18T16:18:19.970+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:19.971+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:19.974+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:19.975+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:18:19.976+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:18:19.977+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:19.978+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:19.978+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:19.979+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:19.979+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:19.981+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:19.982+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:19.983+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:19.988+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:19.989+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:19.989+0000] {subprocess.py:93} INFO -     "numInputRows" : 129,
[2025-07-18T16:18:19.990+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:18:19.992+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 20.769602318467236,
[2025-07-18T16:18:19.993+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:19.994+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:19.996+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:19.998+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:19.998+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:19.999+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:20.001+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:20.001+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:18:20.001+0000] {subprocess.py:93} INFO -     "numOutputRows" : 129
[2025-07-18T16:18:20.002+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:20.002+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:20.002+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:20.002+0000] {subprocess.py:93} INFO -   "id" : "a705b2ed-ad57-41ce-9e7a-629c21aaaacc",
[2025-07-18T16:18:20.002+0000] {subprocess.py:93} INFO -   "runId" : "d921ac12-16c5-4345-97c8-12fba1de01ef",
[2025-07-18T16:18:20.002+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:20.003+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:13.469Z",
[2025-07-18T16:18:20.003+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:18:20.003+0000] {subprocess.py:93} INFO -   "numInputRows" : 129,
[2025-07-18T16:18:20.003+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:18:20.003+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 20.065328978068127,
[2025-07-18T16:18:20.003+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:20.003+0000] {subprocess.py:93} INFO -     "addBatch" : 3544,
[2025-07-18T16:18:20.004+0000] {subprocess.py:93} INFO -     "commitOffsets" : 51,
[2025-07-18T16:18:20.004+0000] {subprocess.py:93} INFO -     "getBatch" : 12,
[2025-07-18T16:18:20.004+0000] {subprocess.py:93} INFO -     "latestOffset" : 726,
[2025-07-18T16:18:20.004+0000] {subprocess.py:93} INFO -     "queryPlanning" : 2017,
[2025-07-18T16:18:20.005+0000] {subprocess.py:93} INFO -     "triggerExecution" : 6428,
[2025-07-18T16:18:20.006+0000] {subprocess.py:93} INFO -     "walCommit" : 52
[2025-07-18T16:18:20.006+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:20.006+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:20.007+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:20.007+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:18:20.007+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:18:20.007+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:20.007+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:20.008+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:20.008+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:20.008+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:20.008+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:20.008+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:20.008+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:20.008+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:20.009+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:20.009+0000] {subprocess.py:93} INFO -     "numInputRows" : 129,
[2025-07-18T16:18:20.009+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:18:20.009+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 20.065328978068127,
[2025-07-18T16:18:20.010+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:20.010+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:20.010+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:20.011+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:20.011+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:20.011+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:20.011+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:20.012+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:18:20.012+0000] {subprocess.py:93} INFO -     "numOutputRows" : 129
[2025-07-18T16:18:20.012+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:20.012+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:20.013+0000] {subprocess.py:93} INFO - 25/07/18 16:18:19 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:20.013+0000] {subprocess.py:93} INFO -   "id" : "6a2789a7-abc5-4ced-8710-95de25958a28",
[2025-07-18T16:18:20.013+0000] {subprocess.py:93} INFO -   "runId" : "a27254af-cc69-42a2-89ac-9c81c4cb53cc",
[2025-07-18T16:18:20.014+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:20.018+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:13.557Z",
[2025-07-18T16:18:20.019+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:18:20.020+0000] {subprocess.py:93} INFO -   "numInputRows" : 129,
[2025-07-18T16:18:20.020+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:18:20.021+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 20.356635632002526,
[2025-07-18T16:18:20.022+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:20.023+0000] {subprocess.py:93} INFO -     "addBatch" : 3549,
[2025-07-18T16:18:20.023+0000] {subprocess.py:93} INFO -     "commitOffsets" : 40,
[2025-07-18T16:18:20.024+0000] {subprocess.py:93} INFO -     "getBatch" : 17,
[2025-07-18T16:18:20.024+0000] {subprocess.py:93} INFO -     "latestOffset" : 647,
[2025-07-18T16:18:20.024+0000] {subprocess.py:93} INFO -     "queryPlanning" : 2020,
[2025-07-18T16:18:20.025+0000] {subprocess.py:93} INFO -     "triggerExecution" : 6337,
[2025-07-18T16:18:20.025+0000] {subprocess.py:93} INFO -     "walCommit" : 47
[2025-07-18T16:18:20.026+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:20.026+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:20.027+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:20.027+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:18:20.027+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:18:20.027+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:20.028+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:20.028+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:20.028+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:20.028+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:20.029+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:20.029+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:20.029+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:20.030+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:20.030+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:20.030+0000] {subprocess.py:93} INFO -     "numInputRows" : 129,
[2025-07-18T16:18:20.030+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:18:20.031+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 20.356635632002526,
[2025-07-18T16:18:20.031+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:20.032+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:20.032+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:20.033+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:20.033+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:20.033+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:20.034+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:20.035+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:18:20.036+0000] {subprocess.py:93} INFO -     "numOutputRows" : 129
[2025-07-18T16:18:20.036+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:20.037+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:20.037+0000] {subprocess.py:93} INFO - 25/07/18 16:18:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 77cb57a6bd53:34127 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:20.038+0000] {subprocess.py:93} INFO - 25/07/18 16:18:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 77cb57a6bd53:34127 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:20.056+0000] {subprocess.py:93} INFO - 25/07/18 16:18:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 77cb57a6bd53:34127 in memory (size: 12.2 KiB, free: 434.4 MiB)
[2025-07-18T16:18:20.067+0000] {subprocess.py:93} INFO - 25/07/18 16:18:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:20.091+0000] {subprocess.py:93} INFO - 25/07/18 16:18:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 77cb57a6bd53:34127 in memory (size: 12.3 KiB, free: 434.4 MiB)
[2025-07-18T16:18:29.926+0000] {subprocess.py:93} INFO - 25/07/18 16:18:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:29.928+0000] {subprocess.py:93} INFO - 25/07/18 16:18:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:29.928+0000] {subprocess.py:93} INFO - 25/07/18 16:18:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:39.925+0000] {subprocess.py:93} INFO - 25/07/18 16:18:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:39.925+0000] {subprocess.py:93} INFO - 25/07/18 16:18:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:39.926+0000] {subprocess.py:93} INFO - 25/07/18 16:18:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:49.930+0000] {subprocess.py:93} INFO - 25/07/18 16:18:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:49.931+0000] {subprocess.py:93} INFO - 25/07/18 16:18:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:49.932+0000] {subprocess.py:93} INFO - 25/07/18 16:18:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:59.941+0000] {subprocess.py:93} INFO - 25/07/18 16:18:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:59.943+0000] {subprocess.py:93} INFO - 25/07/18 16:18:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:59.943+0000] {subprocess.py:93} INFO - 25/07/18 16:18:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:09.941+0000] {subprocess.py:93} INFO - 25/07/18 16:19:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:09.943+0000] {subprocess.py:93} INFO - 25/07/18 16:19:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:09.944+0000] {subprocess.py:93} INFO - 25/07/18 16:19:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:19.951+0000] {subprocess.py:93} INFO - 25/07/18 16:19:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:19.952+0000] {subprocess.py:93} INFO - 25/07/18 16:19:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:19.952+0000] {subprocess.py:93} INFO - 25/07/18 16:19:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:29.959+0000] {subprocess.py:93} INFO - 25/07/18 16:19:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:29.961+0000] {subprocess.py:93} INFO - 25/07/18 16:19:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:29.961+0000] {subprocess.py:93} INFO - 25/07/18 16:19:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:39.966+0000] {subprocess.py:93} INFO - 25/07/18 16:19:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:39.967+0000] {subprocess.py:93} INFO - 25/07/18 16:19:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:39.967+0000] {subprocess.py:93} INFO - 25/07/18 16:19:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:49.976+0000] {subprocess.py:93} INFO - 25/07/18 16:19:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:49.977+0000] {subprocess.py:93} INFO - 25/07/18 16:19:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:49.977+0000] {subprocess.py:93} INFO - 25/07/18 16:19:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:59.977+0000] {subprocess.py:93} INFO - 25/07/18 16:19:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:59.978+0000] {subprocess.py:93} INFO - 25/07/18 16:19:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:59.984+0000] {subprocess.py:93} INFO - 25/07/18 16:19:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:04.206+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.1.4e388066-56b8-4242-bf74-182458ce3341.tmp
[2025-07-18T16:20:04.257+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.1.4e388066-56b8-4242-bf74-182458ce3341.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/1
[2025-07-18T16:20:04.259+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855604192,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:04.292+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.296+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.299+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.311+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.332+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.363+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.373+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.524+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.525+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.526+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.539+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.544+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.684+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:20:04.687+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.688+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:04.695+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Created broadcast 6 from start at <unknown>:0
[2025-07-18T16:20:04.702+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:04.704+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:04.717+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:04.724+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Final stage: ResultStage 3 (start at <unknown>:0)
[2025-07-18T16:20:04.727+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:04.728+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:04.728+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:04.751+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.757+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.757+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 77cb57a6bd53:34127 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:04.757+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:04.761+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:04.768+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-07-18T16:20:04.768+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T16:20:04.769+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-07-18T16:20:04.816+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.1.4a95415f-a76b-4474-8bcf-e25f55ca03be.tmp
[2025-07-18T16:20:04.828+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:04.852+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=129 untilOffset=130, for query queryId=a705b2ed-ad57-41ce-9e7a-629c21aaaacc batchId=1 taskId=3 partitionId=0
[2025-07-18T16:20:04.859+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.1.4a95415f-a76b-4474-8bcf-e25f55ca03be.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/1
[2025-07-18T16:20:04.863+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855604796,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:04.878+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to offset 129 for partition reservations-0
[2025-07-18T16:20:04.884+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.893+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.895+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.902+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:20:04.903+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.906+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.916+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.917+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.928+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.933+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.951+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.952+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.952+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.967+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.973+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.049+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:20:05.067+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.078+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 77cb57a6bd53:34127 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.080+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 8 from start at <unknown>:0
[2025-07-18T16:20:05.084+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:05.085+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:05.088+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:05.092+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
[2025-07-18T16:20:05.092+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:05.095+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:05.101+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 77cb57a6bd53:34127 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:05.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:05.103+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-18T16:20:05.105+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:05.110+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-07-18T16:20:05.141+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:05.142+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=129 untilOffset=130, for query queryId=6a2789a7-abc5-4ced-8710-95de25958a28 batchId=1 taskId=4 partitionId=0
[2025-07-18T16:20:05.214+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to offset 129 for partition checkins-0
[2025-07-18T16:20:05.230+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:20:05.409+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.410+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:20:05.411+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.444+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:20:05.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.1.4b4562e8-9315-4f96-ae42-e39a870112a8.tmp
[2025-07-18T16:20:05.538+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.1.4b4562e8-9315-4f96-ae42-e39a870112a8.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/1
[2025-07-18T16:20:05.539+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855605406,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:05.606+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:20:05.609+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor read 1 records through 1 polls (polled  out 3 records), taking 533518750 nanos, during time span of 725268959 nanos.
[2025-07-18T16:20:05.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4782 bytes result sent to driver
[2025-07-18T16:20:05.647+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 879 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:05.653+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-07-18T16:20:05.659+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: ResultStage 3 (start at <unknown>:0) finished in 0.923 s
[2025-07-18T16:20:05.660+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:05.661+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-07-18T16:20:05.661+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.962022 s
[2025-07-18T16:20:05.672+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:05.675+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing epoch 1 for query a705b2ed-ad57-41ce-9e7a-629c21aaaacc in append mode
[2025-07-18T16:20:05.701+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.726+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.726+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:20:05.735+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.749+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.762+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:20:05.802+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:05.814+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.819+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.820+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.853+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.856+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.877+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:20:05.878+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor read 1 records through 1 polls (polled  out 3 records), taking 533361292 nanos, during time span of 686842917 nanos.
[2025-07-18T16:20:05.899+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4853 bytes result sent to driver
[2025-07-18T16:20:05.928+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 823 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:05.930+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-18T16:20:05.950+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.871 s
[2025-07-18T16:20:05.951+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:05.952+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-18T16:20:05.957+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.880100 s
[2025-07-18T16:20:05.957+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:05.957+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing epoch 1 for query 6a2789a7-abc5-4ced-8710-95de25958a28 in append mode
[2025-07-18T16:20:05.987+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.990+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.990+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:06.049+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.050+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.158+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:06.167+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:06.168+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.169+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Created broadcast 10 from start at <unknown>:0
[2025-07-18T16:20:06.175+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:06.183+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.185+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:06.191+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:06.191+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Final stage: ResultStage 5 (start at <unknown>:0)
[2025-07-18T16:20:06.191+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:06.201+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:06.204+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:06.229+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:06.248+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:20:06.250+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 77cb57a6bd53:34127 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.254+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:06.256+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:06.256+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-07-18T16:20:06.269+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:06.275+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-07-18T16:20:06.363+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:06.364+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=129 untilOffset=130, for query queryId=3e609a08-6175-4a61-a969-0559365755cd batchId=1 taskId=5 partitionId=0
[2025-07-18T16:20:06.370+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to offset 129 for partition feedback-0
[2025-07-18T16:20:06.378+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:20:06.553+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 WARN Tasks: Retrying task after failure: Version 77 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v77.metadata.json
[2025-07-18T16:20:06.553+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 77 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v77.metadata.json
[2025-07-18T16:20:06.554+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:06.554+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:06.555+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:06.555+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:06.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:06.560+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:06.561+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:06.572+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:06.578+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:06.587+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:06.591+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:06.594+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:06.595+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:06.597+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:06.602+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:06.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:06.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:06.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:06.608+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:06.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:06.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:06.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:06.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:06.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:06.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:06.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:06.618+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:06.619+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:06.619+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:06.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:06.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:06.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:06.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:06.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:06.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:06.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:06.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:06.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:06.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:06.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:06.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:06.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:06.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:06.630+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:06.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:06.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:06.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:06.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:06.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:06.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:06.645+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.646+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.648+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:06.649+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 WARN Tasks: Retrying task after failure: Version 86 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v86.metadata.json
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 86 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v86.metadata.json
[2025-07-18T16:20:06.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:06.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:06.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:06.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:06.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:06.651+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:06.653+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:06.653+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:06.653+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:06.654+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:06.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:06.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:06.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:06.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:06.655+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:06.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:06.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:06.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:06.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:06.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:06.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:06.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:06.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:06.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:06.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:06.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:06.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:06.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:06.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:06.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:06.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:06.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:06.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:06.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:06.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:06.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:06.660+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:06.662+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:06.663+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:06.664+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:06.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:06.665+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:06.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:06.666+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:06.666+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:06.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:06.667+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:06.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:06.667+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:06.889+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:06.902+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:20:06.914+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:06.986+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:20:07.081+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:20:07.082+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor read 1 records through 1 polls (polled  out 3 records), taking 545236500 nanos, during time span of 711265333 nanos.
[2025-07-18T16:20:07.117+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4809 bytes result sent to driver
[2025-07-18T16:20:07.119+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 855 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:07.121+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-07-18T16:20:07.123+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: ResultStage 5 (start at <unknown>:0) finished in 0.904 s
[2025-07-18T16:20:07.127+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:07.128+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-07-18T16:20:07.130+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.937849 s
[2025-07-18T16:20:07.132+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:07.132+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committing epoch 1 for query 3e609a08-6175-4a61-a969-0559365755cd in append mode
[2025-07-18T16:20:07.209+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:07.242+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 WARN Tasks: Retrying task after failure: Version 87 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v87.metadata.json
[2025-07-18T16:20:07.245+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 87 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v87.metadata.json
[2025-07-18T16:20:07.249+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:07.253+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:07.265+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:07.267+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:07.272+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:07.272+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:07.273+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:07.274+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:07.275+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:07.275+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:07.276+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:07.277+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:07.277+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:07.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:07.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:07.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:07.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:07.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:07.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:07.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:07.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:07.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:07.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:07.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:07.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:07.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:07.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:07.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:07.289+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:07.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:07.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:07.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 WARN Tasks: Retrying task after failure: Version 78 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v78.metadata.json
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 78 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v78.metadata.json
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:07.308+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:07.309+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:07.310+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:07.310+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:07.311+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:07.313+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:07.316+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:07.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:07.328+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:07.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:07.338+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:07.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:07.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:07.344+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:07.345+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:07.351+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:07.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:07.365+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:07.367+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.369+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:07.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:07.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:07.374+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:07.381+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:07.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:07.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:07.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:07.385+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:07.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:07.388+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:07.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:08.002+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v88.metadata.json
[2025-07-18T16:20:08.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 77cb57a6bd53:34127 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.029+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 77cb57a6bd53:34127 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.058+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 77cb57a6bd53:34127 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.102+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 WARN Tasks: Retrying task after failure: Version 79 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v79.metadata.json
[2025-07-18T16:20:08.103+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 79 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v79.metadata.json
[2025-07-18T16:20:08.104+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:08.104+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:08.104+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:08.112+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:08.113+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:08.114+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:08.115+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:08.116+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:08.117+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:08.119+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:08.119+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:08.120+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:08.121+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:08.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:08.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:08.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:08.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:08.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:08.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:08.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:08.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:08.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:08.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:08.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:08.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:08.126+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:08.126+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.126+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.128+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.129+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.129+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.129+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:08.129+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:08.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:08.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:08.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:08.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:08.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:08.135+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:08.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:08.135+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.136+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:08.136+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:08.200+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SnapshotProducer: Committed snapshot 5207729233713942653 (FastAppend)
[2025-07-18T16:20:08.499+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 WARN SnapshotProducer: Failed to load committed snapshot, skipping manifest clean-up
[2025-07-18T16:20:08.500+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 WARN SnapshotProducer: Failed to notify listeners
[2025-07-18T16:20:08.501+0000] {subprocess.py:93} INFO - java.lang.NullPointerException: Cannot invoke "org.apache.iceberg.Snapshot.sequenceNumber()" because "snapshot" is null
[2025-07-18T16:20:08.504+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.FastAppend.updateEvent(FastAppend.java:174)
[2025-07-18T16:20:08.504+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.notifyListeners(SnapshotProducer.java:449)
[2025-07-18T16:20:08.504+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:441)
[2025-07-18T16:20:08.504+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:08.504+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:08.505+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:08.505+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:08.505+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:08.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:08.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:08.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:08.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:08.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:08.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:08.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:08.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:08.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:08.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:08.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:08.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:08.508+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:08.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:08.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:08.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:08.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:08.512+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.515+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:08.515+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:08.515+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:08.515+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:08.515+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.515+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.516+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:08.516+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:08.516+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.516+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:08.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:08.517+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Committed in 1300 ms
[2025-07-18T16:20:08.517+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:20:08.613+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v88.metadata.json
[2025-07-18T16:20:08.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/1 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.1.027189ac-9071-4e14-82a5-50ff113abd4f.tmp
[2025-07-18T16:20:08.787+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.1.027189ac-9071-4e14-82a5-50ff113abd4f.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/1
[2025-07-18T16:20:08.790+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:08.791+0000] {subprocess.py:93} INFO -   "id" : "3e609a08-6175-4a61-a969-0559365755cd",
[2025-07-18T16:20:08.792+0000] {subprocess.py:93} INFO -   "runId" : "adc07e88-e29c-4be8-a682-c48f3eee7501",
[2025-07-18T16:20:08.794+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:08.795+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:05.399Z",
[2025-07-18T16:20:08.796+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:20:08.798+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:08.800+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 50.0,
[2025-07-18T16:20:08.801+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.29542097488921715,
[2025-07-18T16:20:08.802+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:08.804+0000] {subprocess.py:93} INFO -     "addBatch" : 2747,
[2025-07-18T16:20:08.805+0000] {subprocess.py:93} INFO -     "commitOffsets" : 278,
[2025-07-18T16:20:08.806+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:08.806+0000] {subprocess.py:93} INFO -     "latestOffset" : 7,
[2025-07-18T16:20:08.807+0000] {subprocess.py:93} INFO -     "queryPlanning" : 217,
[2025-07-18T16:20:08.807+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3385,
[2025-07-18T16:20:08.810+0000] {subprocess.py:93} INFO -     "walCommit" : 129
[2025-07-18T16:20:08.811+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:08.811+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:08.812+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:08.812+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:20:08.813+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:08.814+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.816+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:08.817+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.817+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.818+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:08.818+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.819+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:08.819+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.819+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.820+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:08.820+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.820+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:08.821+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.821+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.822+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:08.822+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 50.0,
[2025-07-18T16:20:08.823+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.29542097488921715,
[2025-07-18T16:20:08.824+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:08.825+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:08.828+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:08.831+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:08.831+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:08.832+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:08.833+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:08.834+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:20:08.834+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:08.835+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:08.835+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:08.838+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.2.27862486-9511-4cc1-abb5-2bdae060916b.tmp
[2025-07-18T16:20:08.916+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SnapshotProducer: Committed snapshot 4508743350711456253 (FastAppend)
[2025-07-18T16:20:08.965+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.2.27862486-9511-4cc1-abb5-2bdae060916b.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/2
[2025-07-18T16:20:08.967+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855608791,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:08.980+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 WARN Tasks: Retrying task after failure: Version 80 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v80.metadata.json
[2025-07-18T16:20:08.980+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 80 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v80.metadata.json
[2025-07-18T16:20:08.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:08.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:08.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:08.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:08.981+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:08.984+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:08.984+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:08.984+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:08.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:08.986+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:08.987+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.989+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.990+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.990+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:08.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:08.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:08.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:09.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:09.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:09.002+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:09.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:09.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:09.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:09.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:09.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:09.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:09.009+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:09.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:09.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:09.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.019+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.021+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.023+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.025+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.027+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.059+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.060+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.064+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.070+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.071+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.130+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4508743350711456253, sequenceNumber=87, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT2.946234209S, count=1}, attempts=CounterResult{unit=COUNT, value=3}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=87}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=736}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2876}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=276611}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:09.131+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committed in 2949 ms
[2025-07-18T16:20:09.133+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:20:09.151+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/1 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.1.a828df7a-ca16-41f4-bf18-af8afb451c51.tmp
[2025-07-18T16:20:09.151+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:09.156+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:09.160+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.162+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Created broadcast 12 from start at <unknown>:0
[2025-07-18T16:20:09.163+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:09.173+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:09.177+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:09.178+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
[2025-07-18T16:20:09.179+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:09.181+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:09.181+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:09.182+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:09.193+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:20:09.198+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 77cb57a6bd53:34127 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:09.200+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:09.202+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:09.203+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-18T16:20:09.206+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:09.210+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-07-18T16:20:09.225+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.1.a828df7a-ca16-41f4-bf18-af8afb451c51.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/1
[2025-07-18T16:20:09.228+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:09.234+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:09.239+0000] {subprocess.py:93} INFO -   "id" : "6a2789a7-abc5-4ced-8710-95de25958a28",
[2025-07-18T16:20:09.241+0000] {subprocess.py:93} INFO -   "runId" : "a27254af-cc69-42a2-89ac-9c81c4cb53cc",
[2025-07-18T16:20:09.242+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:09.243+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:04.794Z",
[2025-07-18T16:20:09.244+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:20:09.244+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:09.244+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:20:09.245+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.22573363431151244,
[2025-07-18T16:20:09.246+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:09.251+0000] {subprocess.py:93} INFO -     "addBatch" : 4222,
[2025-07-18T16:20:09.252+0000] {subprocess.py:93} INFO -     "commitOffsets" : 97,
[2025-07-18T16:20:09.253+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:09.253+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:20:09.254+0000] {subprocess.py:93} INFO -     "queryPlanning" : 48,
[2025-07-18T16:20:09.254+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4430,
[2025-07-18T16:20:09.254+0000] {subprocess.py:93} INFO -     "walCommit" : 59
[2025-07-18T16:20:09.254+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:09.256+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:09.256+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:09.256+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:20:09.256+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:09.257+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:09.257+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:09.257+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.257+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.257+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:09.258+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:09.258+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:09.258+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.258+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.258+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:09.259+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:09.259+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:09.259+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.259+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.259+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:09.260+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:20:09.260+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.22573363431151244,
[2025-07-18T16:20:09.260+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:09.261+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:09.263+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:09.263+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:09.263+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:09.264+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:09.264+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:09.264+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:20:09.265+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:09.265+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:09.265+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:09.265+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=130 untilOffset=132, for query queryId=3e609a08-6175-4a61-a969-0559365755cd batchId=2 taskId=6 partitionId=0
[2025-07-18T16:20:09.265+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:20:09.269+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.2.ab814bb3-1943-40ba-9042-f239d8d0805b.tmp
[2025-07-18T16:20:09.327+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.2.ab814bb3-1943-40ba-9042-f239d8d0805b.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/2
[2025-07-18T16:20:09.328+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855609246,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:09.338+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:20:09.339+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 92400750 nanos.
[2025-07-18T16:20:09.342+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4767 bytes result sent to driver
[2025-07-18T16:20:09.349+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.350+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.353+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.354+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 153 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:09.355+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-18T16:20:09.358+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.181 s
[2025-07-18T16:20:09.363+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:09.365+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-07-18T16:20:09.365+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.193065 s
[2025-07-18T16:20:09.368+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.369+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.370+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.370+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:09.371+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committing epoch 2 for query 3e609a08-6175-4a61-a969-0559365755cd in append mode
[2025-07-18T16:20:09.371+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.372+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.390+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.391+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.393+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:09.420+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.423+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:09.429+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.507+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:20:09.521+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:09.526+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:20:09.531+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Created broadcast 14 from start at <unknown>:0
[2025-07-18T16:20:09.533+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:09.550+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:09.554+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:09.557+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
[2025-07-18T16:20:09.558+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:09.559+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:09.564+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:09.577+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T16:20:09.583+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.0 MiB)
[2025-07-18T16:20:09.584+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 77cb57a6bd53:34127 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:20:09.584+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:09.584+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:09.584+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-18T16:20:09.585+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:09.591+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-07-18T16:20:09.616+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:09.626+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=130 untilOffset=132, for query queryId=6a2789a7-abc5-4ced-8710-95de25958a28 batchId=2 taskId=7 partitionId=0
[2025-07-18T16:20:09.690+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:20:09.739+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:20:09.740+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 72805458 nanos.
[2025-07-18T16:20:09.741+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4818 bytes result sent to driver
[2025-07-18T16:20:09.742+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 158 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:09.742+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-18T16:20:09.749+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.176 s
[2025-07-18T16:20:09.750+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:09.750+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-18T16:20:09.755+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 0.207978 s
[2025-07-18T16:20:09.758+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:09.759+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committing epoch 2 for query 6a2789a7-abc5-4ced-8710-95de25958a28 in append mode
[2025-07-18T16:20:10.032+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v91.metadata.json
[2025-07-18T16:20:10.180+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:10.369+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SnapshotProducer: Committed snapshot 4585158319401775939 (FastAppend)
[2025-07-18T16:20:10.461+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=4585158319401775939, sequenceNumber=90, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.027334376S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=90}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=739}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3052}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=291589}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:10.465+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SparkWrite: Committed in 1028 ms
[2025-07-18T16:20:10.465+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:20:10.473+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/2 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.2.efc17439-4ddb-4511-b24e-f8a4c8701f01.tmp
[2025-07-18T16:20:10.495+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v82.metadata.json
[2025-07-18T16:20:10.535+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.2.efc17439-4ddb-4511-b24e-f8a4c8701f01.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/2
[2025-07-18T16:20:10.539+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:10.540+0000] {subprocess.py:93} INFO -   "id" : "3e609a08-6175-4a61-a969-0559365755cd",
[2025-07-18T16:20:10.540+0000] {subprocess.py:93} INFO -   "runId" : "adc07e88-e29c-4be8-a682-c48f3eee7501",
[2025-07-18T16:20:10.540+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:10.540+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:08.789Z",
[2025-07-18T16:20:10.540+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:20:10.540+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:10.540+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.5899705014749262,
[2025-07-18T16:20:10.541+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.146788990825688,
[2025-07-18T16:20:10.541+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:10.541+0000] {subprocess.py:93} INFO -     "addBatch" : 1449,
[2025-07-18T16:20:10.541+0000] {subprocess.py:93} INFO -     "commitOffsets" : 77,
[2025-07-18T16:20:10.541+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:10.542+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:20:10.542+0000] {subprocess.py:93} INFO -     "queryPlanning" : 33,
[2025-07-18T16:20:10.542+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1744,
[2025-07-18T16:20:10.542+0000] {subprocess.py:93} INFO -     "walCommit" : 180
[2025-07-18T16:20:10.543+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:10.543+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:10.544+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:10.544+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:20:10.545+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:10.545+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:10.545+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:10.545+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.546+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.546+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:10.546+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:10.547+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:10.547+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.547+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.547+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:10.547+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:10.547+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:10.548+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:10.548+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:10.548+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:10.548+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.5899705014749262,
[2025-07-18T16:20:10.548+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.146788990825688,
[2025-07-18T16:20:10.549+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:10.549+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:10.549+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:10.549+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:10.549+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:10.549+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:10.550+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:10.550+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:20:10.550+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:10.550+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:10.550+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:10.581+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 WARN Tasks: Retrying task after failure: Version 91 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v91.metadata.json
[2025-07-18T16:20:10.582+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 91 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v91.metadata.json
[2025-07-18T16:20:10.582+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:10.582+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:10.582+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:10.582+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:10.583+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:10.584+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:10.586+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:10.587+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:10.593+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:10.594+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:10.594+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:10.595+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:10.595+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:10.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:10.596+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:10.601+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:10.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:10.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:10.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:10.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:10.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:10.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:10.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:10.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:10.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:10.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:10.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:10.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:10.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:10.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:10.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:10.622+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:10.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:10.624+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:10.624+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:10.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:10.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:10.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:10.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:10.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:10.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:10.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:10.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:10.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:10.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:10.630+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:10.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:10.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:10.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:10.631+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:10.633+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:10.634+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:10.635+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:10.636+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:10.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:10.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:10.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:10.639+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:10.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:10.639+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:10.640+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO SnapshotProducer: Committed snapshot 3735587165120439948 (FastAppend)
[2025-07-18T16:20:10.687+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:10.700+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 77cb57a6bd53:34127 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:10.732+0000] {subprocess.py:93} INFO - 25/07/18 16:20:10 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 77cb57a6bd53:34127 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:11.197+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 77cb57a6bd53:34127 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:11.202+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=3735587165120439948, sequenceNumber=81, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT5.397403043S, count=1}, attempts=CounterResult{unit=COUNT, value=5}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=81}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=741}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2969}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=257039}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:11.208+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Committed in 5401 ms
[2025-07-18T16:20:11.208+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:20:11.209+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:11.217+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/1 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.1.9ad36ac2-1034-48c7-84cf-10ba925dddd0.tmp
[2025-07-18T16:20:11.398+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.1.9ad36ac2-1034-48c7-84cf-10ba925dddd0.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/1
[2025-07-18T16:20:11.427+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:11.432+0000] {subprocess.py:93} INFO -   "id" : "a705b2ed-ad57-41ce-9e7a-629c21aaaacc",
[2025-07-18T16:20:11.433+0000] {subprocess.py:93} INFO -   "runId" : "d921ac12-16c5-4345-97c8-12fba1de01ef",
[2025-07-18T16:20:11.434+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:11.436+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:04.190Z",
[2025-07-18T16:20:11.436+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:20:11.437+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:11.439+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:20:11.439+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.13914011409489355,
[2025-07-18T16:20:11.439+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:11.440+0000] {subprocess.py:93} INFO -     "addBatch" : 6883,
[2025-07-18T16:20:11.440+0000] {subprocess.py:93} INFO -     "commitOffsets" : 181,
[2025-07-18T16:20:11.441+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:11.441+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:20:11.442+0000] {subprocess.py:93} INFO -     "queryPlanning" : 54,
[2025-07-18T16:20:11.443+0000] {subprocess.py:93} INFO -     "triggerExecution" : 7187,
[2025-07-18T16:20:11.443+0000] {subprocess.py:93} INFO -     "walCommit" : 64
[2025-07-18T16:20:11.444+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:11.444+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:11.445+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:11.445+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:20:11.447+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:11.447+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:11.448+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:11.448+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:11.449+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:11.449+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:11.450+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:11.450+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:11.450+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:11.451+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:11.451+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:11.451+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:11.452+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:11.452+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:11.452+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:11.453+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:11.453+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:20:11.453+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.13914011409489355,
[2025-07-18T16:20:11.453+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:11.454+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:11.454+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:11.454+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:11.454+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:11.454+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:11.454+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:11.455+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:20:11.455+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:11.458+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:11.459+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:11.459+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v92.metadata.json
[2025-07-18T16:20:11.505+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.2.0913c6bd-d7b7-4b6d-8c9e-38891e2fd7f9.tmp
[2025-07-18T16:20:11.555+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.2.0913c6bd-d7b7-4b6d-8c9e-38891e2fd7f9.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/2
[2025-07-18T16:20:11.556+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855611426,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:11.558+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SnapshotProducer: Committed snapshot 5460796877062952986 (FastAppend)
[2025-07-18T16:20:11.628+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.629+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.630+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.634+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:11.639+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:11.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.664+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.666+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.668+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=5460796877062952986, sequenceNumber=91, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.530016209S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=91}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=744}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2894}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=288187}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:11.669+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Committed in 1531 ms
[2025-07-18T16:20:11.669+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:20:11.670+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:11.671+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:11.696+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.699+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.700+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:11.700+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/2 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.2.295b12fe-b0c6-4714-b13d-7ff3163a3325.tmp
[2025-07-18T16:20:11.702+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:11.703+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:11.725+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:11.728+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:20:11.730+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 77cb57a6bd53:34127 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:11.734+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkContext: Created broadcast 16 from start at <unknown>:0
[2025-07-18T16:20:11.735+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:11.736+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:11.740+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:11.742+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
[2025-07-18T16:20:11.745+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:11.747+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:11.748+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:11.760+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T16:20:11.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:11.920+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 77cb57a6bd53:34127 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:11.945+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:11.975+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:11.976+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-18T16:20:11.980+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T16:20:11.982+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-07-18T16:20:12.022+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.2.295b12fe-b0c6-4714-b13d-7ff3163a3325.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/2
[2025-07-18T16:20:12.059+0000] {subprocess.py:93} INFO - 25/07/18 16:20:11 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:12.060+0000] {subprocess.py:93} INFO -   "id" : "6a2789a7-abc5-4ced-8710-95de25958a28",
[2025-07-18T16:20:12.060+0000] {subprocess.py:93} INFO -   "runId" : "a27254af-cc69-42a2-89ac-9c81c4cb53cc",
[2025-07-18T16:20:12.061+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:12.064+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:09.230Z",
[2025-07-18T16:20:12.071+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:20:12.082+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:12.083+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.4508566275924256,
[2025-07-18T16:20:12.083+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.724112961622013,
[2025-07-18T16:20:12.084+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:12.084+0000] {subprocess.py:93} INFO -     "addBatch" : 2313,
[2025-07-18T16:20:12.084+0000] {subprocess.py:93} INFO -     "commitOffsets" : 327,
[2025-07-18T16:20:12.084+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:12.085+0000] {subprocess.py:93} INFO -     "latestOffset" : 16,
[2025-07-18T16:20:12.086+0000] {subprocess.py:93} INFO -     "queryPlanning" : 23,
[2025-07-18T16:20:12.090+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2762,
[2025-07-18T16:20:12.092+0000] {subprocess.py:93} INFO -     "walCommit" : 81
[2025-07-18T16:20:12.094+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:12.094+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:12.095+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:12.096+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:20:12.096+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:12.097+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:12.097+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:12.097+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:12.098+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:12.098+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:12.098+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:12.098+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:12.098+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:12.098+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:12.098+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:12.099+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:12.099+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:12.102+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:12.120+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:12.120+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:12.126+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.4508566275924256,
[2025-07-18T16:20:12.126+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.724112961622013,
[2025-07-18T16:20:12.126+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:12.127+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:12.127+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:12.129+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:12.129+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:12.130+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:12.130+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:12.130+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:20:12.130+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:12.130+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:12.131+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:12.131+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:12.132+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=130 untilOffset=132, for query queryId=a705b2ed-ad57-41ce-9e7a-629c21aaaacc batchId=2 taskId=8 partitionId=0
[2025-07-18T16:20:12.133+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:20:12.254+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:20:12.257+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 171062208 nanos.
[2025-07-18T16:20:12.292+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4738 bytes result sent to driver
[2025-07-18T16:20:12.295+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 370 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:12.302+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-18T16:20:12.329+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.549 s
[2025-07-18T16:20:12.337+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:12.347+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-18T16:20:12.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.561366 s
[2025-07-18T16:20:12.391+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:12.415+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO SparkWrite: Committing epoch 2 for query a705b2ed-ad57-41ce-9e7a-629c21aaaacc in append mode
[2025-07-18T16:20:12.454+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:12.907+0000] {subprocess.py:93} INFO - 25/07/18 16:20:12 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v83.metadata.json
[2025-07-18T16:20:13.258+0000] {subprocess.py:93} INFO - 25/07/18 16:20:13 INFO SnapshotProducer: Committed snapshot 7790874965687125183 (FastAppend)
[2025-07-18T16:20:13.571+0000] {subprocess.py:93} INFO - 25/07/18 16:20:13 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=7790874965687125183, sequenceNumber=82, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.124045584S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=82}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=743}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3050}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=260089}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:13.580+0000] {subprocess.py:93} INFO - 25/07/18 16:20:13 INFO SparkWrite: Committed in 1124 ms
[2025-07-18T16:20:13.581+0000] {subprocess.py:93} INFO - 25/07/18 16:20:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:20:13.601+0000] {subprocess.py:93} INFO - 25/07/18 16:20:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/2 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.2.cc433e8d-b268-42a4-91ee-dd7edebda299.tmp
[2025-07-18T16:20:13.989+0000] {subprocess.py:93} INFO - 25/07/18 16:20:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.2.cc433e8d-b268-42a4-91ee-dd7edebda299.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/2
[2025-07-18T16:20:14.009+0000] {subprocess.py:93} INFO - 25/07/18 16:20:14 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:14.012+0000] {subprocess.py:93} INFO -   "id" : "a705b2ed-ad57-41ce-9e7a-629c21aaaacc",
[2025-07-18T16:20:14.012+0000] {subprocess.py:93} INFO -   "runId" : "d921ac12-16c5-4345-97c8-12fba1de01ef",
[2025-07-18T16:20:14.013+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:14.013+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:11.387Z",
[2025-07-18T16:20:14.013+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:20:14.013+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:14.013+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.2778935667639294,
[2025-07-18T16:20:14.013+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.7677543186180422,
[2025-07-18T16:20:14.014+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:14.014+0000] {subprocess.py:93} INFO -     "addBatch" : 1912,
[2025-07-18T16:20:14.014+0000] {subprocess.py:93} INFO -     "commitOffsets" : 426,
[2025-07-18T16:20:14.014+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:20:14.014+0000] {subprocess.py:93} INFO -     "latestOffset" : 39,
[2025-07-18T16:20:14.014+0000] {subprocess.py:93} INFO -     "queryPlanning" : 99,
[2025-07-18T16:20:14.014+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2605,
[2025-07-18T16:20:14.015+0000] {subprocess.py:93} INFO -     "walCommit" : 127
[2025-07-18T16:20:14.015+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:14.015+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:14.015+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:14.015+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:20:14.016+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:14.016+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:14.016+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:14.016+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:14.016+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:14.016+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:14.017+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:14.017+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:14.017+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:14.017+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:14.018+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:14.018+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:14.020+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:14.020+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:14.021+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:14.021+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:14.022+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.2778935667639294,
[2025-07-18T16:20:14.022+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.7677543186180422,
[2025-07-18T16:20:14.022+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:14.022+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:14.023+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:14.023+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:14.024+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:14.025+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:14.025+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:14.026+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:20:14.032+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:14.032+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:14.033+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:15.859+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 77cb57a6bd53:34127 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:15.863+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 77cb57a6bd53:34127 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:15.866+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:20.535+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:21.997+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:24.020+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:30.542+0000] {subprocess.py:93} INFO - 25/07/18 16:20:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:32.005+0000] {subprocess.py:93} INFO - 25/07/18 16:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:34.026+0000] {subprocess.py:93} INFO - 25/07/18 16:20:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:36.197+0000] {subprocess.py:93} INFO - 25/07/18 16:20:36 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:40.549+0000] {subprocess.py:93} INFO - 25/07/18 16:20:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:42.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:44.025+0000] {subprocess.py:93} INFO - 25/07/18 16:20:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:50.550+0000] {subprocess.py:93} INFO - 25/07/18 16:20:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:52.008+0000] {subprocess.py:93} INFO - 25/07/18 16:20:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:54.033+0000] {subprocess.py:93} INFO - 25/07/18 16:20:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:00.554+0000] {subprocess.py:93} INFO - 25/07/18 16:21:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:02.012+0000] {subprocess.py:93} INFO - 25/07/18 16:21:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:04.050+0000] {subprocess.py:93} INFO - 25/07/18 16:21:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:10.558+0000] {subprocess.py:93} INFO - 25/07/18 16:21:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:12.020+0000] {subprocess.py:93} INFO - 25/07/18 16:21:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:14.046+0000] {subprocess.py:93} INFO - 25/07/18 16:21:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:20.560+0000] {subprocess.py:93} INFO - 25/07/18 16:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:22.023+0000] {subprocess.py:93} INFO - 25/07/18 16:21:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:24.054+0000] {subprocess.py:93} INFO - 25/07/18 16:21:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:30.567+0000] {subprocess.py:93} INFO - 25/07/18 16:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:32.031+0000] {subprocess.py:93} INFO - 25/07/18 16:21:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:34.064+0000] {subprocess.py:93} INFO - 25/07/18 16:21:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:40.566+0000] {subprocess.py:93} INFO - 25/07/18 16:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:42.038+0000] {subprocess.py:93} INFO - 25/07/18 16:21:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:44.063+0000] {subprocess.py:93} INFO - 25/07/18 16:21:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:50.568+0000] {subprocess.py:93} INFO - 25/07/18 16:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:52.043+0000] {subprocess.py:93} INFO - 25/07/18 16:21:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:54.075+0000] {subprocess.py:93} INFO - 25/07/18 16:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:00.573+0000] {subprocess.py:93} INFO - 25/07/18 16:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:02.047+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:02.925+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/3 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.3.2c375bd7-50d9-4741-a430-9505e584986b.tmp
[2025-07-18T16:22:02.975+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.3.2c375bd7-50d9-4741-a430-9505e584986b.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/3
[2025-07-18T16:22:02.984+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855722864,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.089+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.092+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.096+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.102+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.108+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.121+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.122+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.122+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.129+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.158+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.197+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.200+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.201+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.205+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.208+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.292+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:22:03.304+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.306+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:03.307+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 18 from start at <unknown>:0
[2025-07-18T16:22:03.307+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:03.310+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:03.311+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:03.311+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Final stage: ResultStage 9 (start at <unknown>:0)
[2025-07-18T16:22:03.312+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:03.312+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:03.312+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:03.323+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.341+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.342+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 77cb57a6bd53:34127 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:03.343+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:03.343+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:03.344+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-18T16:22:03.351+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T16:22:03.360+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2025-07-18T16:22:03.423+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:03.427+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=132 untilOffset=133, for query queryId=a705b2ed-ad57-41ce-9e7a-629c21aaaacc batchId=3 taskId=9 partitionId=0
[2025-07-18T16:22:03.447+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to offset 132 for partition reservations-0
[2025-07-18T16:22:03.452+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:22:03.546+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/3 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.3.bf73b373-2ef6-4a46-85e0-462eb1fb5e4e.tmp
[2025-07-18T16:22:03.631+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.3.bf73b373-2ef6-4a46-85e0-462eb1fb5e4e.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/3
[2025-07-18T16:22:03.638+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855723495,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.671+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.671+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.672+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.688+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.688+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.727+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.728+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.728+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.744+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.752+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.818+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.821+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.826+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.833+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.837+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.845+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.852+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.854+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 77cb57a6bd53:34127 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.855+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 20 from start at <unknown>:0
[2025-07-18T16:22:03.855+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:03.857+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:03.859+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:03.863+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
[2025-07-18T16:22:03.866+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:03.868+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:03.871+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:03.874+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.904+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.909+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 77cb57a6bd53:34127 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.919+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:03.920+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:03.922+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-18T16:22:03.934+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:03.939+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2025-07-18T16:22:03.971+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:03.979+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:22:03.982+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor-1, groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:03.985+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:03.986+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=132 untilOffset=133, for query queryId=6a2789a7-abc5-4ced-8710-95de25958a28 batchId=3 taskId=10 partitionId=0
[2025-07-18T16:22:04.004+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T16:22:04.019+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to offset 132 for partition checkins-0
[2025-07-18T16:22:04.023+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:22:04.068+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T16:22:04.070+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor read 1 records through 1 polls (polled  out 3 records), taking 535293417 nanos, during time span of 631711083 nanos.
[2025-07-18T16:22:04.073+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4677 bytes result sent to driver
[2025-07-18T16:22:04.085+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 734 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.087+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.090+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 9 (start at <unknown>:0) finished in 0.772 s
[2025-07-18T16:22:04.091+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.091+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-18T16:22:04.091+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.781682 s
[2025-07-18T16:22:04.097+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.097+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 3 for query a705b2ed-ad57-41ce-9e7a-629c21aaaacc in append mode
[2025-07-18T16:22:04.131+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/3 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.3.37707ef2-f2e2-4710-949e-00e62ab1f708.tmp
[2025-07-18T16:22:04.212+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.3.37707ef2-f2e2-4710-949e-00e62ab1f708.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/3
[2025-07-18T16:22:04.216+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855724096,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:04.228+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:04.237+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.237+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.238+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.312+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.319+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.329+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 77cb57a6bd53:34127 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.334+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.339+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.342+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.361+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.368+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.369+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.399+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.423+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.618+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.648+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:22:04.686+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor-3, groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.748+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T16:22:04.815+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:04.826+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:22:04.858+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:04.860+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 22 from start at <unknown>:0
[2025-07-18T16:22:04.864+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:04.876+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:04.892+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:04.894+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
[2025-07-18T16:22:04.898+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:04.899+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:04.900+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:04.912+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T16:22:04.913+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor read 1 records through 1 polls (polled  out 3 records), taking 671549209 nanos, during time span of 894306125 nanos.
[2025-07-18T16:22:04.920+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 4908 bytes result sent to driver
[2025-07-18T16:22:04.946+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 1012 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.973+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:22:04.976+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.984+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:22:05.006+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 77cb57a6bd53:34127 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:05.017+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:05.036+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:05.043+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-18T16:22:05.044+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 1.170 s
[2025-07-18T16:22:05.049+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:05.050+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:05.051+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-18T16:22:05.051+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 1.196761 s
[2025-07-18T16:22:05.051+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:05.051+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing epoch 3 for query 6a2789a7-abc5-4ced-8710-95de25958a28 in append mode
[2025-07-18T16:22:05.051+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2025-07-18T16:22:05.097+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:05.098+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=132 untilOffset=133, for query queryId=3e609a08-6175-4a61-a969-0559365755cd batchId=3 taskId=11 partitionId=0
[2025-07-18T16:22:05.101+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to offset 132 for partition feedback-0
[2025-07-18T16:22:05.106+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:22:05.112+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 WARN Tasks: Retrying task after failure: Version 85 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v85.metadata.json
[2025-07-18T16:22:05.113+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 85 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v85.metadata.json
[2025-07-18T16:22:05.113+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:05.113+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:05.114+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:05.114+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:05.115+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:05.116+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.116+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.116+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.116+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.117+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.117+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.117+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.117+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:05.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:05.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:05.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:05.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:05.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:05.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:05.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:05.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:05.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:05.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:05.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:05.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:05.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:05.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.120+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:05.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:05.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:05.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:05.124+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.127+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:05.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:05.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:05.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:05.134+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.136+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:05.136+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:05.137+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.138+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:05.138+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:05.401+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:05.626+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:05.627+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:22:05.631+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor-2, groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:05.655+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T16:22:05.751+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T16:22:05.756+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor read 1 records through 1 polls (polled  out 3 records), taking 530717291 nanos, during time span of 646723000 nanos.
[2025-07-18T16:22:05.759+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 4759 bytes result sent to driver
[2025-07-18T16:22:05.765+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 706 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:05.766+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-18T16:22:05.768+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.842 s
[2025-07-18T16:22:05.771+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:05.775+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-07-18T16:22:05.776+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.902494 s
[2025-07-18T16:22:05.779+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:05.780+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing epoch 3 for query 3e609a08-6175-4a61-a969-0559365755cd in append mode
[2025-07-18T16:22:05.782+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 WARN Tasks: Retrying task after failure: Failed to commit changes using rename: s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json
[2025-07-18T16:22:05.783+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Failed to commit changes using rename: s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json
[2025-07-18T16:22:05.786+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:378)
[2025-07-18T16:22:05.787+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:05.788+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:05.788+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:05.790+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:05.792+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.797+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.817+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.821+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.822+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.826+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.826+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.827+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:05.827+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:05.830+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:05.833+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:05.838+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:05.839+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:05.840+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:05.840+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:05.841+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:05.841+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:05.842+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:05.843+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:05.844+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:05.846+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:05.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.848+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.849+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.850+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.851+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:05.852+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:05.854+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:05.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:05.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:05.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:05.860+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:05.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:05.867+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Failed to rename s3a://warehouse/bronze/Reservations_raw/metadata/6eafbe22-9e25-47d8-ba5a-674ccd73c78a.metadata.json to s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json; destination file exists
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initiateRename(S3AFileSystem.java:1920)
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerRename(S3AFileSystem.java:1988)
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$rename$7(S3AFileSystem.java:1846)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:1844)
[2025-07-18T16:22:05.871+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:368)
[2025-07-18T16:22:05.871+0000] {subprocess.py:93} INFO - 	... 59 more
[2025-07-18T16:22:05.931+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:05.968+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 WARN Tasks: Retrying task after failure: Version 94 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.973+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 94 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.973+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:05.974+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:05.977+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:05.979+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:05.979+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:05.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.982+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.982+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.983+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.988+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.993+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.995+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.997+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:05.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.011+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.016+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.025+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.032+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.042+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.044+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.047+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.048+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.055+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.057+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.063+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.360+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.366+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Version 95 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.367+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 95 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.367+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:06.367+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.367+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.367+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.368+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.368+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.368+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.368+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.370+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.371+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.371+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.371+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.371+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.389+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.389+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.391+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.393+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.439+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Version 87 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v87.metadata.json
[2025-07-18T16:22:06.442+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 87 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v87.metadata.json
[2025-07-18T16:22:06.444+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:06.447+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.448+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.452+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.453+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.455+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.457+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.461+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.463+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.465+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.466+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.468+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.470+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.500+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.502+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.505+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.507+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.511+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.527+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SnapshotProducer: Committed snapshot 8787120888555585603 (FastAppend)
[2025-07-18T16:22:06.805+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=8787120888555585603, sequenceNumber=94, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.867655874S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=94}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=875}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2713}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=309307}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:06.821+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Committed in 889 ms
[2025-07-18T16:22:06.821+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:06.910+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/3 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.3.c8fa93c4-a36a-4665-b788-1e4e5152e9cc.tmp
[2025-07-18T16:22:07.016+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.3.c8fa93c4-a36a-4665-b788-1e4e5152e9cc.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/3
[2025-07-18T16:22:07.026+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:07.027+0000] {subprocess.py:93} INFO -   "id" : "3e609a08-6175-4a61-a969-0559365755cd",
[2025-07-18T16:22:07.027+0000] {subprocess.py:93} INFO -   "runId" : "adc07e88-e29c-4be8-a682-c48f3eee7501",
[2025-07-18T16:22:07.027+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:07.028+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:04.093Z",
[2025-07-18T16:22:07.028+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:22:07.028+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:07.028+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:07.028+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.34223134839151265,
[2025-07-18T16:22:07.029+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:07.029+0000] {subprocess.py:93} INFO -     "addBatch" : 2496,
[2025-07-18T16:22:07.029+0000] {subprocess.py:93} INFO -     "commitOffsets" : 194,
[2025-07-18T16:22:07.029+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:07.030+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:22:07.030+0000] {subprocess.py:93} INFO -     "queryPlanning" : 113,
[2025-07-18T16:22:07.030+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2922,
[2025-07-18T16:22:07.030+0000] {subprocess.py:93} INFO -     "walCommit" : 114
[2025-07-18T16:22:07.030+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:07.030+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:07.031+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:07.031+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:07.031+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:07.031+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:07.032+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:07.032+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.032+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.033+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:07.033+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:07.033+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.034+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.034+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.034+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:07.034+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:07.034+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.034+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.035+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.035+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:07.035+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:07.035+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.34223134839151265,
[2025-07-18T16:22:07.036+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:07.038+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:07.039+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:07.041+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:07.043+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:07.046+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:07.047+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:07.052+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:07.052+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:07.053+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:07.053+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:07.079+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/4 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.4.ed2aa2e8-9bad-41be-a44d-88ed776db864.tmp
[2025-07-18T16:22:07.236+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/.4.ed2aa2e8-9bad-41be-a44d-88ed776db864.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/offsets/4
[2025-07-18T16:22:07.238+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855727035,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:07.254+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.295+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.296+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.299+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.309+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.315+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.348+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.350+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.355+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.391+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.397+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.410+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SnapshotProducer: Committed snapshot 2100617157931994155 (FastAppend)
[2025-07-18T16:22:07.428+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.436+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.436+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.444+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.454+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.462+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v88.metadata.json
[2025-07-18T16:22:07.506+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:07.516+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:07.546+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.548+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 24 from start at <unknown>:0
[2025-07-18T16:22:07.550+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:07.550+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN HadoopTableOperations: Failed to update version hint
[2025-07-18T16:22:07.551+0000] {subprocess.py:93} INFO - org.apache.hadoop.fs.FileAlreadyExistsException: Failed to rename s3a://warehouse/bronze/Reservations_raw/metadata/c2a7f52b-f22d-43a9-8152-44ac1c2f0546-version-hint.temp to s3a://warehouse/bronze/Reservations_raw/metadata/version-hint.text; destination file exists
[2025-07-18T16:22:07.551+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initiateRename(S3AFileSystem.java:1920)
[2025-07-18T16:22:07.552+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerRename(S3AFileSystem.java:1988)
[2025-07-18T16:22:07.553+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$rename$7(S3AFileSystem.java:1846)
[2025-07-18T16:22:07.553+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2025-07-18T16:22:07.554+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2025-07-18T16:22:07.556+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2025-07-18T16:22:07.557+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:1844)
[2025-07-18T16:22:07.558+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.writeVersionHint(HadoopTableOperations.java:300)
[2025-07-18T16:22:07.559+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:167)
[2025-07-18T16:22:07.559+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:07.560+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:07.560+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:07.560+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:07.560+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:07.561+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:07.562+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:07.562+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:07.562+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:07.563+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:07.565+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:07.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:07.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:07.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:07.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:07.570+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:07.570+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:07.570+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:07.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:07.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:07.572+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:07.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:07.573+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:07.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:07.576+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:07.580+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:07.580+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:07.580+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.581+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.581+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.582+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.582+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.582+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.583+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.583+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.583+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.584+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.585+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:07.586+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:07.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:07.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:07.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:07.589+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.589+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.590+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:07.590+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:07.591+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.591+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:07.591+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:07.592+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SnapshotProducer: Committed snapshot 7970488366954130731 (FastAppend)
[2025-07-18T16:22:07.593+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:07.593+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.594+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:07.595+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Final stage: ResultStage 12 (start at <unknown>:0)
[2025-07-18T16:22:07.595+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:07.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:07.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:07.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:07.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 77cb57a6bd53:34127 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.635+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:22:07.637+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 77cb57a6bd53:34127 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.638+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:07.639+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:07.639+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-07-18T16:22:07.650+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:07.650+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2025-07-18T16:22:07.713+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2100617157931994155, sequenceNumber=95, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT2.301863291S, count=1}, attempts=CounterResult{unit=COUNT, value=3}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=95}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=879}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2881}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=304504}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:07.723+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committed in 2299 ms
[2025-07-18T16:22:07.724+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:07.724+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:07.740+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/3 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.3.91aabeab-cec4-434e-841b-eb55164fd485.tmp
[2025-07-18T16:22:07.746+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=133 untilOffset=135, for query queryId=3e609a08-6175-4a61-a969-0559365755cd batchId=4 taskId=12 partitionId=0
[2025-07-18T16:22:07.819+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 77cb57a6bd53:34127 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.883+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T16:22:07.961+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.3.91aabeab-cec4-434e-841b-eb55164fd485.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/3
[2025-07-18T16:22:07.969+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:07.970+0000] {subprocess.py:93} INFO -   "id" : "6a2789a7-abc5-4ced-8710-95de25958a28",
[2025-07-18T16:22:07.972+0000] {subprocess.py:93} INFO -   "runId" : "a27254af-cc69-42a2-89ac-9c81c4cb53cc",
[2025-07-18T16:22:07.973+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:07.973+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:03.485Z",
[2025-07-18T16:22:07.973+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:22:07.974+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:07.975+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:07.975+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.22341376228775692,
[2025-07-18T16:22:07.976+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:07.976+0000] {subprocess.py:93} INFO -     "addBatch" : 4011,
[2025-07-18T16:22:07.977+0000] {subprocess.py:93} INFO -     "commitOffsets" : 259,
[2025-07-18T16:22:07.978+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:07.978+0000] {subprocess.py:93} INFO -     "latestOffset" : 10,
[2025-07-18T16:22:07.978+0000] {subprocess.py:93} INFO -     "queryPlanning" : 69,
[2025-07-18T16:22:07.979+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4476,
[2025-07-18T16:22:07.979+0000] {subprocess.py:93} INFO -     "walCommit" : 125
[2025-07-18T16:22:07.979+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:07.979+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:07.979+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:07.980+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:07.980+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:07.980+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:07.980+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:07.980+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.980+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.982+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:07.983+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:07.983+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.983+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.983+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.983+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:07.984+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:07.984+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.984+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.984+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.984+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:07.984+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:07.985+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.22341376228775692,
[2025-07-18T16:22:07.985+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:07.985+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:07.986+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:07.986+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:07.988+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:07.988+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:07.990+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:07.990+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:07.990+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:07.991+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:07.992+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:08.005+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/4 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.4.b575f092-b60e-4727-9b97-a9e9db0645d7.tmp
[2025-07-18T16:22:08.010+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=7970488366954130731, sequenceNumber=87, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT3.783041001S, count=1}, attempts=CounterResult{unit=COUNT, value=4}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=87}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=879}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2920}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=277785}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:08.011+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committed in 3780 ms
[2025-07-18T16:22:08.011+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:08.047+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T16:22:08.056+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-bb894433-bb09-409b-a660-d0e9c7334a1e--348010276-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 237897917 nanos.
[2025-07-18T16:22:08.058+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/3 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.3.5e2910d4-320c-4d17-af02-0741a3f3f229.tmp
[2025-07-18T16:22:08.061+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 4803 bytes result sent to driver
[2025-07-18T16:22:08.064+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 412 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:08.074+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-07-18T16:22:08.077+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: ResultStage 12 (start at <unknown>:0) finished in 0.494 s
[2025-07-18T16:22:08.081+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:08.086+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-07-18T16:22:08.087+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.510206 s
[2025-07-18T16:22:08.089+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:08.089+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing epoch 4 for query 3e609a08-6175-4a61-a969-0559365755cd in append mode
[2025-07-18T16:22:08.091+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/.4.b575f092-b60e-4727-9b97-a9e9db0645d7.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/offsets/4
[2025-07-18T16:22:08.093+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855727970,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:08.158+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.161+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.163+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.185+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.192+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.3.5e2910d4-320c-4d17-af02-0741a3f3f229.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/3
[2025-07-18T16:22:08.198+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:08.199+0000] {subprocess.py:93} INFO -   "id" : "a705b2ed-ad57-41ce-9e7a-629c21aaaacc",
[2025-07-18T16:22:08.200+0000] {subprocess.py:93} INFO -   "runId" : "d921ac12-16c5-4345-97c8-12fba1de01ef",
[2025-07-18T16:22:08.201+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:08.201+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:02.862Z",
[2025-07-18T16:22:08.201+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:22:08.202+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:08.202+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:08.203+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.18765246762994933,
[2025-07-18T16:22:08.205+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:08.205+0000] {subprocess.py:93} INFO -     "addBatch" : 4904,
[2025-07-18T16:22:08.206+0000] {subprocess.py:93} INFO -     "commitOffsets" : 174,
[2025-07-18T16:22:08.206+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:08.206+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:22:08.206+0000] {subprocess.py:93} INFO -     "queryPlanning" : 111,
[2025-07-18T16:22:08.206+0000] {subprocess.py:93} INFO -     "triggerExecution" : 5329,
[2025-07-18T16:22:08.206+0000] {subprocess.py:93} INFO -     "walCommit" : 125
[2025-07-18T16:22:08.207+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:08.207+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:08.207+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:08.207+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:08.207+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:08.207+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:08.207+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:08.208+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.208+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.209+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:08.209+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:08.209+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:08.209+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.209+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.209+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:08.210+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:08.210+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:08.210+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.210+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.210+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:08.210+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:08.210+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.18765246762994933,
[2025-07-18T16:22:08.211+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:08.211+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:08.211+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:08.211+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:08.211+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:08.211+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:08.211+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:08.212+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:08.212+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:08.212+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:08.212+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:08.213+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.226+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.227+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.228+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.235+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.237+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.240+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/4 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.4.c7802300-91a5-4e18-b424-93d67569cb36.tmp
[2025-07-18T16:22:08.250+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.251+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.252+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.252+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.253+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.265+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:08.266+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:08.268+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:08.269+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 77cb57a6bd53:34127 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.270+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Created broadcast 26 from start at <unknown>:0
[2025-07-18T16:22:08.270+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:08.270+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:08.281+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:08.282+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
[2025-07-18T16:22:08.282+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:08.284+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:08.286+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:08.303+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/.4.c7802300-91a5-4e18-b424-93d67569cb36.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/offsets/4
[2025-07-18T16:22:08.305+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855728205,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:08.310+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:08.321+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.322+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.328+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.330+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:22:08.347+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 77cb57a6bd53:34127 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.357+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:08.376+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:08.377+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-07-18T16:22:08.400+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:08.415+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.427+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2025-07-18T16:22:08.431+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.469+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:08.477+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=133 untilOffset=135, for query queryId=6a2789a7-abc5-4ced-8710-95de25958a28 batchId=4 taskId=13 partitionId=0
[2025-07-18T16:22:08.479+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.480+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.481+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.497+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T16:22:08.506+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.509+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.585+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T16:22:08.602+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d4df938a-a3d7-4b4e-af33-ac11a8be785b-1177457755-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 118265042 nanos.
[2025-07-18T16:22:08.607+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.610+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.612+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:08.614+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.615+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 4856 bytes result sent to driver
[2025-07-18T16:22:08.624+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.645+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 241 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:08.652+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.364 s
[2025-07-18T16:22:08.654+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-18T16:22:08.655+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:08.656+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-07-18T16:22:08.659+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.382086 s
[2025-07-18T16:22:08.662+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:08.663+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing epoch 4 for query 6a2789a7-abc5-4ced-8710-95de25958a28 in append mode
[2025-07-18T16:22:08.716+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:08.722+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.750+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN Tasks: Retrying task after failure: Version 98 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v98.metadata.json
[2025-07-18T16:22:08.753+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 98 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v98.metadata.json
[2025-07-18T16:22:08.754+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:08.756+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:08.757+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:08.757+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:08.757+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:08.760+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:08.763+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:08.767+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:08.769+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:08.770+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:08.771+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:08.773+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:08.775+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:08.777+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:08.778+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:08.778+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:08.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:08.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:08.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:08.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:08.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:08.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:08.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:08.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:08.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:08.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:08.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:08.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:08.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:08.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.796+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:08.802+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.803+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.807+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.808+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.809+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:08.810+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:08.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:08.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:08.813+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.820+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.820+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:08.820+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:08.821+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.830+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:08.832+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:08.846+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T16:22:08.847+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 77cb57a6bd53:34127 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.849+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Created broadcast 28 from start at <unknown>:0
[2025-07-18T16:22:08.851+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:08.858+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:08.862+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:08.863+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Final stage: ResultStage 14 (start at <unknown>:0)
[2025-07-18T16:22:08.871+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:08.874+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:08.877+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:08.882+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T16:22:08.897+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:08.908+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 77cb57a6bd53:34127 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.908+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:08.909+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:08.919+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2025-07-18T16:22:08.925+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T16:22:08.967+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2025-07-18T16:22:09.080+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:09.086+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=133 untilOffset=135, for query queryId=a705b2ed-ad57-41ce-9e7a-629c21aaaacc batchId=4 taskId=14 partitionId=0
[2025-07-18T16:22:09.224+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T16:22:09.283+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 WARN Tasks: Retrying task after failure: Version 98 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v98.metadata.json
[2025-07-18T16:22:09.284+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 98 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v98.metadata.json
[2025-07-18T16:22:09.286+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:09.287+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:09.287+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:09.289+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:09.289+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:09.289+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:09.290+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:09.290+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:09.291+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:09.291+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:09.292+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:09.292+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:09.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:09.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:09.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:09.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:09.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:09.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:09.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:09.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:09.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:09.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:09.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:09.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:09.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:09.300+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.300+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:09.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:09.326+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T16:22:09.327+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d51db8ff-3b81-4f1a-961d-fde582dc6d8c-629959802-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 141841000 nanos.
[2025-07-18T16:22:09.328+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 4699 bytes result sent to driver
[2025-07-18T16:22:09.350+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 426 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:09.363+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO DAGScheduler: ResultStage 14 (start at <unknown>:0) finished in 0.523 s
[2025-07-18T16:22:09.364+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:09.368+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-07-18T16:22:09.385+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2025-07-18T16:22:09.386+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.578910 s
[2025-07-18T16:22:09.393+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:09.393+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SparkWrite: Committing epoch 4 for query a705b2ed-ad57-41ce-9e7a-629c21aaaacc in append mode
[2025-07-18T16:22:09.569+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v99.metadata.json
[2025-07-18T16:22:09.669+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 77cb57a6bd53:34127 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:09.676+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:09.716+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 77cb57a6bd53:34127 in memory (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:09.732+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 77cb57a6bd53:34127 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:09.930+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 WARN Tasks: Retrying task after failure: Version 99 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v99.metadata.json
[2025-07-18T16:22:09.931+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 99 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v99.metadata.json
[2025-07-18T16:22:09.932+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:09.933+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:09.933+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:09.934+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:09.934+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:09.935+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:09.936+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:09.936+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:09.936+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:09.937+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:09.937+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:09.938+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:09.938+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:09.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:09.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:09.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:09.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:09.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:09.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:09.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:09.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:09.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:09.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:09.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:09.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:09.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:09.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:09.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:09.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:09.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.947+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:09.949+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:09.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:09.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:09.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:09.953+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:09.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:09.955+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.955+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:09.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:09.956+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SnapshotProducer: Committed snapshot 3926432809166763324 (FastAppend)
[2025-07-18T16:22:10.105+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=3926432809166763324, sequenceNumber=98, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.838539167S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=98}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=881}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2894}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=320521}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:10.108+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO SparkWrite: Committed in 1839 ms
[2025-07-18T16:22:10.109+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:10.133+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/4 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.4.4a2e736a-ec00-4044-ab62-9177ccbaa691.tmp
[2025-07-18T16:22:10.149+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 WARN Tasks: Retrying task after failure: Version 91 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v91.metadata.json
[2025-07-18T16:22:10.150+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 91 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v91.metadata.json
[2025-07-18T16:22:10.150+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:10.150+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:10.150+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:10.151+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:10.151+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:10.153+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:10.154+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:10.154+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:10.154+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:10.157+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:10.157+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:10.160+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:10.160+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:10.161+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:10.161+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:10.162+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:10.167+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:10.167+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:10.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:10.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:10.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:10.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:10.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:10.168+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:10.169+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:10.169+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:10.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:10.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:10.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:10.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:10.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:10.176+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:10.177+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:10.179+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:10.179+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:10.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:10.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:10.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:10.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:10.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:10.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:10.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:10.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:10.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:10.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:10.195+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:10.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:10.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:10.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:10.196+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:10.196+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:10.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:10.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:10.203+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:10.210+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:10.211+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:10.212+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:10.214+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:10.217+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:10.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:10.220+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/.4.4a2e736a-ec00-4044-ab62-9177ccbaa691.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:16:00+00:00/commits/4
[2025-07-18T16:22:10.222+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:10.228+0000] {subprocess.py:93} INFO -   "id" : "3e609a08-6175-4a61-a969-0559365755cd",
[2025-07-18T16:22:10.229+0000] {subprocess.py:93} INFO -   "runId" : "adc07e88-e29c-4be8-a682-c48f3eee7501",
[2025-07-18T16:22:10.229+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:10.230+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:07.026Z",
[2025-07-18T16:22:10.233+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:22:10.234+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:10.234+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.6818956699624957,
[2025-07-18T16:22:10.234+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6279434850863422,
[2025-07-18T16:22:10.235+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:10.235+0000] {subprocess.py:93} INFO -     "addBatch" : 2782,
[2025-07-18T16:22:10.236+0000] {subprocess.py:93} INFO -     "commitOffsets" : 103,
[2025-07-18T16:22:10.237+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:22:10.238+0000] {subprocess.py:93} INFO -     "latestOffset" : 8,
[2025-07-18T16:22:10.239+0000] {subprocess.py:93} INFO -     "queryPlanning" : 86,
[2025-07-18T16:22:10.242+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3185,
[2025-07-18T16:22:10.247+0000] {subprocess.py:93} INFO -     "walCommit" : 201
[2025-07-18T16:22:10.251+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:10.258+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:10.260+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:10.261+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:10.263+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:10.266+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:10.267+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:10.268+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.268+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.269+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:10.272+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:10.274+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:10.277+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.278+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.278+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:10.278+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:10.279+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:10.281+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.282+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.282+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:10.282+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.6818956699624957,
[2025-07-18T16:22:10.282+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6279434850863422,
[2025-07-18T16:22:10.282+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:10.283+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:10.283+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:10.283+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:10.283+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:10.283+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:10.283+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:10.284+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:10.284+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:10.284+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:10.284+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:10.719+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v92.metadata.json
[2025-07-18T16:22:10.726+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v100.metadata.json
[2025-07-18T16:22:10.802+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO SnapshotProducer: Committed snapshot 6255138513321942750 (FastAppend)
[2025-07-18T16:22:10.842+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO SnapshotProducer: Committed snapshot 3308165441520702722 (FastAppend)
[2025-07-18T16:22:11.058+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6255138513321942750, sequenceNumber=91, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.325509376S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=91}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=886}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3052}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=289861}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:11.059+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committed in 1327 ms
[2025-07-18T16:22:11.060+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:11.062+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/4 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.4.176c8e75-71f4-456c-b097-2e0de146cdc3.tmp
[2025-07-18T16:22:11.092+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/.4.176c8e75-71f4-456c-b097-2e0de146cdc3.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:16:00+00:00/commits/4
[2025-07-18T16:22:11.094+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=3308165441520702722, sequenceNumber=99, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT2.366280376S, count=1}, attempts=CounterResult{unit=COUNT, value=3}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=99}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=886}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=316232}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855489656, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:11.094+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committed in 2366 ms
[2025-07-18T16:22:11.096+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:11.097+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:11.098+0000] {subprocess.py:93} INFO -   "id" : "a705b2ed-ad57-41ce-9e7a-629c21aaaacc",
[2025-07-18T16:22:11.099+0000] {subprocess.py:93} INFO -   "runId" : "d921ac12-16c5-4345-97c8-12fba1de01ef",
[2025-07-18T16:22:11.101+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:11.102+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:08.197Z",
[2025-07-18T16:22:11.103+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:22:11.104+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:11.104+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.37488284910965325,
[2025-07-18T16:22:11.105+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6927606511950121,
[2025-07-18T16:22:11.105+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:11.106+0000] {subprocess.py:93} INFO -     "addBatch" : 2556,
[2025-07-18T16:22:11.106+0000] {subprocess.py:93} INFO -     "commitOffsets" : 80,
[2025-07-18T16:22:11.107+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:11.110+0000] {subprocess.py:93} INFO -     "latestOffset" : 8,
[2025-07-18T16:22:11.110+0000] {subprocess.py:93} INFO -     "queryPlanning" : 148,
[2025-07-18T16:22:11.111+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2887,
[2025-07-18T16:22:11.111+0000] {subprocess.py:93} INFO -     "walCommit" : 92
[2025-07-18T16:22:11.112+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:11.113+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:11.114+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:11.114+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:11.115+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:11.115+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:11.116+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:11.117+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.117+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.119+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:11.120+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:11.121+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:11.122+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.124+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.127+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:11.129+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:11.131+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:11.134+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.135+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.138+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:11.139+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.37488284910965325,
[2025-07-18T16:22:11.141+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6927606511950121,
[2025-07-18T16:22:11.141+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:11.141+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:11.141+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:11.141+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:11.142+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:11.143+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:11.143+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:11.145+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:11.146+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:11.147+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:11.148+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:11.149+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/4 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.4.e01f9f52-751a-4151-8685-76ba8ebe76b1.tmp
[2025-07-18T16:22:11.176+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/.4.e01f9f52-751a-4151-8685-76ba8ebe76b1.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:16:00+00:00/commits/4
[2025-07-18T16:22:11.177+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:11.177+0000] {subprocess.py:93} INFO -   "id" : "6a2789a7-abc5-4ced-8710-95de25958a28",
[2025-07-18T16:22:11.178+0000] {subprocess.py:93} INFO -   "runId" : "a27254af-cc69-42a2-89ac-9c81c4cb53cc",
[2025-07-18T16:22:11.178+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:11.178+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:07.969Z",
[2025-07-18T16:22:11.178+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:22:11.180+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:11.181+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.44603033006244425,
[2025-07-18T16:22:11.182+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6246096189881324,
[2025-07-18T16:22:11.182+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:11.182+0000] {subprocess.py:93} INFO -     "addBatch" : 2878,
[2025-07-18T16:22:11.183+0000] {subprocess.py:93} INFO -     "commitOffsets" : 85,
[2025-07-18T16:22:11.183+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:11.183+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:22:11.183+0000] {subprocess.py:93} INFO -     "queryPlanning" : 123,
[2025-07-18T16:22:11.183+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3202,
[2025-07-18T16:22:11.183+0000] {subprocess.py:93} INFO -     "walCommit" : 114
[2025-07-18T16:22:11.183+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:11.184+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:11.184+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:11.184+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:11.184+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:11.184+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:11.184+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:11.184+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.185+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.185+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:11.185+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:11.185+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:11.185+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.185+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.185+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:11.186+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:11.186+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:11.186+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.186+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.186+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:11.186+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.44603033006244425,
[2025-07-18T16:22:11.187+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6246096189881324,
[2025-07-18T16:22:11.187+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:11.187+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:11.187+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:11.187+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:11.187+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:11.188+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:11.188+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:11.188+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:11.188+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:11.188+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:11.188+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:20.232+0000] {subprocess.py:93} INFO - 25/07/18 16:22:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:21.090+0000] {subprocess.py:93} INFO - 25/07/18 16:22:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:21.176+0000] {subprocess.py:93} INFO - 25/07/18 16:22:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:30.333+0000] {subprocess.py:93} INFO - 25/07/18 16:22:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:31.102+0000] {subprocess.py:93} INFO - 25/07/18 16:22:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:31.191+0000] {subprocess.py:93} INFO - 25/07/18 16:22:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:31.328+0000] {subprocess.py:93} INFO - 25/07/18 16:22:31 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:31.343+0000] {subprocess.py:93} INFO - 25/07/18 16:22:31 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 77cb57a6bd53:34127 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:31.352+0000] {subprocess.py:93} INFO - 25/07/18 16:22:31 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:31.359+0000] {subprocess.py:93} INFO - 25/07/18 16:22:31 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 77cb57a6bd53:34127 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:22:31.368+0000] {subprocess.py:93} INFO - 25/07/18 16:22:31 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 77cb57a6bd53:34127 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:42.765+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.839+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.854+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:56.559+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:57.639+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:00.184+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:07.070+0000] {subprocess.py:93} INFO - 25/07/18 16:23:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:07.206+0000] {subprocess.py:93} INFO - 25/07/18 16:23:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:07.210+0000] {subprocess.py:93} INFO - 25/07/18 16:23:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:07.429+0000] {subprocess.py:93} INFO - 25/07/18 16:23:07 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 77cb57a6bd53:37467 in 10000 milliseconds
[2025-07-18T16:23:14.265+0000] {subprocess.py:93} INFO - 25/07/18 16:23:14 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
[2025-07-18T16:23:14.270+0000] {subprocess.py:93} INFO - 25/07/18 16:23:14 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
[2025-07-18T16:23:14.271+0000] {subprocess.py:93} INFO - 25/07/18 16:23:14 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2025-07-18T16:23:16.878+0000] {subprocess.py:93} INFO - 25/07/18 16:23:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:16.887+0000] {subprocess.py:93} INFO - 25/07/18 16:23:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:16.897+0000] {subprocess.py:93} INFO - 25/07/18 16:23:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:26.873+0000] {subprocess.py:93} INFO - 25/07/18 16:23:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:26.890+0000] {subprocess.py:93} INFO - 25/07/18 16:23:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:26.905+0000] {subprocess.py:93} INFO - 25/07/18 16:23:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:38.868+0000] {subprocess.py:93} INFO - 25/07/18 16:23:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:40.037+0000] {subprocess.py:93} INFO - 25/07/18 16:23:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:42.962+0000] {subprocess.py:93} INFO - 25/07/18 16:23:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:55.492+0000] {subprocess.py:93} INFO - 25/07/18 16:23:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:01.127+0000] {subprocess.py:93} INFO - 25/07/18 16:23:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:06.740+0000] {subprocess.py:93} INFO - 25/07/18 16:23:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:09.865+0000] {subprocess.py:93} INFO - 25/07/18 16:23:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:02.861+0000] {subprocess.py:93} INFO - 25/07/18 16:24:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:06.274+0000] {subprocess.py:93} INFO - 25/07/18 16:24:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:08.514+0000] {subprocess.py:93} INFO - 25/07/18 16:24:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:32.538+0000] {subprocess.py:93} INFO - 25/07/18 16:25:19 INFO NetworkClient: [AdminClient clientId=adminclient-3] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:32.692+0000] {subprocess.py:93} INFO - 25/07/18 16:25:22 INFO NetworkClient: [AdminClient clientId=adminclient-3] Cancelled in-flight LIST_OFFSETS request with correlation id 59661 due to node 1 being disconnected (elapsed time since creation: 50775ms, elapsed time since send: 50775ms, request timeout: 30000ms)
[2025-07-18T16:25:33.069+0000] {subprocess.py:93} INFO - 25/07/18 16:25:31 INFO NetworkClient: [AdminClient clientId=adminclient-2] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:33.127+0000] {subprocess.py:93} INFO - 25/07/18 16:25:32 INFO NetworkClient: [AdminClient clientId=adminclient-2] Cancelled in-flight METADATA request with correlation id 60027 due to node 1 being disconnected (elapsed time since creation: 37742ms, elapsed time since send: 37742ms, request timeout: 30000ms)
[2025-07-18T16:25:33.938+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@5097b815)) by listener AppStatusListener took 1.479866209s.
[2025-07-18T16:25:33.963+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:33.980+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets on broker 1, deadlineMs=1752855904897, tries=1, nextAllowedTryMs=1752855923929) timed out at 1752855923829 after 1 attempt(s)
[2025-07-18T16:25:34.000+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.006+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.020+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.036+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.037+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.039+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.043+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.047+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.054+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.076+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.079+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.082+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.083+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.121+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.132+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.140+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.144+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.145+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.150+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.151+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.152+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.156+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.175+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.198+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.214+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.219+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.229+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.234+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.253+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.268+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.296+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.300+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.313+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=listOffsets on broker 1, deadlineMs=1752855904897, tries=1, nextAllowedTryMs=1752855923929) timed out at 1752855923829 after 1 attempt(s)
[2025-07-18T16:25:34.315+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled listOffsets on broker 1 request with correlation id 59661 due to node 1 being disconnected
[2025-07-18T16:25:34.318+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.322+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855920703, tries=14, nextAllowedTryMs=1752855932367) timed out at 1752855932267 after 14 attempt(s)
[2025-07-18T16:25:34.327+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.348+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.359+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.389+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.439+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.442+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.448+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.454+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.456+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.459+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.461+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.464+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.465+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.467+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.474+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.483+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.492+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.522+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.537+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.538+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.541+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.541+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.542+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.542+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.545+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.546+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855920703, tries=14, nextAllowedTryMs=1752855932367) timed out at 1752855932267 after 14 attempt(s)
[2025-07-18T16:25:34.546+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 60027 due to node 1 being disconnected
[2025-07-18T16:25:34.552+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO KafkaAdminClient: [AdminClient clientId=adminclient-1] Disconnecting from 1 due to timeout while awaiting Call(callName=metadata, deadlineMs=1752855934066, tries=8, nextAllowedTryMs=1752855933938)
[2025-07-18T16:25:34.558+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-1] Client requested disconnect from node 1
[2025-07-18T16:25:34.567+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-1] Cancelled in-flight METADATA request with correlation id 59299 due to node 1 being disconnected (elapsed time since creation: 151ms, elapsed time since send: 151ms, request timeout: 98ms)
[2025-07-18T16:25:34.569+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.570+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855934066, tries=9, nextAllowedTryMs=1752855934224) timed out at 1752855934124 after 9 attempt(s)
[2025-07-18T16:25:34.570+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.570+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.571+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.581+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.582+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.583+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.584+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.585+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.600+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.616+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.625+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.642+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.661+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.668+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.669+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.672+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.677+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.683+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.689+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.698+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.703+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.710+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.713+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.721+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.725+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.727+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.753+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.758+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.759+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.760+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.762+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.771+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.774+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.784+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.791+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855934066, tries=9, nextAllowedTryMs=1752855934224) timed out at 1752855934124 after 9 attempt(s)
[2025-07-18T16:25:34.797+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 59299 due to node 1 being disconnected
[2025-07-18T16:25:34.919+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-07-18T16:25:34.921+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
[2025-07-18T16:25:34.977+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:34.981+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:34.983+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:34.984+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:34.988+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:34.990+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.007+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.008+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.013+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.015+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.016+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.016+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.017+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.018+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.019+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.021+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.049+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.056+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.060+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.070+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.071+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.072+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.074+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.079+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.081+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.084+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.087+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.093+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.094+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.097+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.101+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.106+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.107+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.108+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.108+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.110+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.114+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.115+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.117+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.117+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.119+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.123+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.124+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.127+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.128+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.128+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.151+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.163+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.175+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.179+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.181+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.181+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.182+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.182+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.182+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.183+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.184+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.184+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.185+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.185+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.186+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.186+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.187+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.194+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.197+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.202+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.204+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.205+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.205+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.205+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.206+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.208+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.208+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.209+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.211+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.212+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.212+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.212+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.212+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.213+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.213+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.215+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.215+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.216+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.217+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.229+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.231+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.233+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.248+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.258+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.260+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.263+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.264+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.264+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.264+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.266+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.266+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.266+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.267+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.267+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.269+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.270+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.271+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.271+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.271+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.272+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.272+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.273+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.273+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.273+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.273+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.273+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.285+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.303+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.303+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.303+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.303+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.304+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.305+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.305+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.306+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.306+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.307+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.307+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.307+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.307+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.308+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.308+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.308+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.309+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.309+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.310+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.311+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.311+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.311+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.311+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.311+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.311+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.312+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.312+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.312+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.312+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.315+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935081
[2025-07-18T16:25:35.316+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.316+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.316+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.316+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935094
[2025-07-18T16:25:35.317+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-07-18T16:25:35.317+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:35.317+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:35.328+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.329+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:35.329+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.330+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.331+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.336+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.337+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.338+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.339+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.339+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.342+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.344+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.346+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.346+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.347+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.350+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.351+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.353+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.356+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.361+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.362+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.362+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.362+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.363+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.363+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.363+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.363+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.363+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.365+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.365+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.365+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.366+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.366+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.367+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.367+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.369+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.370+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.382+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.385+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.387+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.390+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.391+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.394+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.426+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.433+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.436+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.444+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.487+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.487+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.564+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.567+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.571+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.573+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.575+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.575+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.575+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.576+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.576+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.577+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.578+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.579+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.586+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.587+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.592+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.593+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.594+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.600+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.601+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.606+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.619+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.628+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.629+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.629+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.630+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.630+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.631+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.631+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935260
[2025-07-18T16:25:35.632+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:35.637+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:44.869+0000] {local_task_job_runner.py:294} WARNING - State of this instance has been externally set to scheduled. Terminating instance.
[2025-07-18T16:25:44.913+0000] {process_utils.py:131} INFO - Sending 15 to group 1318. PIDs of all processes in the group: [1319, 1318]
[2025-07-18T16:25:44.914+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 1318
[2025-07-18T16:25:44.920+0000] {taskinstance.py:1632} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-07-18T16:25:44.923+0000] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2025-07-18T16:25:45.076+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/subprocess.py", line 91, in run_command
    for raw_line in iter(self.sub_process.stdout.readline, b""):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1634, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2025-07-18T16:25:45.152+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=restaurant_pipeline, task_id=stream_to_bronze, execution_date=20250718T161600, start_date=20250718T161805, end_date=20250718T162545
[2025-07-18T16:25:45.294+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 186 for task stream_to_bronze (Task received SIGTERM signal; 1318)
[2025-07-18T16:25:45.408+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1319, status='terminated', started='16:18:05') (1319) terminated with exit code None
[2025-07-18T16:25:45.421+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1318, status='terminated', exitcode=1, started='16:18:05') (1318) terminated with exit code 1
