[2025-07-18T15:24:05.197+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T15:22:00+00:00 [queued]>
[2025-07-18T15:24:05.201+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T15:22:00+00:00 [queued]>
[2025-07-18T15:24:05.201+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-07-18T15:24:05.207+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): stream_to_bronze> on 2025-07-18 15:22:00+00:00
[2025-07-18T15:24:05.210+0000] {standard_task_runner.py:57} INFO - Started process 297 to run task
[2025-07-18T15:24:05.212+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'restaurant_pipeline', 'stream_to_bronze', 'scheduled__2025-07-18T15:22:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_pipeline.py', '--cfg-path', '/tmp/tmp0tf23i95']
[2025-07-18T15:24:05.214+0000] {standard_task_runner.py:85} INFO - Job 99: Subtask stream_to_bronze
[2025-07-18T15:24:05.241+0000] {task_command.py:416} INFO - Running <TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T15:22:00+00:00 [running]> on host 9bcfb43e0ab7
[2025-07-18T15:24:05.277+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='moran' AIRFLOW_CTX_DAG_ID='restaurant_pipeline' AIRFLOW_CTX_TASK_ID='stream_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-07-18T15:22:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T15:22:00+00:00'
[2025-07-18T15:24:05.278+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-07-18T15:24:05.279+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec spark-iceberg spark-submit /home/iceberg/spark/stream_to_bronze.py']
[2025-07-18T15:24:05.283+0000] {subprocess.py:86} INFO - Output:
[2025-07-18T15:24:06.797+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SparkContext: Running Spark version 3.5.6
[2025-07-18T15:24:06.802+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T15:24:06.804+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SparkContext: Java version 17.0.15
[2025-07-18T15:24:06.824+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO ResourceUtils: ==============================================================
[2025-07-18T15:24:06.825+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-18T15:24:06.826+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO ResourceUtils: ==============================================================
[2025-07-18T15:24:06.826+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SparkContext: Submitted application: StreamToBronze
[2025-07-18T15:24:06.862+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-18T15:24:06.863+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO ResourceProfile: Limiting resource is cpu
[2025-07-18T15:24:06.874+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-18T15:24:06.922+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SecurityManager: Changing view acls to: root,spark
[2025-07-18T15:24:06.926+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SecurityManager: Changing modify acls to: root,spark
[2025-07-18T15:24:06.927+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SecurityManager: Changing view acls groups to:
[2025-07-18T15:24:06.928+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SecurityManager: Changing modify acls groups to:
[2025-07-18T15:24:06.928+0000] {subprocess.py:93} INFO - 25/07/18 15:24:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
[2025-07-18T15:24:07.016+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-18T15:24:07.312+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Utils: Successfully started service 'sparkDriver' on port 43819.
[2025-07-18T15:24:07.329+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO SparkEnv: Registering MapOutputTracker
[2025-07-18T15:24:07.362+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-18T15:24:07.379+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-18T15:24:07.382+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-18T15:24:07.382+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-18T15:24:07.416+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c184d2fd-885d-4950-aca4-8b5938168aab
[2025-07-18T15:24:07.429+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-18T15:24:07.450+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-18T15:24:07.549+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-18T15:24:07.611+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-07-18T15:24:07.697+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Executor: Starting executor ID driver on host 77cb57a6bd53
[2025-07-18T15:24:07.698+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T15:24:07.698+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Executor: Java version 17.0.15
[2025-07-18T15:24:07.703+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-18T15:24:07.708+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1a1f4c79 for default.
[2025-07-18T15:24:07.723+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38337.
[2025-07-18T15:24:07.725+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO NettyBlockTransferService: Server created on 77cb57a6bd53:38337
[2025-07-18T15:24:07.726+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-18T15:24:07.733+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 77cb57a6bd53, 38337, None)
[2025-07-18T15:24:07.736+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO BlockManagerMasterEndpoint: Registering block manager 77cb57a6bd53:38337 with 434.4 MiB RAM, BlockManagerId(driver, 77cb57a6bd53, 38337, None)
[2025-07-18T15:24:07.741+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 77cb57a6bd53, 38337, None)
[2025-07-18T15:24:07.741+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 77cb57a6bd53, 38337, None)
[2025-07-18T15:24:07.948+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-18T15:24:07.952+0000] {subprocess.py:93} INFO - 25/07/18 15:24:07 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-07-18T15:24:08.874+0000] {subprocess.py:93} INFO - 25/07/18 15:24:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-07-18T15:24:08.881+0000] {subprocess.py:93} INFO - 25/07/18 15:24:08 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-07-18T15:24:08.881+0000] {subprocess.py:93} INFO - 25/07/18 15:24:08 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-07-18T15:24:09.456+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:09.468+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-07-18T15:24:09.491+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/reservations resolved to file:/tmp/checkpoints/reservations.
[2025-07-18T15:24:09.491+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:24:09.559+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Starting [id = 0314df7c-5598-4928-8d91-374ee67989d1, runId = c6517251-9527-4d66-968c-7d04d13cb56e]. Use file:/tmp/checkpoints/reservations to store the query checkpoint.
[2025-07-18T15:24:09.565+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@3f4f070] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4d20dbf6]
[2025-07-18T15:24:09.592+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4
[2025-07-18T15:24:09.600+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: Getting latest batch 4
[2025-07-18T15:24:09.625+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4
[2025-07-18T15:24:09.626+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: Getting latest batch 4
[2025-07-18T15:24:09.630+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO CommitLog: BatchIds found from listing: 0, 1, 2, 3
[2025-07-18T15:24:09.631+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO CommitLog: Getting latest batch 3
[2025-07-18T15:24:09.634+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Resuming at batch 4 with committed offsets {KafkaV2[Subscribe[reservations]]: {"reservations":{"0":40}}} and available offsets {KafkaV2[Subscribe[reservations]]: {"reservations":{"0":42}}}
[2025-07-18T15:24:09.634+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[reservations]]: {"reservations":{"0":40}}}
[2025-07-18T15:24:09.655+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:09.657+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/checkins resolved to file:/tmp/checkpoints/checkins.
[2025-07-18T15:24:09.658+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:24:09.664+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Starting [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d]. Use file:/tmp/checkpoints/checkins to store the query checkpoint.
[2025-07-18T15:24:09.665+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@40e5420d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4e22a5b9]
[2025-07-18T15:24:09.667+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3
[2025-07-18T15:24:09.667+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: Getting latest batch 3
[2025-07-18T15:24:09.669+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3
[2025-07-18T15:24:09.669+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: Getting latest batch 3
[2025-07-18T15:24:09.673+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO CommitLog: BatchIds found from listing: 0, 1, 2
[2025-07-18T15:24:09.674+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO CommitLog: Getting latest batch 2
[2025-07-18T15:24:09.674+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Resuming at batch 3 with committed offsets {KafkaV2[Subscribe[checkins]]: {"checkins":{"0":39}}} and available offsets {KafkaV2[Subscribe[checkins]]: {"checkins":{"0":40}}}
[2025-07-18T15:24:09.674+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[checkins]]: {"checkins":{"0":39}}}
[2025-07-18T15:24:09.771+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:09.773+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/feedback resolved to file:/tmp/checkpoints/feedback.
[2025-07-18T15:24:09.773+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:24:09.780+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Starting [id = d3aff090-24bc-4a1c-938f-fc839231598c, runId = 16c6e454-9d9c-4acb-9866-9460928e2151]. Use file:/tmp/checkpoints/feedback to store the query checkpoint.
[2025-07-18T15:24:09.782+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@27d73abe] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2910c1ec]
[2025-07-18T15:24:09.784+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3
[2025-07-18T15:24:09.784+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: Getting latest batch 3
[2025-07-18T15:24:09.785+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3
[2025-07-18T15:24:09.785+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO OffsetSeqLog: Getting latest batch 3
[2025-07-18T15:24:09.789+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO CommitLog: BatchIds found from listing: 0, 1, 2, 3
[2025-07-18T15:24:09.793+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO CommitLog: Getting latest batch 3
[2025-07-18T15:24:09.794+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Resuming at batch 4 with committed offsets {KafkaV2[Subscribe[feedback]]: {"feedback":{"0":39}}} and available offsets {KafkaV2[Subscribe[feedback]]: {"feedback":{"0":39}}}
[2025-07-18T15:24:09.794+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[feedback]]: {"feedback":{"0":39}}}
[2025-07-18T15:24:09.814+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:24:09.815+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:24:09.816+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:24:09.817+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:24:09.818+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:24:09.819+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:24:09.820+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:24:09.820+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:24:09.820+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:24:09.820+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:24:09.820+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:24:09.820+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:24:09.821+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:24:09.822+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:24:09.843+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:09.844+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:09.844+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:09.844+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:09.845+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:09.846+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:09.848+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:24:09.849+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:24:09.849+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:24:09.850+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO AppInfoParser: Kafka startTimeMs: 1752852249847
[2025-07-18T15:24:09.935+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:09.936+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:09.969+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:09.969+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:09.995+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:09.996+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:09.996+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:10.001+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:10.002+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:10.002+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:10.002+0000] {subprocess.py:93} INFO - 25/07/18 15:24:09 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.003+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.004+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.007+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.050+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:10.052+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:10.053+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:10.053+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:10.053+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:10.053+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:10.053+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.062+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.063+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.063+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.114+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/4 using temp file file:/tmp/checkpoints/feedback/offsets/.4.775bec89-8837-4fbe-86ab-dff827215afc.tmp
[2025-07-18T15:24:10.141+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.4.775bec89-8837-4fbe-86ab-dff827215afc.tmp to file:/tmp/checkpoints/feedback/offsets/4
[2025-07-18T15:24:10.142+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752852250091,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:24:10.154+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.154+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.155+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.161+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.163+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.170+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.170+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.170+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.175+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.178+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.191+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.192+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.192+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:10.192+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.192+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:10.393+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 146.377208 ms
[2025-07-18T15:24:10.393+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 147.3885 ms
[2025-07-18T15:24:10.394+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 147.083959 ms
[2025-07-18T15:24:10.522+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:24:10.523+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:24:10.525+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:24:10.551+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:24:10.552+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:24:10.552+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:24:10.552+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:24:10.554+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:24:10.555+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Created broadcast 1 from start at <unknown>:0
[2025-07-18T15:24:10.555+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:24:10.556+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Created broadcast 0 from start at <unknown>:0
[2025-07-18T15:24:10.556+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Created broadcast 2 from start at <unknown>:0
[2025-07-18T15:24:10.556+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:24:10.557+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:24:10.557+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:24:10.573+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:24:10.573+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:24:10.573+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:24:10.579+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:24:10.580+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-07-18T15:24:10.580+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:24:10.580+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:24:10.582+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:24:10.635+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T15:24:10.641+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.2 MiB)
[2025-07-18T15:24:10.644+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 77cb57a6bd53:38337 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:24:10.644+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:24:10.649+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:24:10.649+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-18T15:24:10.660+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:24:10.660+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-07-18T15:24:10.660+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:24:10.660+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:24:10.660+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:24:10.666+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T15:24:10.667+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T15:24:10.667+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:24:10.668+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:24:10.669+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:24:10.669+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-18T15:24:10.690+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:24:10.698+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:24:10.700+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
[2025-07-18T15:24:10.701+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:24:10.701+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:24:10.702+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:24:10.702+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:24:10.702+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:24:10.709+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:24:10.710+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:24:10.710+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:24:10.711+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-18T15:24:10.711+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:24:10.712+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-18T15:24:10.712+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-18T15:24:10.713+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:24:10.713+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-18T15:24:10.866+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 17.653667 ms
[2025-07-18T15:24:10.866+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 17.651709 ms
[2025-07-18T15:24:10.891+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 49.359791 ms
[2025-07-18T15:24:10.902+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 41.030584 ms
[2025-07-18T15:24:10.918+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 62.1555 ms
[2025-07-18T15:24:10.929+0000] {subprocess.py:93} INFO - 25/07/18 15:24:10 INFO CodeGenerator: Code generated in 40.061292 ms
[2025-07-18T15:24:11.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:24:11.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:24:11.016+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:24:11.209+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=39 untilOffset=40, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=3 taskId=1 partitionId=0
[2025-07-18T15:24:11.211+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=39 untilOffset=42, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=4 taskId=0 partitionId=0
[2025-07-18T15:24:11.214+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=40 untilOffset=42, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=4 taskId=2 partitionId=0
[2025-07-18T15:24:11.264+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO CodeGenerator: Code generated in 19.116875 ms
[2025-07-18T15:24:11.296+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO CodeGenerator: Code generated in 16.760375 ms
[2025-07-18T15:24:11.324+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:24:11.327+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:24:11.330+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:24:11.331+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:24:11.331+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:24:11.331+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:24:11.331+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:24:11.331+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:24:11.331+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2
[2025-07-18T15:24:11.331+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor
[2025-07-18T15:24:11.332+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:24:11.333+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:24:11.334+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:24:11.334+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:24:11.334+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:24:11.336+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:24:11.338+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:24:11.338+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:24:11.338+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:24:11.339+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:24:11.339+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:24:11.339+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:24:11.339+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:24:11.339+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:24:11.339+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:24:11.340+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:24:11.341+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:24:11.342+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:24:11.342+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:24:11.342+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:24:11.342+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:24:11.342+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:24:11.347+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:24:11.348+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:24:11.352+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:24:11.352+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:24:11.352+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:24:11.353+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:24:11.354+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:24:11.355+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:24:11.356+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:24:11.357+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:24:11.359+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:24:11.359+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:24:11.359+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:24:11.359+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:24:11.360+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:24:11.361+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:24:11.361+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:24:11.361+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:24:11.361+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:24:11.361+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:24:11.363+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:24:11.364+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:24:11.364+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:24:11.364+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:24:11.364+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:24:11.364+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:24:11.364+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:24:11.364+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:24:11.365+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:24:11.366+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:24:11.367+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:24:11.368+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:24:11.368+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:24:11.368+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:24:11.368+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:24:11.368+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:24:11.369+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:24:11.369+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:24:11.370+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:24:11.371+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:24:11.371+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:24:11.371+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:24:11.371+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:24:11.372+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:24:11.372+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:24:11.374+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:24:11.374+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:24:11.375+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:24:11.375+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:24:11.375+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:24:11.375+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:24:11.375+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:24:11.375+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:24:11.377+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:24:11.378+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:24:11.378+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:24:11.378+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:24:11.378+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:24:11.379+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:24:11.379+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:24:11.380+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:24:11.380+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:24:11.381+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:24:11.383+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:24:11.384+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor
[2025-07-18T15:24:11.384+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:24:11.384+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:24:11.385+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:24:11.385+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:24:11.387+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:24:11.387+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:24:11.388+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:24:11.389+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:24:11.390+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:24:11.391+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:24:11.391+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:24:11.392+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:24:11.392+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:24:11.394+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:24:11.394+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:24:11.394+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:24:11.394+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:24:11.394+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:24:11.395+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:24:11.396+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:24:11.396+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:24:11.396+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:24:11.398+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:24:11.399+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:24:11.400+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:24:11.401+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:24:11.401+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:24:11.406+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:24:11.406+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:24:11.406+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:24:11.407+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka startTimeMs: 1752852251383
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka startTimeMs: 1752852251384
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO AppInfoParser: Kafka startTimeMs: 1752852251384
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Assigned to partition(s): checkins-0
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Assigned to partition(s): feedback-0
[2025-07-18T15:24:11.408+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Assigned to partition(s): reservations-0
[2025-07-18T15:24:11.409+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 39 for partition checkins-0
[2025-07-18T15:24:11.409+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 39 for partition feedback-0
[2025-07-18T15:24:11.409+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 40 for partition reservations-0
[2025-07-18T15:24:11.409+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:24:11.409+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:24:11.409+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:24:11.443+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:24:11.444+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:24:11.444+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:24:11.948+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:11.950+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:11.951+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:11.951+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:24:11.951+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:24:11.952+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:24:11.952+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:11.952+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:11.952+0000] {subprocess.py:93} INFO - 25/07/18 15:24:11 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=42, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:24:12.047+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T15:24:12.047+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T15:24:12.047+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T15:24:12.238+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T15:24:12.239+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T15:24:12.240+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 1 records through 1 polls (polled  out 3 records), taking 558954709 nanos, during time span of 848118876 nanos.
[2025-07-18T15:24:12.240+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 2 records through 1 polls (polled  out 2 records), taking 557624917 nanos, during time span of 849432376 nanos.
[2025-07-18T15:24:12.259+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T15:24:12.260+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor read 3 records through 1 polls (polled  out 3 records), taking 557086792 nanos, during time span of 862961501 nanos.
[2025-07-18T15:24:12.261+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4921 bytes result sent to driver
[2025-07-18T15:24:12.261+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4809 bytes result sent to driver
[2025-07-18T15:24:12.261+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4777 bytes result sent to driver
[2025-07-18T15:24:12.282+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1568 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:24:12.283+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-18T15:24:12.283+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1581 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:24:12.284+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-18T15:24:12.284+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1613 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:24:12.285+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-18T15:24:12.299+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 1.593 s
[2025-07-18T15:24:12.299+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:24:12.299+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-18T15:24:12.299+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.724523 s
[2025-07-18T15:24:12.300+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:24:12.300+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committing epoch 4 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:24:12.301+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.641 s
[2025-07-18T15:24:12.305+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:24:12.306+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-18T15:24:12.309+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.720 s
[2025-07-18T15:24:12.310+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.735876 s
[2025-07-18T15:24:12.310+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:24:12.310+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committing epoch 3 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:24:12.311+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:24:12.312+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-18T15:24:12.315+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 1.740881 s
[2025-07-18T15:24:12.321+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:24:12.321+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committing epoch 4 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:24:12.363+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:24:12.363+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:24:12.370+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:12.723+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v5.metadata.json
[2025-07-18T15:24:12.725+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v6.metadata.json
[2025-07-18T15:24:12.725+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v6.metadata.json
[2025-07-18T15:24:12.784+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SnapshotProducer: Committed snapshot 6220694286359342826 (FastAppend)
[2025-07-18T15:24:12.784+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SnapshotProducer: Committed snapshot 180950617574113912 (FastAppend)
[2025-07-18T15:24:12.804+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SnapshotProducer: Committed snapshot 6846507847969457420 (FastAppend)
[2025-07-18T15:24:12.848+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=180950617574113912, sequenceNumber=5, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.4666965S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=42}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3025}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=15930}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:24:12.848+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=6220694286359342826, sequenceNumber=4, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.463943084S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=4}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=40}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2979}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=13289}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:24:12.849+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committed in 481 ms
[2025-07-18T15:24:12.849+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committed in 474 ms
[2025-07-18T15:24:12.849+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:24:12.850+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:24:12.860+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=6846507847969457420, sequenceNumber=5, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.49023525S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=42}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3015}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=16440}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:24:12.860+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO SparkWrite: Committed in 495 ms
[2025-07-18T15:24:12.861+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:24:12.864+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/4 using temp file file:/tmp/checkpoints/reservations/commits/.4.37442c17-9a57-4db7-8005-03578c85ad09.tmp
[2025-07-18T15:24:12.871+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/3 using temp file file:/tmp/checkpoints/checkins/commits/.3.ad3b0746-cca0-43de-982d-b957b9dec993.tmp
[2025-07-18T15:24:12.872+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/4 using temp file file:/tmp/checkpoints/feedback/commits/.4.79356618-691c-4a00-85c0-d2a68e238788.tmp
[2025-07-18T15:24:12.907+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.4.37442c17-9a57-4db7-8005-03578c85ad09.tmp to file:/tmp/checkpoints/reservations/commits/4
[2025-07-18T15:24:12.913+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.3.ad3b0746-cca0-43de-982d-b957b9dec993.tmp to file:/tmp/checkpoints/checkins/commits/3
[2025-07-18T15:24:12.914+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.4.79356618-691c-4a00-85c0-d2a68e238788.tmp to file:/tmp/checkpoints/feedback/commits/4
[2025-07-18T15:24:12.930+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:24:12.931+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:24:12.932+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:24:12.932+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:24:12.933+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:24:09.583Z",
[2025-07-18T15:24:12.933+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T15:24:12.933+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:24:12.933+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:24:12.933+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6018657839301835,
[2025-07-18T15:24:12.934+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:24:12.934+0000] {subprocess.py:93} INFO -     "addBatch" : 2864,
[2025-07-18T15:24:12.935+0000] {subprocess.py:93} INFO -     "commitOffsets" : 61,
[2025-07-18T15:24:12.936+0000] {subprocess.py:93} INFO -     "getBatch" : 11,
[2025-07-18T15:24:12.937+0000] {subprocess.py:93} INFO -     "queryPlanning" : 333,
[2025-07-18T15:24:12.937+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3322
[2025-07-18T15:24:12.937+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:24:12.937+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:24:12.937+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:24:12.938+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:24:12.938+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:24:12.939+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:24:12.939+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-07-18T15:24:12.939+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:12.939+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:12.939+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:24:12.939+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -     "latestOffset" : null,
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6018657839301835
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:24:12.940+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:24:12.941+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:24:12.941+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:24:12.941+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:24:12.941+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:24:12.942+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:24:12.942+0000] {subprocess.py:93} INFO -   "runId" : "16c6e454-9d9c-4acb-9866-9460928e2151",
[2025-07-18T15:24:12.942+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:24:12.942+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:24:09.782Z",
[2025-07-18T15:24:12.942+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T15:24:12.942+0000] {subprocess.py:93} INFO -   "numInputRows" : 3,
[2025-07-18T15:24:12.942+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.9581603321622485,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -     "addBatch" : 2692,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -     "commitOffsets" : 57,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -     "latestOffset" : 297,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -     "queryPlanning" : 23,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3131,
[2025-07-18T15:24:12.943+0000] {subprocess.py:93} INFO -     "walCommit" : 49
[2025-07-18T15:24:12.944+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:24:12.944+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:24:12.944+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:24:12.944+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:24:12.945+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:24:12.945+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:24:12.945+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:24:12.946+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:12.946+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:12.946+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:24:12.946+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:24:12.946+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:24:12.947+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:12.947+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:12.948+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:24:12.948+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:24:12.949+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:24:12.949+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:12.950+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:12.950+0000] {subprocess.py:93} INFO -     "numInputRows" : 3,
[2025-07-18T15:24:12.950+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:24:12.951+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.9581603321622485,
[2025-07-18T15:24:12.955+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:24:12.955+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:24:12.955+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:24:12.956+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:24:12.956+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:24:12.956+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:24:12.956+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:24:12.956+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:24:12.956+0000] {subprocess.py:93} INFO -     "numOutputRows" : 3
[2025-07-18T15:24:12.957+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:24:12.957+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:24:12.957+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:24:12.957+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:24:12.957+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:24:12.957+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:24:12.958+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:24:12.958+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:24:12.958+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:24:12.958+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:24:12.959+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:24:12.959+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:24:12.959+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:24:12.959+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:24:12.960+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:24:12.960+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:24:12.960+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:24:12.960+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:24:12.960+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:24:12.960+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:24:12.960+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:24:12.961+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:24:12.962+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:24:12.963+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:24:12.964+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:24:12.965+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:24:12.965+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:24:12.965+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:24:12.965+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:24:12.965+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:24:12.965+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:24:12.965+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:24:12.966+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:24:09.665Z",
[2025-07-18T15:24:12.966+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T15:24:12.966+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:24:12.967+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:24:12.968+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.3079765937788728,
[2025-07-18T15:24:12.968+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:24:12.968+0000] {subprocess.py:93} INFO -     "addBatch" : 2867,
[2025-07-18T15:24:12.968+0000] {subprocess.py:93} INFO -     "commitOffsets" : 64,
[2025-07-18T15:24:12.968+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:24:12.969+0000] {subprocess.py:93} INFO -     "queryPlanning" : 308,
[2025-07-18T15:24:12.969+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3247
[2025-07-18T15:24:12.969+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:24:12.969+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:24:12.969+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:24:12.969+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:24:12.971+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:24:12.971+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -         "0" : 39
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -     "latestOffset" : null,
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:24:12.972+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.3079765937788728
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:24:12.973+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:24:12.974+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:24:12.974+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:24:12.974+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:24:12.974+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:24:12.974+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:24:12.975+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:24:12.976+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:24:12.976+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:24:12.976+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:24:12.976+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:24:12.976+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:24:12.977+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:24:12.977+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:24:12.977+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:24:12.977+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:24:12.978+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:24:12.978+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:24:12.979+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:24:12.979+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:24:12.983+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:24:12.984+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:24:12.984+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:24:12.985+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:24:12.985+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:24:12.985+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:24:12.985+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:24:12.985+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:24:12.986+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:24:12.986+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:24:12.987+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:24:12.987+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:24:12.987+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:24:12.988+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:24:12.988+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:24:12.988+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:24:12.989+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:24:12.989+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:24:12.989+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:24:12.989+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:24:12.990+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:24:12.990+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:24:12.991+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:24:12.992+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:24:12.993+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:24:12.994+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:24:12.994+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:24:12.996+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:24:12.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:24:12.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:24:12.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:24:12.997+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:24:12.997+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:24:12.997+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:24:12.998+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:24:12.998+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:24:12.998+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:24:12.999+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:24:12.999+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:24:13.000+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:24:13.000+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:24:13.000+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:24:13.000+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:24:13.004+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:24:13.004+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AppInfoParser: Kafka startTimeMs: 1752852252933
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO AppInfoParser: Kafka startTimeMs: 1752852252935
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/4 using temp file file:/tmp/checkpoints/checkins/offsets/.4.653c36a1-470a-4533-a37f-7fb541556ded.tmp
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.4.653c36a1-470a-4533-a37f-7fb541556ded.tmp to file:/tmp/checkpoints/checkins/offsets/4
[2025-07-18T15:24:13.005+0000] {subprocess.py:93} INFO - 25/07/18 15:24:12 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752852252952,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:24:13.017+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.018+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.018+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.018+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:13.023+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:13.045+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.046+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.047+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.052+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:13.053+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:13.069+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.070+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.070+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.081+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:13.082+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:24:13.102+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:24:13.115+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:24:13.115+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:24:13.116+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkContext: Created broadcast 6 from start at <unknown>:0
[2025-07-18T15:24:13.119+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:24:13.121+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:24:13.122+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:24:13.122+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Final stage: ResultStage 3 (start at <unknown>:0)
[2025-07-18T15:24:13.138+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:24:13.145+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:24:13.146+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:24:13.147+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T15:24:13.151+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.0 MiB)
[2025-07-18T15:24:13.152+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:24:13.152+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:24:13.152+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:24:13.153+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-07-18T15:24:13.153+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:24:13.153+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-07-18T15:24:13.179+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:24:13.180+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=40 untilOffset=42, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=4 taskId=3 partitionId=0
[2025-07-18T15:24:13.201+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T15:24:13.209+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:24:13.213+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 77cb57a6bd53:38337 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:24:13.215+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 77cb57a6bd53:38337 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:24:13.218+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:24:13.223+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T15:24:13.223+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 28016166 nanos.
[2025-07-18T15:24:13.223+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 77cb57a6bd53:38337 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:24:13.227+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 77cb57a6bd53:38337 in memory (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:24:13.228+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4912 bytes result sent to driver
[2025-07-18T15:24:13.231+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 83 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:24:13.231+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-07-18T15:24:13.237+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: ResultStage 3 (start at <unknown>:0) finished in 0.115 s
[2025-07-18T15:24:13.237+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:24:13.238+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-07-18T15:24:13.238+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.118558 s
[2025-07-18T15:24:13.239+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:24:13.239+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Committing epoch 4 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:24:13.263+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:24:13.353+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v6.metadata.json
[2025-07-18T15:24:13.379+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SnapshotProducer: Committed snapshot 7348189938253068144 (FastAppend)
[2025-07-18T15:24:13.401+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=7348189938253068144, sequenceNumber=5, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.140005042S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=5}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=42}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2983}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=16272}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:24:13.401+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO SparkWrite: Committed in 140 ms
[2025-07-18T15:24:13.402+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:24:13.405+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/4 using temp file file:/tmp/checkpoints/checkins/commits/.4.a22af5ea-4945-4fa7-ab62-c5ef282b5bb7.tmp
[2025-07-18T15:24:13.418+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.4.a22af5ea-4945-4fa7-ab62-c5ef282b5bb7.tmp to file:/tmp/checkpoints/checkins/commits/4
[2025-07-18T15:24:13.418+0000] {subprocess.py:93} INFO - 25/07/18 15:24:13 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:24:12.932Z",
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.6121824303642486,
[2025-07-18T15:24:13.419+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.123711340206186,
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -     "addBatch" : 368,
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -     "commitOffsets" : 18,
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -     "latestOffset" : 20,
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -     "queryPlanning" : 41,
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -     "triggerExecution" : 485,
[2025-07-18T15:24:13.420+0000] {subprocess.py:93} INFO -     "walCommit" : 34
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -         "0" : 40
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:24:13.421+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.6121824303642486,
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.123711340206186,
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:24:13.422+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:24:13.423+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:24:13.423+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:24:13.423+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:24:13.423+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:24:13.423+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:24:13.423+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:24:16.071+0000] {subprocess.py:93} INFO - 25/07/18 15:24:16 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.4 MiB)
[2025-07-18T15:24:16.073+0000] {subprocess.py:93} INFO - 25/07/18 15:24:16 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:24:22.923+0000] {subprocess.py:93} INFO - 25/07/18 15:24:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:22.930+0000] {subprocess.py:93} INFO - 25/07/18 15:24:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:23.418+0000] {subprocess.py:93} INFO - 25/07/18 15:24:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:32.927+0000] {subprocess.py:93} INFO - 25/07/18 15:24:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:32.944+0000] {subprocess.py:93} INFO - 25/07/18 15:24:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:33.424+0000] {subprocess.py:93} INFO - 25/07/18 15:24:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:42.938+0000] {subprocess.py:93} INFO - 25/07/18 15:24:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:42.949+0000] {subprocess.py:93} INFO - 25/07/18 15:24:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:43.437+0000] {subprocess.py:93} INFO - 25/07/18 15:24:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:52.944+0000] {subprocess.py:93} INFO - 25/07/18 15:24:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:52.957+0000] {subprocess.py:93} INFO - 25/07/18 15:24:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:24:53.446+0000] {subprocess.py:93} INFO - 25/07/18 15:24:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:02.953+0000] {subprocess.py:93} INFO - 25/07/18 15:25:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:02.964+0000] {subprocess.py:93} INFO - 25/07/18 15:25:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:25:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:12.963+0000] {subprocess.py:93} INFO - 25/07/18 15:25:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:12.976+0000] {subprocess.py:93} INFO - 25/07/18 15:25:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:13.456+0000] {subprocess.py:93} INFO - 25/07/18 15:25:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:22.964+0000] {subprocess.py:93} INFO - 25/07/18 15:25:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:22.990+0000] {subprocess.py:93} INFO - 25/07/18 15:25:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:23.457+0000] {subprocess.py:93} INFO - 25/07/18 15:25:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:32.969+0000] {subprocess.py:93} INFO - 25/07/18 15:25:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:32.994+0000] {subprocess.py:93} INFO - 25/07/18 15:25:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:33.468+0000] {subprocess.py:93} INFO - 25/07/18 15:25:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:42.980+0000] {subprocess.py:93} INFO - 25/07/18 15:25:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:42.999+0000] {subprocess.py:93} INFO - 25/07/18 15:25:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:43.470+0000] {subprocess.py:93} INFO - 25/07/18 15:25:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:52.992+0000] {subprocess.py:93} INFO - 25/07/18 15:25:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:53.005+0000] {subprocess.py:93} INFO - 25/07/18 15:25:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:25:53.480+0000] {subprocess.py:93} INFO - 25/07/18 15:25:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:02.957+0000] {subprocess.py:93} INFO - 25/07/18 15:26:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/5 using temp file file:/tmp/checkpoints/reservations/offsets/.5.3f1157c6-7589-41e1-b6c3-3fce80ff0fda.tmp
[2025-07-18T15:26:02.979+0000] {subprocess.py:93} INFO - 25/07/18 15:26:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.5.3f1157c6-7589-41e1-b6c3-3fce80ff0fda.tmp to file:/tmp/checkpoints/reservations/offsets/5
[2025-07-18T15:26:02.980+0000] {subprocess.py:93} INFO - 25/07/18 15:26:02 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752852362947,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:26:02.996+0000] {subprocess.py:93} INFO - 25/07/18 15:26:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:02.997+0000] {subprocess.py:93} INFO - 25/07/18 15:26:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:02.998+0000] {subprocess.py:93} INFO - 25/07/18 15:26:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.000+0000] {subprocess.py:93} INFO - 25/07/18 15:26:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.002+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.009+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.009+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.009+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.015+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:03.015+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.029+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.029+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.029+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.032+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T15:26:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.3 MiB)
[2025-07-18T15:26:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T15:26:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Created broadcast 8 from start at <unknown>:0
[2025-07-18T15:26:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:26:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:26:03.057+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:26:03.059+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
[2025-07-18T15:26:03.061+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:26:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:26:03.063+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:26:03.064+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:26:03.065+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T15:26:03.067+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:26:03.068+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:26:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:26:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-18T15:26:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:26:03.070+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-07-18T15:26:03.092+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:26:03.099+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=42 untilOffset=43, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=5 taskId=4 partitionId=0
[2025-07-18T15:26:03.107+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 42 for partition reservations-0
[2025-07-18T15:26:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:26:03.169+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.170+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:26:03.170+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=44, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.172+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T15:26:03.193+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T15:26:03.194+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 64220375 nanos, during time span of 88675876 nanos.
[2025-07-18T15:26:03.199+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4766 bytes result sent to driver
[2025-07-18T15:26:03.199+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 131 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:26:03.200+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-18T15:26:03.200+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.140 s
[2025-07-18T15:26:03.201+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:26:03.201+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-18T15:26:03.201+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.146700 s
[2025-07-18T15:26:03.201+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:26:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Committing epoch 5 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:26:03.232+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.318+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v7.metadata.json
[2025-07-18T15:26:03.345+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SnapshotProducer: Committed snapshot 4484119506518225334 (FastAppend)
[2025-07-18T15:26:03.367+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=4484119506518225334, sequenceNumber=6, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.134411125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=6}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=43}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2898}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=18828}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:26:03.367+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Committed in 135 ms
[2025-07-18T15:26:03.367+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:26:03.371+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/5 using temp file file:/tmp/checkpoints/reservations/commits/.5.cef41442-a098-4e65-be2a-8520d14480af.tmp
[2025-07-18T15:26:03.386+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.5.cef41442-a098-4e65-be2a-8520d14480af.tmp to file:/tmp/checkpoints/reservations/commits/5
[2025-07-18T15:26:03.387+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:26:03.387+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:26:03.387+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:26:03.388+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:26:03.388+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:26:02.946Z",
[2025-07-18T15:26:03.388+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T15:26:03.388+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:26:03.388+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:26:03.388+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.277904328018223,
[2025-07-18T15:26:03.388+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -     "addBatch" : 362,
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -     "commitOffsets" : 20,
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -     "queryPlanning" : 23,
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -     "triggerExecution" : 439,
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -     "walCommit" : 31
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:26:03.389+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:26:03.390+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:26:03.391+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.277904328018223,
[2025-07-18T15:26:03.392+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:26:03.392+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:26:03.392+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:26:03.392+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:26:03.392+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:26:03.392+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:26:03.393+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:26:03.393+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:26:03.393+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:26:03.393+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:26:03.393+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:26:03.393+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/6 using temp file file:/tmp/checkpoints/reservations/offsets/.6.91f87005-6175-4bb1-94cd-83ece41c0c1b.tmp
[2025-07-18T15:26:03.405+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.6.91f87005-6175-4bb1-94cd-83ece41c0c1b.tmp to file:/tmp/checkpoints/reservations/offsets/6
[2025-07-18T15:26:03.405+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752852363387,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:26:03.413+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.413+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.414+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.414+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.415+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.423+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.424+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.429+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.429+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.429+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:03.431+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.432+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.441+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:26:03.451+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:26:03.452+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:03.452+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Created broadcast 10 from start at <unknown>:0
[2025-07-18T15:26:03.453+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:26:03.453+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:26:03.453+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 77cb57a6bd53:38337 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T15:26:03.453+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:26:03.453+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Final stage: ResultStage 5 (start at <unknown>:0)
[2025-07-18T15:26:03.453+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:26:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:26:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:26:03.456+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:26:03.456+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T15:26:03.456+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:26:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:26:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-07-18T15:26:03.458+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 77cb57a6bd53:38337 in memory (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:26:03.460+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:26:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-07-18T15:26:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:26:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=43 untilOffset=45, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=6 taskId=5 partitionId=0
[2025-07-18T15:26:03.470+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 43 for partition reservations-0
[2025-07-18T15:26:03.471+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:26:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:26:03.480+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 44 for partition reservations-0
[2025-07-18T15:26:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:26:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/5 using temp file file:/tmp/checkpoints/checkins/offsets/.5.af5ff5f4-6a63-4a37-b5b4-bff821609ac0.tmp
[2025-07-18T15:26:03.582+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.5.af5ff5f4-6a63-4a37-b5b4-bff821609ac0.tmp to file:/tmp/checkpoints/checkins/offsets/5
[2025-07-18T15:26:03.582+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752852363563,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:26:03.589+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.589+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.589+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.591+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.592+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.599+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.600+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.600+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.608+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.611+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.611+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.619+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:26:03.625+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:26:03.626+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:26:03.627+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Created broadcast 12 from start at <unknown>:0
[2025-07-18T15:26:03.628+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:26:03.629+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:26:03.630+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:26:03.631+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
[2025-07-18T15:26:03.631+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:26:03.632+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:26:03.633+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:26:03.634+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T15:26:03.635+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:26:03.636+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:26:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:26:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:26:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-18T15:26:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:26:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-07-18T15:26:03.653+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:26:03.653+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=42 untilOffset=43, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=5 taskId=6 partitionId=0
[2025-07-18T15:26:03.657+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 42 for partition checkins-0
[2025-07-18T15:26:03.660+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:26:03.755+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.756+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:26:03.757+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=44, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.758+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T15:26:03.779+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T15:26:03.779+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 100467916 nanos, during time span of 122791667 nanos.
[2025-07-18T15:26:03.785+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4892 bytes result sent to driver
[2025-07-18T15:26:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 152 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:26:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-18T15:26:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.159 s
[2025-07-18T15:26:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:26:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-07-18T15:26:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.160364 s
[2025-07-18T15:26:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:26:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Committing epoch 5 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:26:03.805+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.879+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v7.metadata.json
[2025-07-18T15:26:03.907+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SnapshotProducer: Committed snapshot 7784401210794093617 (FastAppend)
[2025-07-18T15:26:03.926+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=7784401210794093617, sequenceNumber=6, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.121095875S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=6}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=43}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2861}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=19133}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:26:03.926+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Committed in 122 ms
[2025-07-18T15:26:03.926+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:26:03.933+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/5 using temp file file:/tmp/checkpoints/checkins/commits/.5.6b85e052-d00c-42fe-bb9c-0a5e290bc108.tmp
[2025-07-18T15:26:03.948+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.5.6b85e052-d00c-42fe-bb9c-0a5e290bc108.tmp to file:/tmp/checkpoints/checkins/commits/5
[2025-07-18T15:26:03.949+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:26:03.949+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:26:03.949+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:26:03.562Z",
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.5974025974025974,
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -     "addBatch" : 332,
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:26:03.950+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -     "triggerExecution" : 385,
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -     "walCommit" : 18
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:26:03.951+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:26:03.952+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:03.952+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:26:03.952+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:03.952+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:03.952+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:26:03.952+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:03.952+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:03.953+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:03.953+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:03.953+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:26:03.953+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.5974025974025974,
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:26:03.954+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:26:03.955+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:26:03.955+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:26:03.955+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:26:03.955+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:26:03.955+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:26:03.955+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:26:03.956+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:26:03.956+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:26:03.956+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:26:03.956+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/6 using temp file file:/tmp/checkpoints/checkins/offsets/.6.93d7974f-605e-43ed-83a2-6602696ed65c.tmp
[2025-07-18T15:26:03.967+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.6.93d7974f-605e-43ed-83a2-6602696ed65c.tmp to file:/tmp/checkpoints/checkins/offsets/6
[2025-07-18T15:26:03.967+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752852363950,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:26:03.975+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.975+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.975+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.977+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.978+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.983+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.984+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.984+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.985+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.986+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:26:03.986+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.987+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:03.987+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:03.987+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T15:26:03.993+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.994+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.994+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:03.995+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.000+0000] {subprocess.py:93} INFO - 25/07/18 15:26:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.004+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.020+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T15:26:04.020+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 514671375 nanos, during time span of 545213917 nanos.
[2025-07-18T15:26:04.021+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:26:04.021+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.022+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 77cb57a6bd53:38337 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.022+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 14 from start at <unknown>:0
[2025-07-18T15:26:04.022+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:26:04.022+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:26:04.022+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:26:04.023+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
[2025-07-18T15:26:04.023+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:26:04.025+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:26:04.026+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:26:04.028+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4770 bytes result sent to driver
[2025-07-18T15:26:04.029+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.029+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.030+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.030+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.035+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:26:04.035+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:26:04.035+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-18T15:26:04.035+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:26:04.043+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 583 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:26:04.043+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-07-18T15:26:04.044+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: ResultStage 5 (start at <unknown>:0) finished in 0.591 s
[2025-07-18T15:26:04.044+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:26:04.044+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-07-18T15:26:04.045+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-07-18T15:26:04.045+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.594107 s
[2025-07-18T15:26:04.045+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:26:04.045+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committing epoch 6 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:26:04.056+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:26:04.057+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=43 untilOffset=44, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=6 taskId=7 partitionId=0
[2025-07-18T15:26:04.059+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 43 for partition checkins-0
[2025-07-18T15:26:04.061+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:26:04.062+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.064+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:26:04.065+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.065+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T15:26:04.067+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:26:04.091+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T15:26:04.091+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 3815125 nanos, during time span of 32067958 nanos.
[2025-07-18T15:26:04.102+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4899 bytes result sent to driver
[2025-07-18T15:26:04.104+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 77cb57a6bd53:38337 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.107+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 72 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:26:04.108+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-18T15:26:04.109+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.088 s
[2025-07-18T15:26:04.110+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:26:04.111+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-18T15:26:04.112+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 0.090666 s
[2025-07-18T15:26:04.112+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:26:04.113+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committing epoch 6 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:26:04.160+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.184+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/5 using temp file file:/tmp/checkpoints/feedback/offsets/.5.05b52b81-a251-49e5-8199-209fe9fed46c.tmp
[2025-07-18T15:26:04.200+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v8.metadata.json
[2025-07-18T15:26:04.201+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.5.05b52b81-a251-49e5-8199-209fe9fed46c.tmp to file:/tmp/checkpoints/feedback/offsets/5
[2025-07-18T15:26:04.201+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752852364173,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:26:04.208+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.210+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.211+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.211+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.214+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.237+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.238+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.238+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.238+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.239+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.250+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SnapshotProducer: Committed snapshot 5022409008442597374 (FastAppend)
[2025-07-18T15:26:04.254+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v8.metadata.json
[2025-07-18T15:26:04.255+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.256+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.256+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.261+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.263+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.276+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.282+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.284+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.285+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 16 from start at <unknown>:0
[2025-07-18T15:26:04.292+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:26:04.293+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.294+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:26:04.294+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:26:04.295+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
[2025-07-18T15:26:04.295+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:26:04.296+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:26:04.297+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:26:04.298+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.301+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.302+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 77cb57a6bd53:38337 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.303+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:26:04.304+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:26:04.305+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-18T15:26:04.305+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:26:04.305+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-07-18T15:26:04.316+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5022409008442597374, sequenceNumber=7, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.247505792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=7}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=45}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2986}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=21814}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:26:04.317+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committed in 248 ms
[2025-07-18T15:26:04.318+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:26:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:26:04.325+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=42 untilOffset=43, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=5 taskId=8 partitionId=0
[2025-07-18T15:26:04.328+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/6 using temp file file:/tmp/checkpoints/reservations/commits/.6.7271eace-4122-4482-872c-647ac9286bda.tmp
[2025-07-18T15:26:04.329+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 42 for partition feedback-0
[2025-07-18T15:26:04.332+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SnapshotProducer: Committed snapshot 2103404622943952031 (FastAppend)
[2025-07-18T15:26:04.333+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:26:04.352+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.6.7271eace-4122-4482-872c-647ac9286bda.tmp to file:/tmp/checkpoints/reservations/commits/6
[2025-07-18T15:26:04.354+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:26:04.354+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:26:04.355+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:26:04.355+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:26:04.355+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:26:03.386Z",
[2025-07-18T15:26:04.356+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T15:26:04.356+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:26:04.356+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.545454545454546,
[2025-07-18T15:26:04.356+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.072538860103627,
[2025-07-18T15:26:04.356+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:26:04.356+0000] {subprocess.py:93} INFO -     "addBatch" : 897,
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -     "commitOffsets" : 38,
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -     "queryPlanning" : 11,
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -     "triggerExecution" : 965,
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -     "walCommit" : 18
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:26:04.357+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:26:04.358+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.359+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:26:04.360+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.545454545454546,
[2025-07-18T15:26:04.360+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.072538860103627,
[2025-07-18T15:26:04.360+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:26:04.360+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:26:04.360+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:26:04.361+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2103404622943952031, sequenceNumber=7, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.200095375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=7}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=44}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2876}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=22009}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:26:04.362+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committed in 200 ms
[2025-07-18T15:26:04.362+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:26:04.366+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/6 using temp file file:/tmp/checkpoints/checkins/commits/.6.5389b371-8f9e-4f70-b6ad-c98abb679479.tmp
[2025-07-18T15:26:04.366+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.367+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:26:04.368+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=44, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.371+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T15:26:04.386+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.6.5389b371-8f9e-4f70-b6ad-c98abb679479.tmp to file:/tmp/checkpoints/checkins/commits/6
[2025-07-18T15:26:04.386+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:26:04.386+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:26:04.386+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:26:04.387+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:26:04.387+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:26:03.948Z",
[2025-07-18T15:26:04.387+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T15:26:04.388+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:26:04.388+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.5906735751295336,
[2025-07-18T15:26:04.389+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.3094688221709005,
[2025-07-18T15:26:04.389+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:26:04.389+0000] {subprocess.py:93} INFO -     "addBatch" : 381,
[2025-07-18T15:26:04.389+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:26:04.390+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:26:04.391+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:26:04.392+0000] {subprocess.py:93} INFO -     "queryPlanning" : 11,
[2025-07-18T15:26:04.393+0000] {subprocess.py:93} INFO -     "triggerExecution" : 433,
[2025-07-18T15:26:04.393+0000] {subprocess.py:93} INFO -     "walCommit" : 16
[2025-07-18T15:26:04.394+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:26:04.394+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:26:04.394+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:26:04.395+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:26:04.395+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:26:04.396+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:04.396+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:04.396+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.397+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.397+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:26:04.397+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:04.398+0000] {subprocess.py:93} INFO -         "0" : 44
[2025-07-18T15:26:04.398+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.399+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.399+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:26:04.399+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:04.399+0000] {subprocess.py:93} INFO -         "0" : 44
[2025-07-18T15:26:04.400+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.400+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.400+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:26:04.401+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.5906735751295336,
[2025-07-18T15:26:04.401+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.3094688221709005,
[2025-07-18T15:26:04.401+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:26:04.402+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:26:04.402+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:26:04.402+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:26:04.403+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:26:04.403+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:26:04.404+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:26:04.404+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:26:04.404+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:26:04.405+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:26:04.405+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:26:04.405+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T15:26:04.405+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 38674500 nanos, during time span of 57265792 nanos.
[2025-07-18T15:26:04.406+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4848 bytes result sent to driver
[2025-07-18T15:26:04.406+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.406+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 88 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:26:04.407+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-18T15:26:04.407+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.101 s
[2025-07-18T15:26:04.407+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:26:04.407+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-18T15:26:04.407+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.103772 s
[2025-07-18T15:26:04.408+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:26:04.408+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committing epoch 5 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:26:04.409+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:26:04.409+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/7 using temp file file:/tmp/checkpoints/checkins/offsets/.7.4da74e53-26a9-43df-b0eb-18210fdd236c.tmp
[2025-07-18T15:26:04.411+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.420+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.7.4da74e53-26a9-43df-b0eb-18210fdd236c.tmp to file:/tmp/checkpoints/checkins/offsets/7
[2025-07-18T15:26:04.420+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1752852364384,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:26:04.430+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.430+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.430+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.433+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.433+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.438+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.438+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.439+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.441+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.442+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.445+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.445+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.446+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:04.446+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.447+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.455+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:26:04.460+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.461+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.461+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 18 from start at <unknown>:0
[2025-07-18T15:26:04.462+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:26:04.462+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:26:04.462+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 77cb57a6bd53:38337 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.463+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:26:04.463+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Final stage: ResultStage 9 (start at <unknown>:0)
[2025-07-18T15:26:04.464+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:26:04.465+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:26:04.465+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:26:04.465+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 28.0 KiB, free 434.3 MiB)
[2025-07-18T15:26:04.471+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.472+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.472+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:26:04.473+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:26:04.473+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-18T15:26:04.474+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:26:04.476+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2025-07-18T15:26:04.480+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:26:04.484+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=44 untilOffset=45, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=7 taskId=9 partitionId=0
[2025-07-18T15:26:04.488+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 44 for partition checkins-0
[2025-07-18T15:26:04.489+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:26:04.508+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v7.metadata.json
[2025-07-18T15:26:04.534+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SnapshotProducer: Committed snapshot 221624744003115401 (FastAppend)
[2025-07-18T15:26:04.554+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=221624744003115401, sequenceNumber=6, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.144078042S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=6}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=43}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2720}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=19160}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:26:04.555+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Committed in 144 ms
[2025-07-18T15:26:04.556+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:26:04.560+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/5 using temp file file:/tmp/checkpoints/feedback/commits/.5.d994f495-9d0d-44e0-aa15-f2a083111545.tmp
[2025-07-18T15:26:04.576+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.5.d994f495-9d0d-44e0-aa15-f2a083111545.tmp to file:/tmp/checkpoints/feedback/commits/5
[2025-07-18T15:26:04.576+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:26:04.577+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:26:04.577+0000] {subprocess.py:93} INFO -   "runId" : "16c6e454-9d9c-4acb-9866-9460928e2151",
[2025-07-18T15:26:04.577+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:26:04.577+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:26:04.169Z",
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 37.03703703703704,
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.4691358024691357,
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -     "addBatch" : 327,
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -     "commitOffsets" : 20,
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:26:04.578+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:26:04.579+0000] {subprocess.py:93} INFO -     "queryPlanning" : 26,
[2025-07-18T15:26:04.579+0000] {subprocess.py:93} INFO -     "triggerExecution" : 405,
[2025-07-18T15:26:04.579+0000] {subprocess.py:93} INFO -     "walCommit" : 27
[2025-07-18T15:26:04.579+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:26:04.579+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:26:04.579+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:26:04.580+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:26:04.580+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:26:04.580+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:26:04.580+0000] {subprocess.py:93} INFO -         "0" : 42
[2025-07-18T15:26:04.580+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.581+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.581+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:26:04.581+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:26:04.581+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:04.581+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.581+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.582+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:26:04.582+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:26:04.582+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:04.582+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:04.582+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:04.583+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:26:04.583+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 37.03703703703704,
[2025-07-18T15:26:04.583+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.4691358024691357,
[2025-07-18T15:26:04.584+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:26:04.585+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:26:04.585+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:26:04.585+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:26:04.585+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:26:04.585+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:26:04.585+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:26:04.585+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:26:04.586+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:26:04.586+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:26:04.586+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:26:04.586+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/6 using temp file file:/tmp/checkpoints/feedback/offsets/.6.bb5c4839-dffe-4f1f-8d04-2b3950780042.tmp
[2025-07-18T15:26:04.597+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.6.bb5c4839-dffe-4f1f-8d04-2b3950780042.tmp to file:/tmp/checkpoints/feedback/offsets/6
[2025-07-18T15:26:04.598+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752852364578,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:26:04.601+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.602+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.602+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.603+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.604+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.608+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.609+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.609+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.610+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.610+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.619+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.619+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.619+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:04.621+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.625+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:26:04.631+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.636+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.636+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:26:04.636+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.637+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 20 from start at <unknown>:0
[2025-07-18T15:26:04.637+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:26:04.637+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:26:04.638+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:26:04.638+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
[2025-07-18T15:26:04.638+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:26:04.638+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:26:04.639+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:26:04.639+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.642+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2025-07-18T15:26:04.643+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 77cb57a6bd53:38337 (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:26:04.643+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:26:04.643+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:26:04.643+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-18T15:26:04.644+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:26:04.644+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2025-07-18T15:26:04.649+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:26:04.657+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=43 untilOffset=45, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=6 taskId=10 partitionId=0
[2025-07-18T15:26:04.658+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 43 for partition feedback-0
[2025-07-18T15:26:04.658+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:26:04.660+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.660+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:26:04.661+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.661+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 44 for partition feedback-0
[2025-07-18T15:26:04.662+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:26:04.991+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.992+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:26:04.992+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:04.993+0000] {subprocess.py:93} INFO - 25/07/18 15:26:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T15:26:05.005+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T15:26:05.006+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 504486792 nanos, during time span of 517983042 nanos.
[2025-07-18T15:26:05.006+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4870 bytes result sent to driver
[2025-07-18T15:26:05.007+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 534 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:26:05.007+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-18T15:26:05.008+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DAGScheduler: ResultStage 9 (start at <unknown>:0) finished in 0.545 s
[2025-07-18T15:26:05.008+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:26:05.008+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-18T15:26:05.008+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.546804 s
[2025-07-18T15:26:05.008+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:26:05.009+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SparkWrite: Committing epoch 7 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:26:05.019+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:26:05.087+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v9.metadata.json
[2025-07-18T15:26:05.121+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SnapshotProducer: Committed snapshot 2799304322405000178 (FastAppend)
[2025-07-18T15:26:05.147+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2799304322405000178, sequenceNumber=8, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.1276605S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=8}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=45}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2903}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=24912}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:26:05.149+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SparkWrite: Committed in 127 ms
[2025-07-18T15:26:05.149+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:26:05.151+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/7 using temp file file:/tmp/checkpoints/checkins/commits/.7.54aae09e-f84c-4fca-a0ec-25f8ec7bb98b.tmp
[2025-07-18T15:26:05.162+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.7.54aae09e-f84c-4fca-a0ec-25f8ec7bb98b.tmp to file:/tmp/checkpoints/checkins/commits/7
[2025-07-18T15:26:05.162+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:26:05.162+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:26:05.162+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:26:05.163+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:26:05.163+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:26:04.382Z",
[2025-07-18T15:26:05.163+0000] {subprocess.py:93} INFO -   "batchId" : 7,
[2025-07-18T15:26:05.163+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:26:05.163+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.3041474654377883,
[2025-07-18T15:26:05.163+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.2836970474967908,
[2025-07-18T15:26:05.163+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:26:05.164+0000] {subprocess.py:93} INFO -     "addBatch" : 712,
[2025-07-18T15:26:05.164+0000] {subprocess.py:93} INFO -     "commitOffsets" : 15,
[2025-07-18T15:26:05.164+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -     "queryPlanning" : 14,
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -     "triggerExecution" : 779,
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -     "walCommit" : 35
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:05.165+0000] {subprocess.py:93} INFO -         "0" : 44
[2025-07-18T15:26:05.166+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:05.167+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:05.168+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:26:05.169+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:05.169+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:26:05.170+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:05.170+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:05.170+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:26:05.171+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:26:05.173+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:26:05.174+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:05.174+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:05.174+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:26:05.174+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.3041474654377883,
[2025-07-18T15:26:05.175+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.2836970474967908,
[2025-07-18T15:26:05.175+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:26:05.175+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:26:05.176+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:26:05.176+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:26:05.177+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:26:05.177+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:26:05.177+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:26:05.178+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:26:05.178+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:26:05.178+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:26:05.178+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:26:05.178+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:05.178+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:26:05.179+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=45, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:26:05.179+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T15:26:05.180+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T15:26:05.182+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor read 2 records through 2 polls (polled  out 2 records), taking 505058709 nanos, during time span of 518200292 nanos.
[2025-07-18T15:26:05.183+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 4814 bytes result sent to driver
[2025-07-18T15:26:05.183+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 534 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:26:05.184+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-18T15:26:05.184+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.540 s
[2025-07-18T15:26:05.185+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:26:05.185+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-18T15:26:05.186+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.542177 s
[2025-07-18T15:26:05.186+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:26:05.186+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SparkWrite: Committing epoch 6 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:26:05.189+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:26:05.262+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v8.metadata.json
[2025-07-18T15:26:05.289+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SnapshotProducer: Committed snapshot 7752827702874276040 (FastAppend)
[2025-07-18T15:26:05.309+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7752827702874276040, sequenceNumber=7, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.119503375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=7}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=45}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3004}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=22164}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:26:05.310+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO SparkWrite: Committed in 120 ms
[2025-07-18T15:26:05.310+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:26:05.316+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/6 using temp file file:/tmp/checkpoints/feedback/commits/.6.02b20318-f2a6-46db-b08a-92a4bf8b49b4.tmp
[2025-07-18T15:26:05.326+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.6.02b20318-f2a6-46db-b08a-92a4bf8b49b4.tmp to file:/tmp/checkpoints/feedback/commits/6
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO - 25/07/18 15:26:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO -   "runId" : "16c6e454-9d9c-4acb-9866-9460928e2151",
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:26:04.575Z",
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:26:05.327+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.926108374384236,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.6666666666666665,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -     "addBatch" : 704,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -     "commitOffsets" : 17,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -     "triggerExecution" : 750,
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -     "walCommit" : 18
[2025-07-18T15:26:05.328+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -         "0" : 43
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:05.329+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.926108374384236,
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.6666666666666665,
[2025-07-18T15:26:05.330+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:26:05.331+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:26:14.360+0000] {subprocess.py:93} INFO - 25/07/18 15:26:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:15.169+0000] {subprocess.py:93} INFO - 25/07/18 15:26:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:15.338+0000] {subprocess.py:93} INFO - 25/07/18 15:26:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:23.060+0000] {subprocess.py:93} INFO - 25/07/18 15:26:23 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:26:23.063+0000] {subprocess.py:93} INFO - 25/07/18 15:26:23 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:26:23.066+0000] {subprocess.py:93} INFO - 25/07/18 15:26:23 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.4 MiB)
[2025-07-18T15:26:23.068+0000] {subprocess.py:93} INFO - 25/07/18 15:26:23 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 77cb57a6bd53:38337 in memory (size: 12.1 KiB, free: 434.4 MiB)
[2025-07-18T15:26:24.366+0000] {subprocess.py:93} INFO - 25/07/18 15:26:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:25.170+0000] {subprocess.py:93} INFO - 25/07/18 15:26:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:25.344+0000] {subprocess.py:93} INFO - 25/07/18 15:26:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:34.371+0000] {subprocess.py:93} INFO - 25/07/18 15:26:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:35.171+0000] {subprocess.py:93} INFO - 25/07/18 15:26:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:35.351+0000] {subprocess.py:93} INFO - 25/07/18 15:26:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:44.377+0000] {subprocess.py:93} INFO - 25/07/18 15:26:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:45.183+0000] {subprocess.py:93} INFO - 25/07/18 15:26:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:45.362+0000] {subprocess.py:93} INFO - 25/07/18 15:26:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:54.381+0000] {subprocess.py:93} INFO - 25/07/18 15:26:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:55.195+0000] {subprocess.py:93} INFO - 25/07/18 15:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:26:55.376+0000] {subprocess.py:93} INFO - 25/07/18 15:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:04.387+0000] {subprocess.py:93} INFO - 25/07/18 15:27:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:05.197+0000] {subprocess.py:93} INFO - 25/07/18 15:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:05.378+0000] {subprocess.py:93} INFO - 25/07/18 15:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:14.393+0000] {subprocess.py:93} INFO - 25/07/18 15:27:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:15.201+0000] {subprocess.py:93} INFO - 25/07/18 15:27:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:15.383+0000] {subprocess.py:93} INFO - 25/07/18 15:27:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:24.403+0000] {subprocess.py:93} INFO - 25/07/18 15:27:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:25.207+0000] {subprocess.py:93} INFO - 25/07/18 15:27:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:25.390+0000] {subprocess.py:93} INFO - 25/07/18 15:27:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:34.419+0000] {subprocess.py:93} INFO - 25/07/18 15:27:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:35.210+0000] {subprocess.py:93} INFO - 25/07/18 15:27:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:35.398+0000] {subprocess.py:93} INFO - 25/07/18 15:27:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:44.426+0000] {subprocess.py:93} INFO - 25/07/18 15:27:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:45.214+0000] {subprocess.py:93} INFO - 25/07/18 15:27:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:45.401+0000] {subprocess.py:93} INFO - 25/07/18 15:27:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:54.421+0000] {subprocess.py:93} INFO - 25/07/18 15:27:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:55.218+0000] {subprocess.py:93} INFO - 25/07/18 15:27:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:27:55.408+0000] {subprocess.py:93} INFO - 25/07/18 15:27:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:02.047+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/7 using temp file file:/tmp/checkpoints/reservations/offsets/.7.aa0a6693-7860-4933-8078-29e97d6823ce.tmp
[2025-07-18T15:28:02.099+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.7.aa0a6693-7860-4933-8078-29e97d6823ce.tmp to file:/tmp/checkpoints/reservations/offsets/7
[2025-07-18T15:28:02.101+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1752852482030,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:28:02.128+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.128+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.129+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.132+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.136+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.145+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.145+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.146+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.147+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.150+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.174+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.175+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.176+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.178+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.179+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.196+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T15:28:02.202+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T15:28:02.203+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:28:02.204+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Created broadcast 22 from start at <unknown>:0
[2025-07-18T15:28:02.205+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:28:02.206+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:28:02.206+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:28:02.207+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
[2025-07-18T15:28:02.208+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:28:02.209+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:28:02.210+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:28:02.210+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:28:02.227+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T15:28:02.227+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:28:02.228+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:28:02.228+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:28:02.229+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-18T15:28:02.231+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:28:02.234+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2025-07-18T15:28:02.247+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:28:02.248+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=45 untilOffset=46, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=7 taskId=11 partitionId=0
[2025-07-18T15:28:02.254+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 45 for partition reservations-0
[2025-07-18T15:28:02.258+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:28:02.426+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:02.427+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:28:02.427+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:02.454+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T15:28:02.502+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T15:28:02.502+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 1 records through 1 polls (polled  out 2 records), taking 174027458 nanos, during time span of 246877208 nanos.
[2025-07-18T15:28:02.503+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 4693 bytes result sent to driver
[2025-07-18T15:28:02.505+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 275 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:28:02.505+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-18T15:28:02.507+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.301 s
[2025-07-18T15:28:02.508+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:28:02.509+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-07-18T15:28:02.509+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.304687 s
[2025-07-18T15:28:02.509+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:28:02.509+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Committing epoch 7 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:28:02.520+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.605+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v9.metadata.json
[2025-07-18T15:28:02.640+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SnapshotProducer: Committed snapshot 2943632227217470913 (FastAppend)
[2025-07-18T15:28:02.644+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/8 using temp file file:/tmp/checkpoints/checkins/offsets/.8.6dee27a7-0344-45c5-a03a-7cd1f75102da.tmp
[2025-07-18T15:28:02.672+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=2943632227217470913, sequenceNumber=8, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.14420375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=8}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=46}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2941}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=24755}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:28:02.672+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Committed in 145 ms
[2025-07-18T15:28:02.673+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:28:02.673+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.8.6dee27a7-0344-45c5-a03a-7cd1f75102da.tmp to file:/tmp/checkpoints/checkins/offsets/8
[2025-07-18T15:28:02.673+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1752852482638,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:28:02.673+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.673+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/7 using temp file file:/tmp/checkpoints/reservations/commits/.7.01150258-09cd-4230-9717-c1acf9ef6fe1.tmp
[2025-07-18T15:28:02.673+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.673+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.674+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.676+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.680+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.681+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.681+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.682+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.683+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.690+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.690+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.690+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:02.692+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.695+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.7.01150258-09cd-4230-9717-c1acf9ef6fe1.tmp to file:/tmp/checkpoints/reservations/commits/7
[2025-07-18T15:28:02.696+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.696+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:28:02.696+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:28:02.696+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:28:02.696+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:28:02.696+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:28:02.029Z",
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -   "batchId" : 7,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.506024096385542,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -     "addBatch" : 526,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -     "commitOffsets" : 29,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:28:02.697+0000] {subprocess.py:93} INFO -     "queryPlanning" : 40,
[2025-07-18T15:28:02.698+0000] {subprocess.py:93} INFO -     "triggerExecution" : 664,
[2025-07-18T15:28:02.698+0000] {subprocess.py:93} INFO -     "walCommit" : 65
[2025-07-18T15:28:02.698+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:28:02.698+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:28:02.698+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:28:02.698+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:28:02.699+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:28:02.699+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:28:02.699+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:28:02.699+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:02.699+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:02.699+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:28:02.699+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:28:02.700+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:02.700+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:02.700+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:02.700+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:28:02.701+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:28:02.701+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:02.701+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:02.701+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:02.701+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:28:02.701+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:28:02.702+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.506024096385542,
[2025-07-18T15:28:02.702+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:28:02.702+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:28:02.702+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:28:02.702+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:28:02.702+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:28:02.702+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:28:02.703+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:28:02.703+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:28:02.703+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:28:02.703+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:28:02.703+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:28:02.709+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:28:02.711+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/8 using temp file file:/tmp/checkpoints/reservations/offsets/.8.025710a1-0962-4a71-8f44-5aabc5b4be54.tmp
[2025-07-18T15:28:02.722+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:28:02.723+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:28:02.723+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Created broadcast 24 from start at <unknown>:0
[2025-07-18T15:28:02.726+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:28:02.727+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:28:02.727+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:28:02.727+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Final stage: ResultStage 12 (start at <unknown>:0)
[2025-07-18T15:28:02.727+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:28:02.727+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:28:02.728+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:28:02.729+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T15:28:02.733+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:28:02.733+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:28:02.733+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:28:02.734+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:28:02.737+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-07-18T15:28:02.740+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:28:02.740+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2025-07-18T15:28:02.744+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.8.025710a1-0962-4a71-8f44-5aabc5b4be54.tmp to file:/tmp/checkpoints/reservations/offsets/8
[2025-07-18T15:28:02.744+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1752852482696,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:28:02.744+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:28:02.746+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=45 untilOffset=46, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=8 taskId=12 partitionId=0
[2025-07-18T15:28:02.752+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.752+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.753+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.753+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 45 for partition checkins-0
[2025-07-18T15:28:02.754+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.754+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.755+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:28:02.758+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.759+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.759+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.760+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.765+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.765+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.767+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.767+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:02.768+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.768+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:02.776+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:28:02.779+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:28:02.779+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:02.780+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Created broadcast 26 from start at <unknown>:0
[2025-07-18T15:28:02.780+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:28:02.780+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:28:02.781+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:28:02.781+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
[2025-07-18T15:28:02.781+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:28:02.781+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:28:02.781+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:28:02.782+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:28:02.784+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:28:02.786+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:02.786+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:28:02.786+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:28:02.787+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-07-18T15:28:02.787+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:28:02.787+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2025-07-18T15:28:02.792+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:28:02.793+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=46 untilOffset=48, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=8 taskId=13 partitionId=0
[2025-07-18T15:28:02.798+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 47 for partition reservations-0
[2025-07-18T15:28:02.798+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:28:02.833+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:02.833+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:28:02.834+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=47, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:02.834+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T15:28:02.845+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T15:28:02.846+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 80554709 nanos, during time span of 93857750 nanos.
[2025-07-18T15:28:02.847+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 4804 bytes result sent to driver
[2025-07-18T15:28:02.847+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 114 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:28:02.847+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-07-18T15:28:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: ResultStage 12 (start at <unknown>:0) finished in 0.121 s
[2025-07-18T15:28:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:28:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-07-18T15:28:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.124244 s
[2025-07-18T15:28:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:28:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Committing epoch 8 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:28:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.000+0000] {subprocess.py:93} INFO - 25/07/18 15:28:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v10.metadata.json
[2025-07-18T15:28:03.082+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SnapshotProducer: Committed snapshot 6200362597176660169 (FastAppend)
[2025-07-18T15:28:03.121+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=6200362597176660169, sequenceNumber=9, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.262410458S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=9}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=46}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2820}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=27732}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:28:03.123+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committed in 262 ms
[2025-07-18T15:28:03.124+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:28:03.128+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/8 using temp file file:/tmp/checkpoints/checkins/commits/.8.ae30bc60-69cd-4151-9cb3-4699c75a5bfe.tmp
[2025-07-18T15:28:03.158+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.8.ae30bc60-69cd-4151-9cb3-4699c75a5bfe.tmp to file:/tmp/checkpoints/checkins/commits/8
[2025-07-18T15:28:03.158+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:28:03.158+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:28:02.637Z",
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "batchId" : 8,
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.9267822736030829,
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:28:03.160+0000] {subprocess.py:93} INFO -     "addBatch" : 443,
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "commitOffsets" : 37,
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "queryPlanning" : 10,
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "triggerExecution" : 519,
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "walCommit" : 27
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:28:03.161+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:28:03.162+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:28:03.163+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.163+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.163+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:28:03.163+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:28:03.163+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:03.164+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.164+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.164+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:28:03.164+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:28:03.164+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:03.164+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.165+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.9267822736030829,
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:28:03.166+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:28:03.167+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:28:03.167+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:28:03.167+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:28:03.167+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:28:03.167+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:28:03.171+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/9 using temp file file:/tmp/checkpoints/checkins/offsets/.9.1ae2d5b8-48bf-4f80-b404-aaa858611ce3.tmp
[2025-07-18T15:28:03.224+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.9.1ae2d5b8-48bf-4f80-b404-aaa858611ce3.tmp to file:/tmp/checkpoints/checkins/offsets/9
[2025-07-18T15:28:03.227+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1752852483160,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:28:03.231+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.232+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.232+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.248+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.261+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.261+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.262+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.262+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.263+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/7 using temp file file:/tmp/checkpoints/feedback/offsets/.7.4d10c42f-d0b4-4080-b841-06b231eb4e08.tmp
[2025-07-18T15:28:03.269+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.281+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.282+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.283+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.290+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.293+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.302+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.7.4d10c42f-d0b4-4080-b841-06b231eb4e08.tmp to file:/tmp/checkpoints/feedback/offsets/7
[2025-07-18T15:28:03.303+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1752852483250,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:28:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:28:03.306+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.307+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:28:03.309+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T15:28:03.309+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:28:03.309+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.310+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Created broadcast 28 from start at <unknown>:0
[2025-07-18T15:28:03.315+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:28:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:28:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T15:28:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:28:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Final stage: ResultStage 14 (start at <unknown>:0)
[2025-07-18T15:28:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:28:03.317+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:28:03.320+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:28:03.321+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T15:28:03.323+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.0 MiB)
[2025-07-18T15:28:03.323+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.324+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.325+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.325+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:28:03.326+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.327+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:28:03.328+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.330+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:28:03.332+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2025-07-18T15:28:03.333+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:28:03.333+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.336+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2025-07-18T15:28:03.339+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.339+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.345+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T15:28:03.346+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 2 records through 1 polls (polled  out 1 records), taking 510170001 nanos, during time span of 546706042 nanos.
[2025-07-18T15:28:03.359+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 4785 bytes result sent to driver
[2025-07-18T15:28:03.362+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:28:03.363+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=46 untilOffset=48, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=9 taskId=14 partitionId=0
[2025-07-18T15:28:03.365+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 77cb57a6bd53:38337 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:28:03.366+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.368+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.368+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.369+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 46 for partition checkins-0
[2025-07-18T15:28:03.371+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.372+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.372+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:28:03.372+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:28:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.375+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 47 for partition checkins-0
[2025-07-18T15:28:03.376+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 588 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:28:03.377+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-18T15:28:03.380+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:28:03.381+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.598 s
[2025-07-18T15:28:03.381+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:28:03.382+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-07-18T15:28:03.382+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.600551 s
[2025-07-18T15:28:03.382+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:03.382+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:28:03.382+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committing epoch 8 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:28:03.388+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:28:03.388+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 77cb57a6bd53:38337 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:28:03.394+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:28:03.396+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:03.396+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Created broadcast 30 from start at <unknown>:0
[2025-07-18T15:28:03.397+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:28:03.398+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:28:03.398+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:28:03.399+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:28:03.400+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
[2025-07-18T15:28:03.400+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:28:03.400+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:28:03.401+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:28:03.409+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T15:28:03.411+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T15:28:03.412+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 77cb57a6bd53:38337 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:28:03.412+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:28:03.413+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:28:03.414+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:28:03.415+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-07-18T15:28:03.420+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:28:03.420+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2025-07-18T15:28:03.438+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:28:03.440+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=45 untilOffset=46, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=7 taskId=15 partitionId=0
[2025-07-18T15:28:03.442+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 45 for partition feedback-0
[2025-07-18T15:28:03.447+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:28:03.462+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:28:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=47, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.480+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T15:28:03.507+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T15:28:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 18872625 nanos, during time span of 66074000 nanos.
[2025-07-18T15:28:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 4777 bytes result sent to driver
[2025-07-18T15:28:03.511+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 97 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:28:03.512+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-07-18T15:28:03.513+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.110 s
[2025-07-18T15:28:03.514+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:28:03.515+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2025-07-18T15:28:03.516+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.118596 s
[2025-07-18T15:28:03.517+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:28:03.517+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committing epoch 7 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:28:03.539+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.594+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v10.metadata.json
[2025-07-18T15:28:03.662+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SnapshotProducer: Committed snapshot 4247087901082643962 (FastAppend)
[2025-07-18T15:28:03.674+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v9.metadata.json
[2025-07-18T15:28:03.694+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=4247087901082643962, sequenceNumber=9, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.280185042S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=9}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=48}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2998}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=27753}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:28:03.694+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committed in 281 ms
[2025-07-18T15:28:03.694+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:28:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/8 using temp file file:/tmp/checkpoints/reservations/commits/.8.148acf4e-ba29-4a6b-a719-e5c1239231ef.tmp
[2025-07-18T15:28:03.722+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SnapshotProducer: Committed snapshot 4835696644171782622 (FastAppend)
[2025-07-18T15:28:03.743+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.8.148acf4e-ba29-4a6b-a719-e5c1239231ef.tmp to file:/tmp/checkpoints/reservations/commits/8
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:28:02.694Z",
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO -   "batchId" : 8,
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:28:03.745+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.007518796992481,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.921229586935639,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -     "addBatch" : 936,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -     "commitOffsets" : 44,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1041,
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -     "walCommit" : 46
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:28:03.746+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:28:03.748+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:28:03.749+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:28:03.749+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:28:03.749+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:03.749+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.749+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.749+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:28:03.749+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:28:03.750+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:28:03.750+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.750+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.750+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:28:03.750+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:28:03.750+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:28:03.751+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.754+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.755+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:28:03.757+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.007518796992481,
[2025-07-18T15:28:03.757+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.921229586935639,
[2025-07-18T15:28:03.757+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:28:03.758+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:28:03.758+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:28:03.758+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:28:03.759+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:28:03.759+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:28:03.759+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:28:03.760+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:28:03.760+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:28:03.760+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:28:03.760+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:28:03.760+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=4835696644171782622, sequenceNumber=8, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.220068167S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=8}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=46}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2899}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=25063}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:28:03.760+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committed in 221 ms
[2025-07-18T15:28:03.760+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:28:03.764+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/7 using temp file file:/tmp/checkpoints/feedback/commits/.7.528fda07-ce89-4623-b80f-ca49965c9782.tmp
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.7.528fda07-ce89-4623-b80f-ca49965c9782.tmp to file:/tmp/checkpoints/feedback/commits/7
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO -   "runId" : "16c6e454-9d9c-4acb-9866-9460928e2151",
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:28:03.238Z",
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO -   "batchId" : 7,
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:28:03.790+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:28:03.791+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.8181818181818181,
[2025-07-18T15:28:03.791+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:28:03.791+0000] {subprocess.py:93} INFO -     "addBatch" : 427,
[2025-07-18T15:28:03.791+0000] {subprocess.py:93} INFO -     "commitOffsets" : 34,
[2025-07-18T15:28:03.791+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:28:03.791+0000] {subprocess.py:93} INFO -     "latestOffset" : 11,
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -     "queryPlanning" : 27,
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -     "triggerExecution" : 550,
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -     "walCommit" : 50
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -         "0" : 45
[2025-07-18T15:28:03.792+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.793+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.793+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:28:03.793+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:28:03.793+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:03.793+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.793+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.793+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:28:03.794+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.8181818181818181,
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:28:03.795+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:28:03.796+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:28:03.796+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:28:03.796+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:28:03.796+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:28:03.796+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:28:03.800+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:28:03.801+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:28:03.801+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:28:03.801+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:28:03.802+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/8 using temp file file:/tmp/checkpoints/feedback/offsets/.8.49c7284f-b518-4f78-887a-a21e9c31fc2d.tmp
[2025-07-18T15:28:03.820+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.8.49c7284f-b518-4f78-887a-a21e9c31fc2d.tmp to file:/tmp/checkpoints/feedback/offsets/8
[2025-07-18T15:28:03.821+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1752852483791,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:28:03.825+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.826+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.826+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.830+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.830+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.833+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.833+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.834+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.834+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.835+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.840+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:03.842+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.843+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:28:03.853+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:28:03.854+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:28:03.856+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:28:03.857+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Created broadcast 32 from start at <unknown>:0
[2025-07-18T15:28:03.861+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:28:03.861+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:28:03.862+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:28:03.863+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Final stage: ResultStage 16 (start at <unknown>:0)
[2025-07-18T15:28:03.864+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:28:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:28:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:28:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 27.5 KiB, free 434.0 MiB)
[2025-07-18T15:28:03.867+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.0 MiB)
[2025-07-18T15:28:03.867+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 77cb57a6bd53:38337 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:28:03.867+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:28:03.868+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:28:03.868+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-07-18T15:28:03.868+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:28:03.868+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2025-07-18T15:28:03.871+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:28:03.871+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=46 untilOffset=48, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=8 taskId=16 partitionId=0
[2025-07-18T15:28:03.876+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 46 for partition feedback-0
[2025-07-18T15:28:03.878+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:28:03.878+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.879+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.879+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:28:03.879+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:28:03.879+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.880+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:03.881+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T15:28:03.881+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 47 for partition feedback-0
[2025-07-18T15:28:03.881+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:28:03.893+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T15:28:03.893+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 2 records through 2 polls (polled  out 2 records), taking 505786335 nanos, during time span of 524356834 nanos.
[2025-07-18T15:28:03.894+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 4863 bytes result sent to driver
[2025-07-18T15:28:03.895+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 564 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:28:03.895+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: ResultStage 14 (start at <unknown>:0) finished in 0.578 s
[2025-07-18T15:28:03.895+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:28:03.896+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-07-18T15:28:03.896+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2025-07-18T15:28:03.897+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.584703 s
[2025-07-18T15:28:03.898+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:28:03.898+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committing epoch 9 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:28:03.911+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:28:03.977+0000] {subprocess.py:93} INFO - 25/07/18 15:28:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v11.metadata.json
[2025-07-18T15:28:04.011+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SnapshotProducer: Committed snapshot 4987466180766990401 (FastAppend)
[2025-07-18T15:28:04.027+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4987466180766990401, sequenceNumber=10, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.116088208S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=10}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=48}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2952}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=30684}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:28:04.027+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SparkWrite: Committed in 116 ms
[2025-07-18T15:28:04.028+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:28:04.031+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/9 using temp file file:/tmp/checkpoints/checkins/commits/.9.8ac8b7e7-a9c4-47f7-918a-b905db564a3f.tmp
[2025-07-18T15:28:04.041+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.9.8ac8b7e7-a9c4-47f7-918a-b905db564a3f.tmp to file:/tmp/checkpoints/checkins/commits/9
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:28:03.157Z",
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "batchId" : 9,
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.846153846153846,
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.2650056625141564,
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -     "addBatch" : 778,
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -     "commitOffsets" : 14,
[2025-07-18T15:28:04.042+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     "queryPlanning" : 27,
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     "triggerExecution" : 883,
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     "walCommit" : 60
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:04.043+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.846153846153846,
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.2650056625141564,
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:28:04.044+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:28:04.045+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:28:04.386+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:04.387+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:28:04.388+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=48, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:28:04.389+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T15:28:04.414+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T15:28:04.415+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor read 2 records through 2 polls (polled  out 2 records), taking 508830166 nanos, during time span of 540117166 nanos.
[2025-07-18T15:28:04.416+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 4775 bytes result sent to driver
[2025-07-18T15:28:04.417+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 555 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:28:04.418+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-07-18T15:28:04.418+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO DAGScheduler: ResultStage 16 (start at <unknown>:0) finished in 0.561 s
[2025-07-18T15:28:04.418+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:28:04.419+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-07-18T15:28:04.419+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 0.564406 s
[2025-07-18T15:28:04.421+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:28:04.422+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SparkWrite: Committing epoch 8 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:28:04.434+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:28:04.495+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v10.metadata.json
[2025-07-18T15:28:04.520+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SnapshotProducer: Committed snapshot 7641988615111274305 (FastAppend)
[2025-07-18T15:28:04.551+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7641988615111274305, sequenceNumber=9, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.117254583S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=9}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=48}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2907}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=27970}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:28:04.552+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO SparkWrite: Committed in 118 ms
[2025-07-18T15:28:04.552+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:28:04.562+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/8 using temp file file:/tmp/checkpoints/feedback/commits/.8.84c18578-3e90-447c-9331-522a0dfa6c20.tmp
[2025-07-18T15:28:04.596+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.8.84c18578-3e90-447c-9331-522a0dfa6c20.tmp to file:/tmp/checkpoints/feedback/commits/8
[2025-07-18T15:28:04.597+0000] {subprocess.py:93} INFO - 25/07/18 15:28:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:28:04.597+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:28:04.597+0000] {subprocess.py:93} INFO -   "runId" : "16c6e454-9d9c-4acb-9866-9460928e2151",
[2025-07-18T15:28:04.597+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:28:04.597+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:28:03.789Z",
[2025-07-18T15:28:04.597+0000] {subprocess.py:93} INFO -   "batchId" : 8,
[2025-07-18T15:28:04.597+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.629764065335753,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.4937655860349124,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -     "addBatch" : 719,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -     "commitOffsets" : 42,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -     "queryPlanning" : 10,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -     "triggerExecution" : 802,
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -     "walCommit" : 29
[2025-07-18T15:28:04.598+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -         "0" : 46
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:28:04.599+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.629764065335753,
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.4937655860349124,
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:28:04.600+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:28:04.601+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:28:04.601+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:28:04.601+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:28:04.601+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:28:04.603+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:28:04.604+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:28:04.604+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:28:04.604+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:28:04.604+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:28:13.742+0000] {subprocess.py:93} INFO - 25/07/18 15:28:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:14.044+0000] {subprocess.py:93} INFO - 25/07/18 15:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:14.604+0000] {subprocess.py:93} INFO - 25/07/18 15:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:19.242+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:19.243+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 77cb57a6bd53:38337 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:28:19.246+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:19.247+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:19.249+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 77cb57a6bd53:38337 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:28:19.250+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 77cb57a6bd53:38337 in memory (size: 12.1 KiB, free: 434.4 MiB)
[2025-07-18T15:28:19.253+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.4 MiB)
[2025-07-18T15:28:19.254+0000] {subprocess.py:93} INFO - 25/07/18 15:28:19 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 77cb57a6bd53:38337 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T15:28:23.753+0000] {subprocess.py:93} INFO - 25/07/18 15:28:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:24.044+0000] {subprocess.py:93} INFO - 25/07/18 15:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:24.604+0000] {subprocess.py:93} INFO - 25/07/18 15:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:33.764+0000] {subprocess.py:93} INFO - 25/07/18 15:28:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:34.046+0000] {subprocess.py:93} INFO - 25/07/18 15:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:34.609+0000] {subprocess.py:93} INFO - 25/07/18 15:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:43.770+0000] {subprocess.py:93} INFO - 25/07/18 15:28:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:44.056+0000] {subprocess.py:93} INFO - 25/07/18 15:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:44.612+0000] {subprocess.py:93} INFO - 25/07/18 15:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:53.771+0000] {subprocess.py:93} INFO - 25/07/18 15:28:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:54.059+0000] {subprocess.py:93} INFO - 25/07/18 15:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:28:54.627+0000] {subprocess.py:93} INFO - 25/07/18 15:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:03.794+0000] {subprocess.py:93} INFO - 25/07/18 15:29:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:04.067+0000] {subprocess.py:93} INFO - 25/07/18 15:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:04.639+0000] {subprocess.py:93} INFO - 25/07/18 15:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:10.096+0000] {subprocess.py:93} INFO - 25/07/18 15:29:10 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2025-07-18T15:29:12.969+0000] {subprocess.py:93} INFO - 25/07/18 15:29:12 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
[2025-07-18T15:29:12.970+0000] {subprocess.py:93} INFO - 25/07/18 15:29:12 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
[2025-07-18T15:29:13.802+0000] {subprocess.py:93} INFO - 25/07/18 15:29:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:14.077+0000] {subprocess.py:93} INFO - 25/07/18 15:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:14.649+0000] {subprocess.py:93} INFO - 25/07/18 15:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:23.809+0000] {subprocess.py:93} INFO - 25/07/18 15:29:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:24.079+0000] {subprocess.py:93} INFO - 25/07/18 15:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:24.657+0000] {subprocess.py:93} INFO - 25/07/18 15:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:33.817+0000] {subprocess.py:93} INFO - 25/07/18 15:29:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:34.085+0000] {subprocess.py:93} INFO - 25/07/18 15:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:34.665+0000] {subprocess.py:93} INFO - 25/07/18 15:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:43.820+0000] {subprocess.py:93} INFO - 25/07/18 15:29:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:44.089+0000] {subprocess.py:93} INFO - 25/07/18 15:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:44.676+0000] {subprocess.py:93} INFO - 25/07/18 15:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:53.830+0000] {subprocess.py:93} INFO - 25/07/18 15:29:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:54.103+0000] {subprocess.py:93} INFO - 25/07/18 15:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:29:54.676+0000] {subprocess.py:93} INFO - 25/07/18 15:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:02.214+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/9 using temp file file:/tmp/checkpoints/reservations/offsets/.9.190287c4-9238-44ea-a751-486b718411b5.tmp
[2025-07-18T15:30:02.251+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.9.190287c4-9238-44ea-a751-486b718411b5.tmp to file:/tmp/checkpoints/reservations/offsets/9
[2025-07-18T15:30:02.251+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1752852602195,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:30:02.287+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.289+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.289+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.302+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.305+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.312+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.312+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.312+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.317+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.319+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.326+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.327+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.327+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.345+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T15:30:02.350+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T15:30:02.351+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:30:02.351+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkContext: Created broadcast 34 from start at <unknown>:0
[2025-07-18T15:30:02.351+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:30:02.353+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:30:02.355+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:30:02.355+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
[2025-07-18T15:30:02.356+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:30:02.356+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:30:02.358+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:30:02.359+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:30:02.360+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T15:30:02.361+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:30:02.364+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:30:02.365+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:30:02.366+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-07-18T15:30:02.370+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:30:02.371+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2025-07-18T15:30:02.392+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:30:02.399+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=48 untilOffset=49, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=9 taskId=17 partitionId=0
[2025-07-18T15:30:02.406+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 48 for partition reservations-0
[2025-07-18T15:30:02.410+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:30:02.592+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:02.593+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:30:02.593+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:02.597+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T15:30:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T15:30:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 1 records through 1 polls (polled  out 2 records), taking 189016042 nanos, during time span of 243454209 nanos.
[2025-07-18T15:30:02.649+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 4691 bytes result sent to driver
[2025-07-18T15:30:02.649+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 286 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:30:02.649+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-07-18T15:30:02.650+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.295 s
[2025-07-18T15:30:02.650+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:30:02.650+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-07-18T15:30:02.650+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.298406 s
[2025-07-18T15:30:02.651+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:30:02.651+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Committing epoch 9 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:30:02.659+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:02.760+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v11.metadata.json
[2025-07-18T15:30:02.850+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/10 using temp file file:/tmp/checkpoints/checkins/offsets/.10.950b51b1-c2d0-4bd6-b539-e4aa6ffdcdcf.tmp
[2025-07-18T15:30:02.853+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SnapshotProducer: Committed snapshot 7990311713054138884 (FastAppend)
[2025-07-18T15:30:02.937+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.10.950b51b1-c2d0-4bd6-b539-e4aa6ffdcdcf.tmp to file:/tmp/checkpoints/checkins/offsets/10
[2025-07-18T15:30:02.939+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1752852602826,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:30:02.971+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=7990311713054138884, sequenceNumber=10, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.280966125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=10}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=49}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2934}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=30687}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:30:02.976+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Committed in 281 ms
[2025-07-18T15:30:02.976+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:30:02.976+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:02.976+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:02.977+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:02.982+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.983+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:02.985+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/9 using temp file file:/tmp/checkpoints/reservations/commits/.9.c16941e5-3796-4677-bd7e-8a200fbc2fab.tmp
[2025-07-18T15:30:02.993+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:02.994+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:02.995+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.000+0000] {subprocess.py:93} INFO - 25/07/18 15:30:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.006+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.020+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.021+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.023+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.024+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.9.c16941e5-3796-4677-bd7e-8a200fbc2fab.tmp to file:/tmp/checkpoints/reservations/commits/9
[2025-07-18T15:30:03.064+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:30:03.065+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:30:03.065+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:30:03.066+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:30:03.067+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:30:02.193Z",
[2025-07-18T15:30:03.067+0000] {subprocess.py:93} INFO -   "batchId" : 9,
[2025-07-18T15:30:03.067+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:30:03.067+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:30:03.068+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.160092807424594,
[2025-07-18T15:30:03.069+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:30:03.070+0000] {subprocess.py:93} INFO -     "addBatch" : 633,
[2025-07-18T15:30:03.070+0000] {subprocess.py:93} INFO -     "commitOffsets" : 115,
[2025-07-18T15:30:03.071+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:30:03.071+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:30:03.076+0000] {subprocess.py:93} INFO -     "queryPlanning" : 55,
[2025-07-18T15:30:03.078+0000] {subprocess.py:93} INFO -     "triggerExecution" : 862,
[2025-07-18T15:30:03.079+0000] {subprocess.py:93} INFO -     "walCommit" : 55
[2025-07-18T15:30:03.081+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:30:03.082+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:30:03.083+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:30:03.084+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:30:03.085+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:30:03.086+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:30:03.087+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:30:03.087+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:03.087+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:03.087+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:30:03.087+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:30:03.088+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:03.092+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:03.092+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:03.092+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:30:03.093+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:30:03.093+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:03.093+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:03.094+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:03.094+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:30:03.094+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:30:03.094+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.160092807424594,
[2025-07-18T15:30:03.094+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:30:03.095+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:30:03.095+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:30:03.095+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:30:03.095+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:30:03.096+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:30:03.109+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:30:03.112+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:30:03.120+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:30:03.122+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:30:03.122+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:30:03.126+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:30:03.127+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:30:03.130+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:30:03.131+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 36 from start at <unknown>:0
[2025-07-18T15:30:03.131+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:30:03.132+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:30:03.132+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Got job 18 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:30:03.132+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
[2025-07-18T15:30:03.135+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:30:03.135+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:30:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:30:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T15:30:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/10 using temp file file:/tmp/checkpoints/reservations/offsets/.10.b23e47cc-6208-43a2-b268-1de29eac794a.tmp
[2025-07-18T15:30:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T15:30:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:30:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:30:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:30:03.137+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-07-18T15:30:03.141+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:30:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2025-07-18T15:30:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:30:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=48 untilOffset=49, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=10 taskId=18 partitionId=0
[2025-07-18T15:30:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 48 for partition checkins-0
[2025-07-18T15:30:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:30:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.10.b23e47cc-6208-43a2-b268-1de29eac794a.tmp to file:/tmp/checkpoints/reservations/offsets/10
[2025-07-18T15:30:03.144+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1752852603064,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:30:03.148+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.150+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.150+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.151+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.151+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.154+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.154+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.154+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.155+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.156+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.162+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.163+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.163+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.165+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.168+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:30:03.177+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:30:03.178+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:30:03.178+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 38 from start at <unknown>:0
[2025-07-18T15:30:03.178+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:30:03.178+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:30:03.179+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Got job 19 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:30:03.180+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Final stage: ResultStage 19 (start at <unknown>:0)
[2025-07-18T15:30:03.180+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:30:03.180+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:30:03.180+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:30:03.181+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:30:03.183+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:30:03.185+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:30:03.185+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:30:03.186+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:30:03.186+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-07-18T15:30:03.186+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:30:03.187+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2025-07-18T15:30:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:30:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=49 untilOffset=51, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=10 taskId=19 partitionId=0
[2025-07-18T15:30:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 50 for partition reservations-0
[2025-07-18T15:30:03.203+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:30:03.211+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:03.211+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:30:03.211+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:03.213+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T15:30:03.233+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T15:30:03.234+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 1 records through 1 polls (polled  out 2 records), taking 99354376 nanos, during time span of 129256001 nanos.
[2025-07-18T15:30:03.235+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 4820 bytes result sent to driver
[2025-07-18T15:30:03.237+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 152 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:30:03.258+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-07-18T15:30:03.258+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.165 s
[2025-07-18T15:30:03.259+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:30:03.259+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-07-18T15:30:03.268+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Job 18 finished: start at <unknown>:0, took 0.168363 s
[2025-07-18T15:30:03.270+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:30:03.270+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Committing epoch 10 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:30:03.286+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.357+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v12.metadata.json
[2025-07-18T15:30:03.379+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SnapshotProducer: Committed snapshot 6101562642940782545 (FastAppend)
[2025-07-18T15:30:03.401+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=6101562642940782545, sequenceNumber=11, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.113606666S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=11}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=49}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2904}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=33588}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:30:03.401+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Committed in 114 ms
[2025-07-18T15:30:03.401+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:30:03.405+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/10 using temp file file:/tmp/checkpoints/checkins/commits/.10.3b89368c-1161-4a16-a9ed-c65e797b974c.tmp
[2025-07-18T15:30:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.10.3b89368c-1161-4a16-a9ed-c65e797b974c.tmp to file:/tmp/checkpoints/checkins/commits/10
[2025-07-18T15:30:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:30:03.421+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:30:03.422+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:30:03.422+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:30:03.422+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:30:02.812Z",
[2025-07-18T15:30:03.422+0000] {subprocess.py:93} INFO -   "batchId" : 10,
[2025-07-18T15:30:03.422+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:30:03.422+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 50.0,
[2025-07-18T15:30:03.423+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.6474464579901154,
[2025-07-18T15:30:03.423+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:30:03.423+0000] {subprocess.py:93} INFO -     "addBatch" : 419,
[2025-07-18T15:30:03.424+0000] {subprocess.py:93} INFO -     "commitOffsets" : 20,
[2025-07-18T15:30:03.424+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:30:03.424+0000] {subprocess.py:93} INFO -     "latestOffset" : 13,
[2025-07-18T15:30:03.424+0000] {subprocess.py:93} INFO -     "queryPlanning" : 44,
[2025-07-18T15:30:03.424+0000] {subprocess.py:93} INFO -     "triggerExecution" : 607,
[2025-07-18T15:30:03.425+0000] {subprocess.py:93} INFO -     "walCommit" : 107
[2025-07-18T15:30:03.425+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:30:03.425+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:30:03.426+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:30:03.426+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:30:03.426+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:30:03.426+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:30:03.426+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:30:03.426+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:03.427+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:03.427+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:30:03.427+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:30:03.427+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:03.427+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:03.427+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:03.428+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:30:03.428+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:30:03.428+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:03.428+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:03.428+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:03.428+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:30:03.429+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 50.0,
[2025-07-18T15:30:03.429+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.6474464579901154,
[2025-07-18T15:30:03.430+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:30:03.430+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:30:03.430+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:30:03.431+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:30:03.431+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:30:03.431+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:30:03.431+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:30:03.431+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:30:03.431+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:30:03.432+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:30:03.432+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:30:03.432+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/9 using temp file file:/tmp/checkpoints/feedback/offsets/.9.8501973d-39f1-4c18-860b-f7a423943f2f.tmp
[2025-07-18T15:30:03.434+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/11 using temp file file:/tmp/checkpoints/checkins/offsets/.11.d25c36d5-dae0-41fe-bd19-736d9093f3ee.tmp
[2025-07-18T15:30:03.449+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.9.8501973d-39f1-4c18-860b-f7a423943f2f.tmp to file:/tmp/checkpoints/feedback/offsets/9
[2025-07-18T15:30:03.449+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MicroBatchExecution: Committed offsets for batch 9. Metadata OffsetSeqMetadata(0,1752852603419,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:30:03.454+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.11.d25c36d5-dae0-41fe-bd19-736d9093f3ee.tmp to file:/tmp/checkpoints/checkins/offsets/11
[2025-07-18T15:30:03.454+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1752852603422,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:30:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.456+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.456+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.458+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.458+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.458+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.460+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.460+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.461+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.466+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.467+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.469+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.469+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.469+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.470+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.471+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.473+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.474+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.474+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.474+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:03.475+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.476+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:03.496+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:30:03.497+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:30:03.502+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:30:03.502+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:30:03.503+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 41 from start at <unknown>:0
[2025-07-18T15:30:03.503+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:30:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:30:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Got job 20 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:30:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
[2025-07-18T15:30:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:30:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:30:03.511+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[87] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:30:03.511+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T15:30:03.517+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:30:03.518+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:30:03.520+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:30:03.522+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 40 from start at <unknown>:0
[2025-07-18T15:30:03.524+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 77cb57a6bd53:38337 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:30:03.528+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:30:03.529+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:30:03.530+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:30:03.533+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[87] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:30:03.534+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2025-07-18T15:30:03.535+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Got job 21 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:30:03.536+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Final stage: ResultStage 21 (start at <unknown>:0)
[2025-07-18T15:30:03.536+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:30:03.536+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:30:03.537+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:30:03.538+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2025-07-18T15:30:03.538+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[83] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:30:03.539+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:30:03.540+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 27.5 KiB, free 434.0 MiB)
[2025-07-18T15:30:03.541+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.0 MiB)
[2025-07-18T15:30:03.542+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 77cb57a6bd53:38337 (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T15:30:03.543+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:30:03.544+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[83] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:30:03.545+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2025-07-18T15:30:03.546+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:30:03.547+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=49 untilOffset=51, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=11 taskId=20 partitionId=0
[2025-07-18T15:30:03.548+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:30:03.550+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 77cb57a6bd53:38337 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:30:03.552+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2025-07-18T15:30:03.553+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 77cb57a6bd53:38337 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:30:03.555+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:30:03.556+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=48 untilOffset=49, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=9 taskId=21 partitionId=0
[2025-07-18T15:30:03.556+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to offset 50 for partition checkins-0
[2025-07-18T15:30:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:30:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:30:03.558+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 48 for partition feedback-0
[2025-07-18T15:30:03.561+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:30:03.625+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:03.626+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:30:03.627+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=50, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:03.635+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 21, attempt 0, stage 21.0)
[2025-07-18T15:30:03.662+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DataWritingSparkTask: Committed partition 0 (task 21, attempt 0, stage 21.0)
[2025-07-18T15:30:03.662+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 69017166 nanos, during time span of 105378292 nanos.
[2025-07-18T15:30:03.668+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 4810 bytes result sent to driver
[2025-07-18T15:30:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 145 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:30:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2025-07-18T15:30:03.683+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: ResultStage 21 (start at <unknown>:0) finished in 0.160 s
[2025-07-18T15:30:03.685+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:30:03.687+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2025-07-18T15:30:03.688+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Job 21 finished: start at <unknown>:0, took 0.162883 s
[2025-07-18T15:30:03.688+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:30:03.690+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Committing epoch 9 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:30:03.707+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:03.708+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:30:03.710+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:03.716+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T15:30:03.722+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:03.745+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DataWritingSparkTask: Committed partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T15:30:03.748+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 2 records through 1 polls (polled  out 1 records), taking 509738375 nanos, during time span of 551115626 nanos.
[2025-07-18T15:30:03.749+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 4781 bytes result sent to driver
[2025-07-18T15:30:03.759+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 573 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:30:03.760+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-07-18T15:30:03.767+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: ResultStage 19 (start at <unknown>:0) finished in 0.582 s
[2025-07-18T15:30:03.768+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:30:03.769+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2025-07-18T15:30:03.769+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO DAGScheduler: Job 19 finished: start at <unknown>:0, took 0.587980 s
[2025-07-18T15:30:03.769+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:30:03.770+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Committing epoch 10 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:30:03.809+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:30:03.892+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v11.metadata.json
[2025-07-18T15:30:03.974+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO SnapshotProducer: Committed snapshot 4296216687934797651 (FastAppend)
[2025-07-18T15:30:03.989+0000] {subprocess.py:93} INFO - 25/07/18 15:30:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v12.metadata.json
[2025-07-18T15:30:04.032+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=4296216687934797651, sequenceNumber=10, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.29880375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=10}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=49}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2906}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=30876}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:30:04.033+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Committed in 300 ms
[2025-07-18T15:30:04.033+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 9, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:30:04.059+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:04.070+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/9 using temp file file:/tmp/checkpoints/feedback/commits/.9.ad62e286-116c-4ce7-8ae0-3787cac59df0.tmp
[2025-07-18T15:30:04.088+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:30:04.092+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:04.093+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SnapshotProducer: Committed snapshot 3226502371811234018 (FastAppend)
[2025-07-18T15:30:04.107+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T15:30:04.167+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.9.ad62e286-116c-4ce7-8ae0-3787cac59df0.tmp to file:/tmp/checkpoints/feedback/commits/9
[2025-07-18T15:30:04.168+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T15:30:04.169+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor read 2 records through 1 polls (polled  out 1 records), taking 542264376 nanos, during time span of 628665084 nanos.
[2025-07-18T15:30:04.172+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 4863 bytes result sent to driver
[2025-07-18T15:30:04.187+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 654 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:30:04.188+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-07-18T15:30:04.189+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.674 s
[2025-07-18T15:30:04.192+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:30:04.194+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2025-07-18T15:30:04.197+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Job 20 finished: start at <unknown>:0, took 0.678519 s
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Committing epoch 11 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO -   "runId" : "16c6e454-9d9c-4acb-9866-9460928e2151",
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:30:03.418Z",
[2025-07-18T15:30:04.198+0000] {subprocess.py:93} INFO -   "batchId" : 9,
[2025-07-18T15:30:04.199+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:30:04.199+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:30:04.201+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.3440860215053763,
[2025-07-18T15:30:04.201+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:30:04.204+0000] {subprocess.py:93} INFO -     "addBatch" : 560,
[2025-07-18T15:30:04.204+0000] {subprocess.py:93} INFO -     "commitOffsets" : 141,
[2025-07-18T15:30:04.204+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:30:04.204+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:30:04.204+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T15:30:04.204+0000] {subprocess.py:93} INFO -     "triggerExecution" : 744,
[2025-07-18T15:30:04.204+0000] {subprocess.py:93} INFO -     "walCommit" : 29
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -         "0" : 48
[2025-07-18T15:30:04.205+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:30:04.206+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:04.207+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.208+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.209+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:30:04.209+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:30:04.210+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.3440860215053763,
[2025-07-18T15:30:04.212+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:30:04.213+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:30:04.214+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:30:04.215+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:30:04.217+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:30:04.220+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:30:04.223+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:30:04.226+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:30:04.229+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:30:04.229+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:30:04.230+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:30:04.232+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:30:04.234+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=3226502371811234018, sequenceNumber=11, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.42205375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=11}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=51}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3076}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=33763}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:30:04.246+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Committed in 424 ms
[2025-07-18T15:30:04.247+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:30:04.250+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/10 using temp file file:/tmp/checkpoints/feedback/offsets/.10.49931ba5-3600-4d6b-bfc1-50edca0ef6e1.tmp
[2025-07-18T15:30:04.267+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/10 using temp file file:/tmp/checkpoints/reservations/commits/.10.7597c562-025c-4dfc-8148-2a58e72c2041.tmp
[2025-07-18T15:30:04.339+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.10.49931ba5-3600-4d6b-bfc1-50edca0ef6e1.tmp to file:/tmp/checkpoints/feedback/offsets/10
[2025-07-18T15:30:04.341+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MicroBatchExecution: Committed offsets for batch 10. Metadata OffsetSeqMetadata(0,1752852604207,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:30:04.366+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.367+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.367+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.372+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.10.7597c562-025c-4dfc-8148-2a58e72c2041.tmp to file:/tmp/checkpoints/reservations/commits/10
[2025-07-18T15:30:04.378+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:04.379+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:30:04.382+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:30:04.383+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:30:04.387+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:30:04.388+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:30:03.061Z",
[2025-07-18T15:30:04.388+0000] {subprocess.py:93} INFO -   "batchId" : 10,
[2025-07-18T15:30:04.388+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:30:04.388+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.3041474654377883,
[2025-07-18T15:30:04.389+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.529051987767584,
[2025-07-18T15:30:04.390+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:30:04.392+0000] {subprocess.py:93} INFO -     "addBatch" : 1085,
[2025-07-18T15:30:04.396+0000] {subprocess.py:93} INFO -     "commitOffsets" : 135,
[2025-07-18T15:30:04.397+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:30:04.398+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:30:04.399+0000] {subprocess.py:93} INFO -     "queryPlanning" : 10,
[2025-07-18T15:30:04.400+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1308,
[2025-07-18T15:30:04.402+0000] {subprocess.py:93} INFO -     "walCommit" : 74
[2025-07-18T15:30:04.406+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:30:04.408+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:30:04.410+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:30:04.414+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:30:04.416+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:30:04.417+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:30:04.418+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:04.421+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.421+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.423+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:30:04.424+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:30:04.425+0000] {subprocess.py:93} INFO -         "0" : 51
[2025-07-18T15:30:04.426+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.427+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.428+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:30:04.429+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:30:04.429+0000] {subprocess.py:93} INFO -         "0" : 51
[2025-07-18T15:30:04.429+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.430+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.430+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:30:04.430+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.3041474654377883,
[2025-07-18T15:30:04.431+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.529051987767584,
[2025-07-18T15:30:04.431+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:30:04.431+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:30:04.431+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:30:04.432+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:30:04.432+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:30:04.434+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:30:04.435+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:30:04.436+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:30:04.436+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:30:04.437+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:30:04.438+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:30:04.439+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:04.440+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.440+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.442+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.443+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:04.443+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:04.444+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.446+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.447+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:04.448+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:04.448+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:30:04.449+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:30:04.450+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:30:04.450+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:30:04.450+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkContext: Created broadcast 44 from start at <unknown>:0
[2025-07-18T15:30:04.451+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:30:04.451+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:30:04.451+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Got job 22 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:30:04.453+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Final stage: ResultStage 22 (start at <unknown>:0)
[2025-07-18T15:30:04.455+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:30:04.456+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:30:04.457+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[91] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:30:04.458+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 27.5 KiB, free 434.0 MiB)
[2025-07-18T15:30:04.459+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.0 MiB)
[2025-07-18T15:30:04.460+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 77cb57a6bd53:38337 (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T15:30:04.461+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:30:04.461+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[91] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:30:04.462+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-07-18T15:30:04.466+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:30:04.467+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2025-07-18T15:30:04.468+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v13.metadata.json
[2025-07-18T15:30:04.469+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:30:04.470+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=49 untilOffset=51, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=10 taskId=22 partitionId=0
[2025-07-18T15:30:04.470+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 49 for partition feedback-0
[2025-07-18T15:30:04.472+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:30:04.475+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:04.476+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:30:04.477+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:04.480+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to offset 50 for partition feedback-0
[2025-07-18T15:30:04.492+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:30:04.525+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SnapshotProducer: Committed snapshot 5025746658896320670 (FastAppend)
[2025-07-18T15:30:04.570+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=5025746658896320670, sequenceNumber=12, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.352993708S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=12}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=51}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2906}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=36494}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:30:04.571+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SparkWrite: Committed in 353 ms
[2025-07-18T15:30:04.571+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:30:04.579+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/11 using temp file file:/tmp/checkpoints/checkins/commits/.11.658621b9-c66f-4a17-845f-d7728caadfd4.tmp
[2025-07-18T15:30:04.594+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.11.658621b9-c66f-4a17-845f-d7728caadfd4.tmp to file:/tmp/checkpoints/checkins/commits/11
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO -   "runId" : "9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d",
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:30:03.420Z",
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO -   "batchId" : 11,
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:30:04.596+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.2894736842105265,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.7050298380221653,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "addBatch" : 1108,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "commitOffsets" : 24,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1173,
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "walCommit" : 30
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:30:04.597+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:30:04.598+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:30:04.598+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:30:04.598+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -         "0" : 51
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -         "0" : 51
[2025-07-18T15:30:04.599+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.2894736842105265,
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.7050298380221653,
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:30:04.600+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:30:04.602+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:30:04.602+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:30:04.602+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:30:04.602+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:30:04.602+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:30:04.602+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:30:04.602+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:30:04.987+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:04.987+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:30:04.988+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=51, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:30:04.989+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 22, attempt 0, stage 22.0)
[2025-07-18T15:30:04.999+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO DataWritingSparkTask: Committed partition 0 (task 22, attempt 0, stage 22.0)
[2025-07-18T15:30:04.999+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor read 2 records through 2 polls (polled  out 2 records), taking 515103584 nanos, during time span of 532941625 nanos.
[2025-07-18T15:30:05.000+0000] {subprocess.py:93} INFO - 25/07/18 15:30:04 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 4777 bytes result sent to driver
[2025-07-18T15:30:05.001+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 568 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:30:05.001+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-07-18T15:30:05.002+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO DAGScheduler: ResultStage 22 (start at <unknown>:0) finished in 0.580 s
[2025-07-18T15:30:05.002+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:30:05.002+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-07-18T15:30:05.002+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO DAGScheduler: Job 22 finished: start at <unknown>:0, took 0.587163 s
[2025-07-18T15:30:05.002+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:30:05.002+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO SparkWrite: Committing epoch 10 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:30:05.010+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:30:05.080+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v12.metadata.json
[2025-07-18T15:30:05.106+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO SnapshotProducer: Committed snapshot 3020884300831446094 (FastAppend)
[2025-07-18T15:30:05.119+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=3020884300831446094, sequenceNumber=11, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.108806208S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=11}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=51}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2891}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=33767}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:30:05.120+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO SparkWrite: Committed in 109 ms
[2025-07-18T15:30:05.120+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:30:05.123+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/10 using temp file file:/tmp/checkpoints/feedback/commits/.10.f4b83549-d7bc-44a5-9e5d-f47eb0b126b3.tmp
[2025-07-18T15:30:05.135+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.10.f4b83549-d7bc-44a5-9e5d-f47eb0b126b3.tmp to file:/tmp/checkpoints/feedback/commits/10
[2025-07-18T15:30:05.135+0000] {subprocess.py:93} INFO - 25/07/18 15:30:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:30:05.135+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:30:05.135+0000] {subprocess.py:93} INFO -   "runId" : "16c6e454-9d9c-4acb-9866-9460928e2151",
[2025-07-18T15:30:05.136+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:30:04.178Z",
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -   "batchId" : 10,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.6315789473684212,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.092050209205021,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -     "addBatch" : 733,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -     "latestOffset" : 29,
[2025-07-18T15:30:05.137+0000] {subprocess.py:93} INFO -     "queryPlanning" : 43,
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -     "triggerExecution" : 956,
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -     "walCommit" : 131
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -         "0" : 49
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:05.138+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -         "0" : 51
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -         "0" : 51
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:30:05.139+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.6315789473684212,
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.092050209205021,
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:30:05.140+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:30:14.388+0000] {subprocess.py:93} INFO - 25/07/18 15:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:14.596+0000] {subprocess.py:93} INFO - 25/07/18 15:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:15.146+0000] {subprocess.py:93} INFO - 25/07/18 15:30:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:19.982+0000] {subprocess.py:93} INFO - 25/07/18 15:30:19 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 77cb57a6bd53:38337 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:30:19.988+0000] {subprocess.py:93} INFO - 25/07/18 15:30:19 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:30:20.002+0000] {subprocess.py:93} INFO - 25/07/18 15:30:19 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:30:20.014+0000] {subprocess.py:93} INFO - 25/07/18 15:30:20 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 77cb57a6bd53:38337 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:30:20.022+0000] {subprocess.py:93} INFO - 25/07/18 15:30:20 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:30:20.028+0000] {subprocess.py:93} INFO - 25/07/18 15:30:20 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 77cb57a6bd53:38337 in memory (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:30:20.033+0000] {subprocess.py:93} INFO - 25/07/18 15:30:20 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 77cb57a6bd53:38337 in memory (size: 12.2 KiB, free: 434.4 MiB)
[2025-07-18T15:30:20.037+0000] {subprocess.py:93} INFO - 25/07/18 15:30:20 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 77cb57a6bd53:38337 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:30:24.395+0000] {subprocess.py:93} INFO - 25/07/18 15:30:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:24.607+0000] {subprocess.py:93} INFO - 25/07/18 15:30:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:25.144+0000] {subprocess.py:93} INFO - 25/07/18 15:30:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:34.404+0000] {subprocess.py:93} INFO - 25/07/18 15:30:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:34.606+0000] {subprocess.py:93} INFO - 25/07/18 15:30:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:35.160+0000] {subprocess.py:93} INFO - 25/07/18 15:30:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:44.413+0000] {subprocess.py:93} INFO - 25/07/18 15:30:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:44.608+0000] {subprocess.py:93} INFO - 25/07/18 15:30:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:45.167+0000] {subprocess.py:93} INFO - 25/07/18 15:30:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:54.412+0000] {subprocess.py:93} INFO - 25/07/18 15:30:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:54.621+0000] {subprocess.py:93} INFO - 25/07/18 15:30:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:30:55.176+0000] {subprocess.py:93} INFO - 25/07/18 15:30:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:04.417+0000] {subprocess.py:93} INFO - 25/07/18 15:31:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:04.619+0000] {subprocess.py:93} INFO - 25/07/18 15:31:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:05.180+0000] {subprocess.py:93} INFO - 25/07/18 15:31:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:14.419+0000] {subprocess.py:93} INFO - 25/07/18 15:31:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:14.626+0000] {subprocess.py:93} INFO - 25/07/18 15:31:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:15.191+0000] {subprocess.py:93} INFO - 25/07/18 15:31:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:24.428+0000] {subprocess.py:93} INFO - 25/07/18 15:31:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:24.626+0000] {subprocess.py:93} INFO - 25/07/18 15:31:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:25.195+0000] {subprocess.py:93} INFO - 25/07/18 15:31:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:34.427+0000] {subprocess.py:93} INFO - 25/07/18 15:31:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:34.636+0000] {subprocess.py:93} INFO - 25/07/18 15:31:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:35.195+0000] {subprocess.py:93} INFO - 25/07/18 15:31:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:44.435+0000] {subprocess.py:93} INFO - 25/07/18 15:31:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:44.647+0000] {subprocess.py:93} INFO - 25/07/18 15:31:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:45.208+0000] {subprocess.py:93} INFO - 25/07/18 15:31:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:54.441+0000] {subprocess.py:93} INFO - 25/07/18 15:31:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:54.655+0000] {subprocess.py:93} INFO - 25/07/18 15:31:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:31:55.210+0000] {subprocess.py:93} INFO - 25/07/18 15:31:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:32:02.928+0000] {subprocess.py:93} INFO - 25/07/18 15:32:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/11 using temp file file:/tmp/checkpoints/reservations/offsets/.11.b2b5a009-4bee-4b10-b353-b43e0c86fe27.tmp
[2025-07-18T15:32:02.950+0000] {subprocess.py:93} INFO - 25/07/18 15:32:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.11.b2b5a009-4bee-4b10-b353-b43e0c86fe27.tmp to file:/tmp/checkpoints/reservations/offsets/11
[2025-07-18T15:32:02.951+0000] {subprocess.py:93} INFO - 25/07/18 15:32:02 INFO MicroBatchExecution: Committed offsets for batch 11. Metadata OffsetSeqMetadata(0,1752852722913,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:32:02.991+0000] {subprocess.py:93} INFO - 25/07/18 15:32:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:02.991+0000] {subprocess.py:93} INFO - 25/07/18 15:32:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:02.992+0000] {subprocess.py:93} INFO - 25/07/18 15:32:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:02.999+0000] {subprocess.py:93} INFO - 25/07/18 15:32:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.002+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.010+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.011+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.012+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.013+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.014+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.024+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.029+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T15:32:03.048+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T15:32:03.048+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 77cb57a6bd53:38337 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T15:32:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Created broadcast 46 from start at <unknown>:0
[2025-07-18T15:32:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 11, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:32:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:32:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Got job 23 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:32:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Final stage: ResultStage 23 (start at <unknown>:0)
[2025-07-18T15:32:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:32:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:32:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[95] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:32:03.057+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T15:32:03.061+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T15:32:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T15:32:03.063+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:32:03.063+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[95] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:32:03.064+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2025-07-18T15:32:03.065+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:32:03.066+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2025-07-18T15:32:03.078+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:32:03.080+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=51 untilOffset=52, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=11 taskId=23 partitionId=0
[2025-07-18T15:32:03.086+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 51 for partition reservations-0
[2025-07-18T15:32:03.093+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:32:03.116+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:32:03.117+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:32:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=53, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:32:03.131+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 23, attempt 0, stage 23.0)
[2025-07-18T15:32:03.190+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 23.0)
[2025-07-18T15:32:03.190+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 33169417 nanos, during time span of 99084167 nanos.
[2025-07-18T15:32:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 4736 bytes result sent to driver
[2025-07-18T15:32:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 125 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:32:03.191+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-07-18T15:32:03.192+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: ResultStage 23 (start at <unknown>:0) finished in 0.134 s
[2025-07-18T15:32:03.192+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:32:03.192+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2025-07-18T15:32:03.193+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Job 23 finished: start at <unknown>:0, took 0.138026 s
[2025-07-18T15:32:03.193+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:32:03.193+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Committing epoch 11 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:32:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.397+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v13.metadata.json
[2025-07-18T15:32:03.430+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SnapshotProducer: Committed snapshot 8857958141751552906 (FastAppend)
[2025-07-18T15:32:03.473+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=8857958141751552906, sequenceNumber=12, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.270005917S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=12}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=52}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2941}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=36704}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752852247670, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:32:03.474+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Committed in 271 ms
[2025-07-18T15:32:03.475+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 11, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:32:03.485+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/11 using temp file file:/tmp/checkpoints/reservations/commits/.11.3201333e-0855-49bf-88f1-1055bda7bc31.tmp
[2025-07-18T15:32:03.512+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.11.3201333e-0855-49bf-88f1-1055bda7bc31.tmp to file:/tmp/checkpoints/reservations/commits/11
[2025-07-18T15:32:03.513+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:32:03.514+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:32:03.514+0000] {subprocess.py:93} INFO -   "runId" : "c6517251-9527-4d66-968c-7d04d13cb56e",
[2025-07-18T15:32:03.515+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:32:03.515+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:32:02.912Z",
[2025-07-18T15:32:03.515+0000] {subprocess.py:93} INFO -   "batchId" : 11,
[2025-07-18T15:32:03.519+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:32:03.520+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:32:03.521+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.669449081803005,
[2025-07-18T15:32:03.521+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:32:03.521+0000] {subprocess.py:93} INFO -     "addBatch" : 466,
[2025-07-18T15:32:03.521+0000] {subprocess.py:93} INFO -     "commitOffsets" : 40,
[2025-07-18T15:32:03.522+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:32:03.522+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:32:03.522+0000] {subprocess.py:93} INFO -     "queryPlanning" : 53,
[2025-07-18T15:32:03.522+0000] {subprocess.py:93} INFO -     "triggerExecution" : 599,
[2025-07-18T15:32:03.522+0000] {subprocess.py:93} INFO -     "walCommit" : 36
[2025-07-18T15:32:03.523+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:32:03.523+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:32:03.523+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:32:03.523+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:32:03.524+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:32:03.524+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:32:03.525+0000] {subprocess.py:93} INFO -         "0" : 51
[2025-07-18T15:32:03.526+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:32:03.527+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:32:03.527+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:32:03.528+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:32:03.528+0000] {subprocess.py:93} INFO -         "0" : 52
[2025-07-18T15:32:03.528+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:32:03.528+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:32:03.529+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:32:03.529+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:32:03.529+0000] {subprocess.py:93} INFO -         "0" : 52
[2025-07-18T15:32:03.529+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:32:03.529+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:32:03.529+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:32:03.529+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:32:03.530+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.669449081803005,
[2025-07-18T15:32:03.530+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:32:03.532+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:32:03.533+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:32:03.534+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:32:03.534+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:32:03.534+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:32:03.534+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:32:03.534+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:32:03.535+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:32:03.535+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:32:03.535+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:32:03.535+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/12 using temp file file:/tmp/checkpoints/reservations/offsets/.12.b2f83afc-7356-47d3-abd0-ac70b980c795.tmp
[2025-07-18T15:32:03.541+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/12 using temp file file:/tmp/checkpoints/checkins/offsets/.12.7e4ff74d-0d1d-42ef-b681-6214e5d80d58.tmp
[2025-07-18T15:32:03.554+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.12.b2f83afc-7356-47d3-abd0-ac70b980c795.tmp to file:/tmp/checkpoints/reservations/offsets/12
[2025-07-18T15:32:03.555+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MicroBatchExecution: Committed offsets for batch 12. Metadata OffsetSeqMetadata(0,1752852723514,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:32:03.562+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 WARN CheckpointFileManager: Failed to rename temp file file:/tmp/checkpoints/checkins/offsets/.12.7e4ff74d-0d1d-42ef-b681-6214e5d80d58.tmp to file:/tmp/checkpoints/checkins/offsets/12 because file exists
[2025-07-18T15:32:03.563+0000] {subprocess.py:93} INFO - org.apache.hadoop.fs.FileAlreadyExistsException: rename destination file:/tmp/checkpoints/checkins/offsets/12 already exists.
[2025-07-18T15:32:03.565+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1600)
[2025-07-18T15:32:03.567+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)
[2025-07-18T15:32:03.568+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)
[2025-07-18T15:32:03.569+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:32:03.570+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)
[2025-07-18T15:32:03.572+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:32:03.574+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)
[2025-07-18T15:32:03.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)
[2025-07-18T15:32:03.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)
[2025-07-18T15:32:03.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:204)
[2025-07-18T15:32:03.579+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237)
[2025-07-18T15:32:03.582+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130)
[2025-07-18T15:32:03.584+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:764)
[2025-07-18T15:32:03.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:536)
[2025-07-18T15:32:03.587+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:32:03.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:32:03.588+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:32:03.589+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:535)
[2025-07-18T15:32:03.589+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T15:32:03.589+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T15:32:03.590+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T15:32:03.590+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T15:32:03.591+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.591+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:32:03.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:32:03.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:32:03.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T15:32:03.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T15:32:03.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T15:32:03.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T15:32:03.597+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.597+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:32:03.598+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T15:32:03.598+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T15:32:03.599+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.599+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T15:32:03.600+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T15:32:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.602+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:32:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:32:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:32:03.606+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:32:03.607+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 77cb57a6bd53:38337 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:32:03.607+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Created broadcast 48 from start at <unknown>:0
[2025-07-18T15:32:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 12, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:32:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:32:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Got job 24 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:32:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Final stage: ResultStage 24 (start at <unknown>:0)
[2025-07-18T15:32:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:32:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:32:03.611+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[99] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:32:03.611+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T15:32:03.622+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T15:32:03.623+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 77cb57a6bd53:38337 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:32:03.623+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:32:03.623+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[99] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:32:03.624+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2025-07-18T15:32:03.625+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:32:03.625+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2025-07-18T15:32:03.633+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:32:03.634+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=52 untilOffset=54, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=12 taskId=24 partitionId=0
[2025-07-18T15:32:03.635+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 52 for partition reservations-0
[2025-07-18T15:32:03.636+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:32:03.636+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:32:03.636+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:32:03.637+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=54, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:32:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to offset 53 for partition reservations-0
[2025-07-18T15:32:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor-1, groupId=spark-kafka-source-e2c85640-a436-444a-93f3-4723d354ef80-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:32:03.655+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 ERROR MicroBatchExecution: Query [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d] terminated with error
[2025-07-18T15:32:03.656+0000] {subprocess.py:93} INFO - org.apache.spark.SparkConcurrentModificationException: Multiple streaming queries are concurrently using file:/tmp/checkpoints/checkins/offsets.
[2025-07-18T15:32:03.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.multiStreamingQueriesUsingPathConcurrentlyError(QueryExecutionErrors.scala:1858)
[2025-07-18T15:32:03.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:210)
[2025-07-18T15:32:03.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.addNewBatchByStream(HDFSMetadataLog.scala:237)
[2025-07-18T15:32:03.656+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.add(HDFSMetadataLog.scala:130)
[2025-07-18T15:32:03.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.markMicroBatchStart(MicroBatchExecution.scala:764)
[2025-07-18T15:32:03.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$13(MicroBatchExecution.scala:536)
[2025-07-18T15:32:03.657+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:32:03.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:32:03.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:32:03.657+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:535)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:32:03.658+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T15:32:03.659+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: rename destination file:/tmp/checkpoints/checkins/offsets/12 already exists.
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1600)
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:32:03.660+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:489)
[2025-07-18T15:32:03.661+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)
[2025-07-18T15:32:03.661+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)
[2025-07-18T15:32:03.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)
[2025-07-18T15:32:03.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)
[2025-07-18T15:32:03.661+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.write(HDFSMetadataLog.scala:204)
[2025-07-18T15:32:03.661+0000] {subprocess.py:93} INFO - 	... 28 more
[2025-07-18T15:32:03.661+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
[2025-07-18T15:32:03.663+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:32:03.663+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:32:03.663+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:32:03.664+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MicroBatchExecution: Async log purge executor pool for query [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d] has been shutdown
[2025-07-18T15:32:03.700+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-07-18T15:32:03.701+0000] {subprocess.py:93} INFO -   File "/home/iceberg/spark/stream_to_bronze.py", line 94, in <module>
[2025-07-18T15:32:03.702+0000] {subprocess.py:93} INFO -     spark.streams.awaitAnyTermination()
[2025-07-18T15:32:03.703+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 596, in awaitAnyTermination
[2025-07-18T15:32:03.704+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-07-18T15:32:03.705+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-07-18T15:32:03.710+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 9c3dcb26-4cdf-43c6-ba81-68e2e1f0da0d] terminated with exception: Multiple streaming queries are concurrently using file:/tmp/checkpoints/checkins/offsets.
[2025-07-18T15:32:03.746+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-07-18T15:32:03.746+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2, groupId=spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-07-18T15:32:03.751+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:32:03.751+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:32:03.752+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:32:03.752+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-b688dcb9-eab4-4987-855e-04b756644a03-2028037020-executor-2 unregistered
[2025-07-18T15:32:03.753+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
[2025-07-18T15:32:03.753+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3, groupId=spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor] Request joining group due to: consumer pro-actively leaving the group
[2025-07-18T15:32:03.753+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:32:03.754+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:32:03.754+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:32:03.755+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-85a63b43-fd5a-49c4-9712-b5e66a12b6bc-603580605-executor-3 unregistered
[2025-07-18T15:32:03.755+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Invoking stop() from shutdown hook
[2025-07-18T15:32:03.755+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-07-18T15:32:03.766+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkUI: Stopped Spark web UI at http://77cb57a6bd53:4040
[2025-07-18T15:32:03.771+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: ResultStage 24 (start at <unknown>:0) failed in 0.161 s due to Stage cancelled because SparkContext was shut down
[2025-07-18T15:32:03.772+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO DAGScheduler: Job 24 failed: start at <unknown>:0, took 0.163311 s
[2025-07-18T15:32:03.772+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is aborting.
[2025-07-18T15:32:03.775+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 12, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] failed to abort.
[2025-07-18T15:32:03.778+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 ERROR MicroBatchExecution: Query [id = 0314df7c-5598-4928-8d91-374ee67989d1, runId = c6517251-9527-4d66-968c-7d04d13cb56e] terminated with error
[2025-07-18T15:32:03.778+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Writing job failed.
[2025-07-18T15:32:03.778+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.writingJobFailedError(QueryExecutionErrors.scala:903)
[2025-07-18T15:32:03.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:416)
[2025-07-18T15:32:03.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T15:32:03.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T15:32:03.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T15:32:03.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T15:32:03.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T15:32:03.779+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T15:32:03.780+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T15:32:03.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T15:32:03.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T15:32:03.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T15:32:03.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T15:32:03.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T15:32:03.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:32:03.781+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.782+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.783+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job 24 cancelled because SparkContext was shut down
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1259)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1257)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1257)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3129)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3015)
[2025-07-18T15:32:03.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3015)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2258)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2258)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.stop(SparkContext.scala:2211)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:681)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at scala.util.Try$.apply(Try.scala:213)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-07-18T15:32:03.786+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:995)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 	... 45 more
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 	Suppressed: java.lang.IllegalStateException: Shutdown in progress
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 		at java.base/java.lang.ApplicationShutdownHooks.add(Unknown Source)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 		at java.base/java.lang.Runtime.addShutdownHook(Unknown Source)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addShutdownHook(MoreExecutors.java:289)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.addDelayedShutdownHook(MoreExecutors.java:266)
[2025-07-18T15:32:03.787+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:241)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors$Application.getExitingExecutorService(MoreExecutors.java:246)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors.getExitingExecutorService(MoreExecutors.java:129)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.util.ThreadPools.newWorkerPool(ThreadPools.java:67)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.hadoop.HadoopFileIO.executorService(HadoopFileIO.java:195)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.hadoop.HadoopFileIO.deleteFiles(HadoopFileIO.java:170)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkCleanupUtil.bulkDelete(SparkCleanupUtil.java:99)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkCleanupUtil.deletePaths(SparkCleanupUtil.java:91)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkCleanupUtil.deleteFiles(SparkCleanupUtil.java:85)
[2025-07-18T15:32:03.788+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkWrite.abort(SparkWrite.java:244)
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkWrite.access$800(SparkWrite.java:84)
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 		at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.abort(SparkWrite.java:545)
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 		at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.abort(MicroBatchWrite.scala:43)
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 		at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:411)
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 		... 45 more
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics scheduler closed
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T15:32:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO Metrics: Metrics reporters closed
[2025-07-18T15:32:03.790+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MicroBatchExecution: Async log purge executor pool for query [id = 0314df7c-5598-4928-8d91-374ee67989d1, runId = c6517251-9527-4d66-968c-7d04d13cb56e] has been shutdown
[2025-07-18T15:32:03.790+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-07-18T15:32:03.790+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MemoryStore: MemoryStore cleared
[2025-07-18T15:32:03.790+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO BlockManager: BlockManager stopped
[2025-07-18T15:32:03.790+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-07-18T15:32:03.791+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-07-18T15:32:03.796+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO SparkContext: Successfully stopped SparkContext
[2025-07-18T15:32:03.797+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ShutdownHookManager: Shutdown hook called
[2025-07-18T15:32:03.797+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-0acdac31-b3ba-46e0-bdb0-d346fd572187
[2025-07-18T15:32:03.798+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-0acdac31-b3ba-46e0-bdb0-d346fd572187/pyspark-bfab7f53-29be-417f-8fe6-7b98721ce2fc
[2025-07-18T15:32:03.800+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-c16a54b4-398d-4b4f-8417-8d212d63e0da
[2025-07-18T15:32:03.803+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-07-18T15:32:03.803+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-07-18T15:32:03.804+0000] {subprocess.py:93} INFO - 25/07/18 15:32:03 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-07-18T15:32:04.210+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-07-18T15:32:04.224+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-07-18T15:32:04.227+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=restaurant_pipeline, task_id=stream_to_bronze, execution_date=20250718T152200, start_date=20250718T152405, end_date=20250718T153204
[2025-07-18T15:32:04.253+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 99 for task stream_to_bronze (Bash command failed. The command returned a non-zero exit code 1.; 297)
[2025-07-18T15:32:04.290+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-07-18T15:32:04.320+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
