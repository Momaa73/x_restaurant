[2025-07-18T16:14:35.249+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze manual__2025-07-18T16:14:30.001261+00:00 [queued]>
[2025-07-18T16:14:35.273+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze manual__2025-07-18T16:14:30.001261+00:00 [queued]>
[2025-07-18T16:14:35.273+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-07-18T16:14:35.285+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): stream_to_bronze> on 2025-07-18 16:14:30.001261+00:00
[2025-07-18T16:14:35.289+0000] {standard_task_runner.py:57} INFO - Started process 1250 to run task
[2025-07-18T16:14:35.291+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'restaurant_pipeline', 'stream_to_bronze', 'manual__2025-07-18T16:14:30.001261+00:00', '--job-id', '180', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_pipeline.py', '--cfg-path', '/tmp/tmplrq9e9fk']
[2025-07-18T16:14:35.296+0000] {standard_task_runner.py:85} INFO - Job 180: Subtask stream_to_bronze
[2025-07-18T16:14:35.335+0000] {task_command.py:416} INFO - Running <TaskInstance: restaurant_pipeline.stream_to_bronze manual__2025-07-18T16:14:30.001261+00:00 [running]> on host 9bcfb43e0ab7
[2025-07-18T16:14:35.377+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='moran' AIRFLOW_CTX_DAG_ID='restaurant_pipeline' AIRFLOW_CTX_TASK_ID='stream_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-07-18T16:14:30.001261+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-18T16:14:30.001261+00:00'
[2025-07-18T16:14:35.377+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-07-18T16:14:35.378+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', "docker exec -e AIRFLOW_CTX_DAG_RUN_ID='manual__2025-07-18T16:14:30.001261+00:00' spark-iceberg spark-submit /home/iceberg/spark/stream_to_bronze.py"]
[2025-07-18T16:14:35.383+0000] {subprocess.py:86} INFO - Output:
[2025-07-18T16:14:37.541+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkContext: Running Spark version 3.5.6
[2025-07-18T16:14:37.542+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:14:37.542+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkContext: Java version 17.0.15
[2025-07-18T16:14:37.558+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO ResourceUtils: ==============================================================
[2025-07-18T16:14:37.558+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-18T16:14:37.558+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO ResourceUtils: ==============================================================
[2025-07-18T16:14:37.558+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkContext: Submitted application: StreamToBronze
[2025-07-18T16:14:37.568+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-18T16:14:37.573+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO ResourceProfile: Limiting resource is cpu
[2025-07-18T16:14:37.573+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-18T16:14:37.602+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SecurityManager: Changing view acls to: root,spark
[2025-07-18T16:14:37.602+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SecurityManager: Changing modify acls to: root,spark
[2025-07-18T16:14:37.602+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SecurityManager: Changing view acls groups to:
[2025-07-18T16:14:37.602+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SecurityManager: Changing modify acls groups to:
[2025-07-18T16:14:37.602+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
[2025-07-18T16:14:37.634+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-18T16:14:37.791+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO Utils: Successfully started service 'sparkDriver' on port 35465.
[2025-07-18T16:14:37.810+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkEnv: Registering MapOutputTracker
[2025-07-18T16:14:37.838+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-18T16:14:37.852+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-18T16:14:37.852+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-18T16:14:37.854+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-18T16:14:37.868+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-65b7ab7f-a9ce-4ce9-bd2c-e6e031b56aa6
[2025-07-18T16:14:37.875+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-18T16:14:37.883+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-18T16:14:37.952+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-18T16:14:38.000+0000] {subprocess.py:93} INFO - 25/07/18 16:14:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-07-18T16:14:38.061+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO Executor: Starting executor ID driver on host 77cb57a6bd53
[2025-07-18T16:14:38.061+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:14:38.061+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO Executor: Java version 17.0.15
[2025-07-18T16:14:38.064+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-18T16:14:38.065+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2801f36f for default.
[2025-07-18T16:14:38.082+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41117.
[2025-07-18T16:14:38.082+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO NettyBlockTransferService: Server created on 77cb57a6bd53:41117
[2025-07-18T16:14:38.082+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-18T16:14:38.087+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 77cb57a6bd53, 41117, None)
[2025-07-18T16:14:38.088+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO BlockManagerMasterEndpoint: Registering block manager 77cb57a6bd53:41117 with 434.4 MiB RAM, BlockManagerId(driver, 77cb57a6bd53, 41117, None)
[2025-07-18T16:14:38.090+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 77cb57a6bd53, 41117, None)
[2025-07-18T16:14:38.090+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 77cb57a6bd53, 41117, None)
[2025-07-18T16:14:38.365+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-18T16:14:38.370+0000] {subprocess.py:93} INFO - 25/07/18 16:14:38 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-07-18T16:14:39.474+0000] {subprocess.py:93} INFO - 25/07/18 16:14:39 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-07-18T16:14:39.480+0000] {subprocess.py:93} INFO - 25/07/18 16:14:39 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-07-18T16:14:39.481+0000] {subprocess.py:93} INFO - 25/07/18 16:14:39 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-07-18T16:14:40.626+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:40.675+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-07-18T16:14:40.746+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00 resolved to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00.
[2025-07-18T16:14:40.747+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:14:40.813+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/metadata using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/.metadata.a356d9f1-d4cf-41d3-9bd6-1d7426ee0d8d.tmp
[2025-07-18T16:14:40.875+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/.metadata.a356d9f1-d4cf-41d3-9bd6-1d7426ee0d8d.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/metadata
[2025-07-18T16:14:40.897+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO MicroBatchExecution: Starting [id = 91de1f1c-31ac-4205-a078-ccc8a3f415c2, runId = d7826a15-f048-4c6f-ac0e-1c932a7da334]. Use file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00 to store the query checkpoint.
[2025-07-18T16:14:40.903+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@59ed35e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@60a54ac]
[2025-07-18T16:14:40.929+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:14:40.930+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:14:40.931+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:14:40.932+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:14:40.978+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:40.984+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00 resolved to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00.
[2025-07-18T16:14:40.985+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:14:40.991+0000] {subprocess.py:93} INFO - 25/07/18 16:14:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/metadata using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/.metadata.356b7bd7-e541-49d3-b757-ce2341e40326.tmp
[2025-07-18T16:14:41.009+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/.metadata.356b7bd7-e541-49d3-b757-ce2341e40326.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/metadata
[2025-07-18T16:14:41.016+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Starting [id = c9c7dd2e-34f6-40c1-860c-fbb9629e8b19, runId = 7d171787-b562-405b-afd9-54b53d58d80e]. Use file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00 to store the query checkpoint.
[2025-07-18T16:14:41.018+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@71995f20] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7f5eaebb]
[2025-07-18T16:14:41.019+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:14:41.020+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:14:41.020+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:14:41.020+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:14:41.100+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.100+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00 resolved to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00.
[2025-07-18T16:14:41.100+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:14:41.108+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/metadata using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/.metadata.c2eb9315-eb71-4461-976a-6c2538e5b2e2.tmp
[2025-07-18T16:14:41.126+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/.metadata.c2eb9315-eb71-4461-976a-6c2538e5b2e2.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/metadata
[2025-07-18T16:14:41.136+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Starting [id = ec415088-91b0-434b-9242-7a8791fccbbd, runId = 125cb6b4-915e-4720-aa51-b7c4bad8f8f7]. Use file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00 to store the query checkpoint.
[2025-07-18T16:14:41.137+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5f4618fc] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5181ecbc]
[2025-07-18T16:14:41.138+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:14:41.139+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:14:41.139+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:14:41.139+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:14:41.146+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:14:41.147+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:14:41.147+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:14:41.148+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:14:41.148+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:14:41.149+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:14:41.149+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:14:41.149+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:14:41.150+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:14:41.150+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:14:41.150+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:14:41.151+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:14:41.151+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:14:41.151+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:14:41.152+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:14:41.152+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:14:41.152+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:14:41.152+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:14:41.153+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:14:41.153+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:14:41.154+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:14:41.154+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:14:41.155+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:14:41.155+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:14:41.156+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:14:41.156+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:14:41.156+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:14:41.157+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:14:41.157+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:14:41.158+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:14:41.158+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:14:41.158+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:14:41.158+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:14:41.159+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:14:41.160+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:14:41.161+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:14:41.162+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:14:41.163+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:14:41.164+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:14:41.165+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:14:41.166+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:14:41.167+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:14:41.167+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:14:41.167+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:14:41.167+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:14:41.167+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:14:41.167+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:14:41.167+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:14:41.168+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:14:41.168+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:14:41.169+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:14:41.169+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:14:41.169+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:14:41.170+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:14:41.170+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:14:41.171+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:14:41.171+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:14:41.171+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:14:41.171+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:14:41.171+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:14:41.171+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:14:41.172+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:14:41.172+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:14:41.172+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:14:41.175+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:14:41.175+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:14:41.175+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:14:41.175+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:14:41.175+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:14:41.175+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:14:41.175+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:14:41.176+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:14:41.176+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:14:41.176+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:14:41.176+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:14:41.176+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:14:41.176+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:14:41.176+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:14:41.177+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:14:41.177+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:14:41.177+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:14:41.177+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:14:41.178+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:14:41.178+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:14:41.178+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:14:41.178+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:14:41.178+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:14:41.179+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:14:41.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:14:41.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:14:41.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:14:41.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:14:41.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:14:41.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:14:41.180+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:14:41.181+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:14:41.182+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:14:41.182+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:14:41.182+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:14:41.182+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:14:41.182+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:14:41.182+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:14:41.182+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:14:41.183+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:14:41.183+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:14:41.183+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:14:41.183+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:14:41.183+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:14:41.200+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:14:41.200+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:14:41.200+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:14:41.200+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:14:41.201+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:14:41.201+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka startTimeMs: 1752855281199
[2025-07-18T16:14:41.203+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:14:41.203+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:14:41.203+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka startTimeMs: 1752855281199
[2025-07-18T16:14:41.203+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:14:41.203+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:14:41.203+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO AppInfoParser: Kafka startTimeMs: 1752855281199
[2025-07-18T16:14:41.397+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/sources/0/0 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/sources/0/.0.a6df65b4-478b-4eb4-814e-e1bb270ec883.tmp
[2025-07-18T16:14:41.397+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/sources/0/0 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/sources/0/.0.e4aba4f5-0494-4300-baf9-8ab452247421.tmp
[2025-07-18T16:14:41.398+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/sources/0/0 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/sources/0/.0.6e7fd539-f1a4-4db9-9251-758f4e99a312.tmp
[2025-07-18T16:14:41.419+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/sources/0/.0.e4aba4f5-0494-4300-baf9-8ab452247421.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/sources/0/0
[2025-07-18T16:14:41.419+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/sources/0/.0.a6df65b4-478b-4eb4-814e-e1bb270ec883.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/sources/0/0
[2025-07-18T16:14:41.420+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaMicroBatchStream: Initial offsets: {"checkins":{"0":0}}
[2025-07-18T16:14:41.420+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaMicroBatchStream: Initial offsets: {"feedback":{"0":0}}
[2025-07-18T16:14:41.421+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/sources/0/.0.6e7fd539-f1a4-4db9-9251-758f4e99a312.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/sources/0/0
[2025-07-18T16:14:41.422+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaMicroBatchStream: Initial offsets: {"reservations":{"0":0}}
[2025-07-18T16:14:41.436+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/0 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.0.394b28df-a6a0-43c5-bd18-14aa928576ab.tmp
[2025-07-18T16:14:41.436+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/0 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.0.ba80c1ec-7110-4826-b57b-1abaebe89259.tmp
[2025-07-18T16:14:41.436+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/0 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.0.10e5a2bf-9b01-4047-9d2e-d82de2c4825e.tmp
[2025-07-18T16:14:41.462+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.0.ba80c1ec-7110-4826-b57b-1abaebe89259.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/0
[2025-07-18T16:14:41.463+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.0.10e5a2bf-9b01-4047-9d2e-d82de2c4825e.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/0
[2025-07-18T16:14:41.463+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855281426,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:14:41.463+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855281426,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:14:41.465+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.0.394b28df-a6a0-43c5-bd18-14aa928576ab.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/0
[2025-07-18T16:14:41.466+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855281426,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:14:41.623+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.624+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.624+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.624+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.624+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.624+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.626+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.626+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.626+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.748+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.749+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.750+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.789+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.789+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.790+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.817+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.818+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.819+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.820+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.820+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.821+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.821+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.822+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.823+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.823+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.823+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.824+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.824+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.825+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.825+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.862+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:41.863+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:41.864+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.865+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.871+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.871+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.872+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:41.872+0000] {subprocess.py:93} INFO - 25/07/18 16:14:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:14:42.077+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 121.328541 ms
[2025-07-18T16:14:42.079+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 121.315208 ms
[2025-07-18T16:14:42.080+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 121.012666 ms
[2025-07-18T16:14:42.179+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:14:42.179+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:14:42.180+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:14:42.223+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:14:42.224+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:14:42.224+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:14:42.225+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:14:42.225+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:14:42.230+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:14:42.231+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Created broadcast 2 from start at <unknown>:0
[2025-07-18T16:14:42.232+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Created broadcast 1 from start at <unknown>:0
[2025-07-18T16:14:42.232+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Created broadcast 0 from start at <unknown>:0
[2025-07-18T16:14:42.232+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:14:42.232+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:14:42.232+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:14:42.270+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:14:42.271+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:14:42.272+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:14:42.282+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:14:42.283+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-07-18T16:14:42.283+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:14:42.283+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:14:42.286+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:14:42.337+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T16:14:42.345+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T16:14:42.348+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:14:42.349+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:14:42.375+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:14:42.400+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-18T16:14:42.411+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:14:42.414+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-07-18T16:14:42.415+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:14:42.415+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:14:42.416+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:14:42.420+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:14:42.430+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:14:42.431+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:14:42.432+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:14:42.434+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:14:42.436+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-18T16:14:42.460+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:14:42.466+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:14:42.466+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
[2025-07-18T16:14:42.467+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:14:42.467+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:14:42.467+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:14:42.469+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T16:14:42.476+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:14:42.477+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-18T16:14:42.481+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:14:42.481+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:14:42.481+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:14:42.481+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:14:42.482+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-18T16:14:42.482+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-18T16:14:42.482+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:14:42.482+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-18T16:14:42.592+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 30.759959 ms
[2025-07-18T16:14:42.593+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 33.178833 ms
[2025-07-18T16:14:42.598+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 39.505125 ms
[2025-07-18T16:14:42.607+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 16.049292 ms
[2025-07-18T16:14:42.611+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 20.206709 ms
[2025-07-18T16:14:42.613+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 16.247209 ms
[2025-07-18T16:14:42.667+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:14:42.667+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:14:42.668+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:14:42.880+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=0 untilOffset=123, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=0 taskId=2 partitionId=0
[2025-07-18T16:14:42.880+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=0 untilOffset=123, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=0 taskId=1 partitionId=0
[2025-07-18T16:14:42.886+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=0 untilOffset=123, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=0 taskId=0 partitionId=0
[2025-07-18T16:14:42.909+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 8.185291 ms
[2025-07-18T16:14:42.932+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO CodeGenerator: Code generated in 12.205166 ms
[2025-07-18T16:14:42.949+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:14:42.950+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:14:42.950+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:14:42.950+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:14:42.950+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:14:42.950+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:14:42.950+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:14:42.951+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:14:42.951+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1
[2025-07-18T16:14:42.951+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:14:42.951+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:14:42.952+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:14:42.952+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:14:42.952+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:14:42.952+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:14:42.952+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:14:42.952+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:14:42.953+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor
[2025-07-18T16:14:42.953+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:14:42.953+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:14:42.953+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:14:42.953+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:14:42.953+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:14:42.953+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:14:42.954+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:14:42.954+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:14:42.954+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:14:42.954+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:14:42.955+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:14:42.955+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:14:42.955+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:14:42.955+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:14:42.956+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:14:42.956+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:14:42.956+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:14:42.957+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:14:42.957+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:14:42.957+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:14:42.957+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:14:42.957+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:14:42.957+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:14:42.958+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:14:42.958+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:14:42.958+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:14:42.958+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:14:42.959+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:14:42.959+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:14:42.959+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:14:42.960+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:14:42.960+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:14:42.960+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:14:42.961+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:14:42.962+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:14:42.962+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:14:42.963+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:14:42.963+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:14:42.963+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:14:42.964+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:14:42.964+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:14:42.965+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:14:42.965+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:14:42.965+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:14:42.966+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:14:42.966+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:14:42.966+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:14:42.966+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:14:42.966+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:14:42.966+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:14:42.966+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:14:42.967+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:14:42.968+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:14:42.969+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:14:42.970+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:14:42.971+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:14:42.972+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:14:42.973+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:14:42.973+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:14:42.973+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:14:42.974+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:14:42.975+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:14:42.975+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:14:42.975+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:14:42.975+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:14:42.976+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:14:42.976+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:14:42.976+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:14:42.976+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:14:42.976+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:14:42.976+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:14:42.977+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:14:42.977+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:14:42.977+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:14:42.977+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:14:42.978+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:14:42.978+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:14:42.978+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:14:42.978+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:14:42.978+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:14:42.978+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:14:42.978+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:14:42.979+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:14:42.979+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:14:42.979+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:14:42.979+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:14:42.979+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:14:42.979+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:14:42.979+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:14:42.980+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:14:42.980+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:14:42.980+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:14:42.981+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:14:42.982+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:14:42.983+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:14:42.984+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:14:42.985+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:14:42.985+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:14:42.985+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:14:42.985+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:14:42.985+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:14:42.985+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:14:42.985+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor
[2025-07-18T16:14:42.986+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:14:42.986+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:14:42.986+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:14:42.986+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:14:42.987+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:14:42.988+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:14:42.988+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:14:42.988+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:14:42.988+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:14:42.989+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:14:42.989+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:14:42.989+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:14:42.989+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:14:42.989+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:14:42.989+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:14:42.990+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:14:42.991+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:14:42.992+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:14:42.992+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:14:42.992+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:14:42.992+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:14:42.992+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:14:42.992+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:14:42.993+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:14:42.993+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:14:42.993+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:14:42.993+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:14:42.993+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:14:42.993+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:14:42.994+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:14:42.994+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:14:42.994+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:14:42.994+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:14:42.995+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:14:42.995+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:14:42.995+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:14:42.995+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:14:42.995+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:14:42.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:14:42.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:14:42.997+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:14:42.997+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:14:42.997+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:14:42.998+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:14:42.998+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:14:42.999+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:14:42.999+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:14:42.999+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:14:42.999+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:14:43.000+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:14:43.000+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:14:43.000+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:14:43.000+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:14:43.001+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:14:43.001+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:14:43.001+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka startTimeMs: 1752855282986
[2025-07-18T16:14:43.001+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:14:43.001+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Assigned to partition(s): checkins-0
[2025-07-18T16:14:43.001+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka startTimeMs: 1752855282986
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Assigned to partition(s): reservations-0
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO AppInfoParser: Kafka startTimeMs: 1752855282986
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Assigned to partition(s): feedback-0
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 0 for partition feedback-0
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 0 for partition checkins-0
[2025-07-18T16:14:43.002+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to offset 0 for partition reservations-0
[2025-07-18T16:14:43.003+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:14:43.003+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:14:43.003+0000] {subprocess.py:93} INFO - 25/07/18 16:14:42 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:14:43.030+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:14:43.031+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:14:43.031+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:14:43.534+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:14:43.535+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:14:43.536+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:14:43.536+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:14:43.536+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:14:43.536+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:14:43.537+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=123, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:14:43.537+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=123, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:14:43.537+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=123, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:14:43.636+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:14:43.637+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:14:43.637+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:14:43.972+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:14:43.974+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:14:43.978+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:14:43.980+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 123 records through 1 polls (polled  out 123 records), taking 541062209 nanos, during time span of 979093292 nanos.
[2025-07-18T16:14:43.981+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 123 records through 1 polls (polled  out 123 records), taking 540664583 nanos, during time span of 977939542 nanos.
[2025-07-18T16:14:43.982+0000] {subprocess.py:93} INFO - 25/07/18 16:14:43 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 123 records through 1 polls (polled  out 123 records), taking 541123375 nanos, during time span of 981172792 nanos.
[2025-07-18T16:14:44.033+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4934 bytes result sent to driver
[2025-07-18T16:14:44.035+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4869 bytes result sent to driver
[2025-07-18T16:14:44.036+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4795 bytes result sent to driver
[2025-07-18T16:14:44.077+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1635 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:14:44.084+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-18T16:14:44.085+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1599 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:14:44.086+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-18T16:14:44.088+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1611 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:14:44.092+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-18T16:14:44.098+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.794 s
[2025-07-18T16:14:44.100+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:14:44.104+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-18T16:14:44.106+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 1.629 s
[2025-07-18T16:14:44.107+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:14:44.108+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-18T16:14:44.109+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.685 s
[2025-07-18T16:14:44.109+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:14:44.110+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 1.831221 s
[2025-07-18T16:14:44.110+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.831334 s
[2025-07-18T16:14:44.110+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:14:44.112+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committing epoch 0 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:14:44.112+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-18T16:14:44.112+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.837898 s
[2025-07-18T16:14:44.113+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:14:44.114+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committing epoch 0 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:14:44.117+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:14:44.117+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committing epoch 0 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:14:44.175+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:14:44.177+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:14:44.177+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:14:44.680+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v72.metadata.json
[2025-07-18T16:14:44.680+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v74.metadata.json
[2025-07-18T16:14:44.693+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v64.metadata.json
[2025-07-18T16:14:44.808+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SnapshotProducer: Committed snapshot 3549018319311371759 (FastAppend)
[2025-07-18T16:14:44.810+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SnapshotProducer: Committed snapshot 3154941522553896373 (FastAppend)
[2025-07-18T16:14:44.812+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SnapshotProducer: Committed snapshot 1607759047209332213 (FastAppend)
[2025-07-18T16:14:44.854+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=3549018319311371759, sequenceNumber=73, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.658032S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=73}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=123}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=462}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=8879}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=229426}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:14:44.855+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committed in 675 ms
[2025-07-18T16:14:44.855+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=3154941522553896373, sequenceNumber=71, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.665644001S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=71}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=123}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=462}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=7414}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=220713}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:14:44.856+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committed in 675 ms
[2025-07-18T16:14:44.856+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=1607759047209332213, sequenceNumber=63, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.669587876S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=63}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=123}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=462}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=5841}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=197076}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:14:44.857+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO SparkWrite: Committed in 677 ms
[2025-07-18T16:14:44.857+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:14:44.858+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:14:44.858+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:14:44.867+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/0 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.0.d495188c-6e43-4676-9355-7bfc813c79c6.tmp
[2025-07-18T16:14:44.868+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/0 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.0.186cd1fd-fcc9-41b7-bf1a-708afcb0c6c8.tmp
[2025-07-18T16:14:44.873+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/0 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.0.041362b6-d91a-407c-9668-86879e92ba07.tmp
[2025-07-18T16:14:44.896+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.0.186cd1fd-fcc9-41b7-bf1a-708afcb0c6c8.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/0
[2025-07-18T16:14:44.896+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.0.d495188c-6e43-4676-9355-7bfc813c79c6.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/0
[2025-07-18T16:14:44.896+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.0.041362b6-d91a-407c-9668-86879e92ba07.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/0
[2025-07-18T16:14:44.919+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:14:44.920+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:14:44.920+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:14:44.920+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:14:44.921+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:14:41.016Z",
[2025-07-18T16:14:44.921+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:14:44.921+0000] {subprocess.py:93} INFO -   "numInputRows" : 123,
[2025-07-18T16:14:44.922+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:14:44.922+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 31.7092034029389,
[2025-07-18T16:14:44.922+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:14:44.922+0000] {subprocess.py:93} INFO -     "addBatch" : 3054,
[2025-07-18T16:14:44.923+0000] {subprocess.py:93} INFO -     "commitOffsets" : 38,
[2025-07-18T16:14:44.923+0000] {subprocess.py:93} INFO -     "getBatch" : 10,
[2025-07-18T16:14:44.924+0000] {subprocess.py:93} INFO -     "latestOffset" : 409,
[2025-07-18T16:14:44.924+0000] {subprocess.py:93} INFO -     "queryPlanning" : 323,
[2025-07-18T16:14:44.924+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3879,
[2025-07-18T16:14:44.925+0000] {subprocess.py:93} INFO -     "walCommit" : 37
[2025-07-18T16:14:44.925+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:14:44.925+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:14:44.925+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:14:44.925+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:14:44.925+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:14:44.926+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:14:44.926+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:14:44.926+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:14:44.926+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:14:44.927+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:14:44.927+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:14:44.927+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:14:44.927+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:14:44.927+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:14:44.927+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -     "numInputRows" : 123,
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 31.7092034029389,
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:14:44.928+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO -     "numOutputRows" : 123
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:14:44.929+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:14:44.930+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:14:44.930+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:14:41.137Z",
[2025-07-18T16:14:44.930+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:14:44.930+0000] {subprocess.py:93} INFO -   "numInputRows" : 123,
[2025-07-18T16:14:44.930+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:14:44.930+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 32.73017562533262,
[2025-07-18T16:14:44.930+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "addBatch" : 3054,
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "commitOffsets" : 38,
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "getBatch" : 9,
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "latestOffset" : 287,
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "queryPlanning" : 323,
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3758,
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "walCommit" : 35
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:14:44.931+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:14:44.932+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:14:44.932+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:14:44.932+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:14:44.932+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:14:44.932+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:14:44.932+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:14:44.932+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -     "numInputRows" : 123,
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 32.73017562533262,
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:14:44.933+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO -     "numOutputRows" : 123
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:14:44.934+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:14:44.937+0000] {subprocess.py:93} INFO - 25/07/18 16:14:44 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:14:44.938+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:14:44.940+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:14:44.942+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:14:44.945+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:14:40.920Z",
[2025-07-18T16:14:44.947+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:14:44.948+0000] {subprocess.py:93} INFO -   "numInputRows" : 123,
[2025-07-18T16:14:44.949+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:14:44.949+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 30.943396226415093,
[2025-07-18T16:14:44.950+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:14:44.955+0000] {subprocess.py:93} INFO -     "addBatch" : 3053,
[2025-07-18T16:14:44.956+0000] {subprocess.py:93} INFO -     "commitOffsets" : 38,
[2025-07-18T16:14:44.956+0000] {subprocess.py:93} INFO -     "getBatch" : 12,
[2025-07-18T16:14:44.956+0000] {subprocess.py:93} INFO -     "latestOffset" : 492,
[2025-07-18T16:14:44.956+0000] {subprocess.py:93} INFO -     "queryPlanning" : 324,
[2025-07-18T16:14:44.957+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3975,
[2025-07-18T16:14:44.957+0000] {subprocess.py:93} INFO -     "walCommit" : 35
[2025-07-18T16:14:44.957+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:14:44.957+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:14:44.957+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:14:44.957+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:14:44.957+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:14:44.958+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:14:44.958+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:14:44.958+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:14:44.958+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:14:44.958+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:14:44.961+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:14:44.961+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:14:44.961+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:14:44.962+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:14:44.962+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:14:44.962+0000] {subprocess.py:93} INFO -     "numInputRows" : 123,
[2025-07-18T16:14:44.963+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:14:44.963+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 30.943396226415093,
[2025-07-18T16:14:44.963+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:14:44.963+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:14:44.964+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:14:44.964+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:14:44.964+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:14:44.964+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:14:44.964+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:14:44.964+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:14:44.965+0000] {subprocess.py:93} INFO -     "numOutputRows" : 123
[2025-07-18T16:14:44.965+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:14:44.966+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:14:46.977+0000] {subprocess.py:93} INFO - 25/07/18 16:14:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:14:46.984+0000] {subprocess.py:93} INFO - 25/07/18 16:14:46 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:14:46.988+0000] {subprocess.py:93} INFO - 25/07/18 16:14:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:14:46.991+0000] {subprocess.py:93} INFO - 25/07/18 16:14:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:14:46.993+0000] {subprocess.py:93} INFO - 25/07/18 16:14:46 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:14:46.997+0000] {subprocess.py:93} INFO - 25/07/18 16:14:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.4 MiB)
[2025-07-18T16:14:54.914+0000] {subprocess.py:93} INFO - 25/07/18 16:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:14:54.915+0000] {subprocess.py:93} INFO - 25/07/18 16:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:14:54.915+0000] {subprocess.py:93} INFO - 25/07/18 16:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:04.918+0000] {subprocess.py:93} INFO - 25/07/18 16:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:04.920+0000] {subprocess.py:93} INFO - 25/07/18 16:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:04.920+0000] {subprocess.py:93} INFO - 25/07/18 16:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:14.921+0000] {subprocess.py:93} INFO - 25/07/18 16:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:14.923+0000] {subprocess.py:93} INFO - 25/07/18 16:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:14.923+0000] {subprocess.py:93} INFO - 25/07/18 16:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:24.926+0000] {subprocess.py:93} INFO - 25/07/18 16:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:24.928+0000] {subprocess.py:93} INFO - 25/07/18 16:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:24.928+0000] {subprocess.py:93} INFO - 25/07/18 16:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:34.926+0000] {subprocess.py:93} INFO - 25/07/18 16:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:34.927+0000] {subprocess.py:93} INFO - 25/07/18 16:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:34.930+0000] {subprocess.py:93} INFO - 25/07/18 16:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:44.935+0000] {subprocess.py:93} INFO - 25/07/18 16:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:44.936+0000] {subprocess.py:93} INFO - 25/07/18 16:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:44.937+0000] {subprocess.py:93} INFO - 25/07/18 16:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:54.939+0000] {subprocess.py:93} INFO - 25/07/18 16:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:54.940+0000] {subprocess.py:93} INFO - 25/07/18 16:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:15:54.940+0000] {subprocess.py:93} INFO - 25/07/18 16:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:02.774+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/1 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.1.760de586-010d-4283-b2d5-9aae01b2671d.tmp
[2025-07-18T16:16:02.814+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.1.760de586-010d-4283-b2d5-9aae01b2671d.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/1
[2025-07-18T16:16:02.814+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855362752,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:02.862+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.863+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.863+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.864+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:02.865+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:02.878+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.880+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.881+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.884+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:02.885+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:02.897+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.900+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.900+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:02.902+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:02.906+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:02.939+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:16:02.945+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:16:02.947+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:02.948+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkContext: Created broadcast 6 from start at <unknown>:0
[2025-07-18T16:16:02.949+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:02.949+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:02.951+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:02.954+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO DAGScheduler: Final stage: ResultStage 3 (start at <unknown>:0)
[2025-07-18T16:16:02.954+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:02.955+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:02.957+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:02.958+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:16:02.964+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:16:02.965+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:02.966+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:02.966+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:02.967+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-07-18T16:16:02.968+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:16:02.969+0000] {subprocess.py:93} INFO - 25/07/18 16:16:02 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-07-18T16:16:03.015+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:03.018+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=123 untilOffset=124, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=1 taskId=3 partitionId=0
[2025-07-18T16:16:03.028+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to offset 123 for partition reservations-0
[2025-07-18T16:16:03.035+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:16:03.157+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:03.157+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:16:03.158+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:03.158+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:16:03.184+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:16:03.185+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 1 records through 1 polls (polled  out 2 records), taking 124511000 nanos, during time span of 157389750 nanos.
[2025-07-18T16:16:03.185+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4727 bytes result sent to driver
[2025-07-18T16:16:03.188+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 221 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:03.188+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-07-18T16:16:03.188+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: ResultStage 3 (start at <unknown>:0) finished in 0.235 s
[2025-07-18T16:16:03.189+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:03.189+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-07-18T16:16:03.189+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.239536 s
[2025-07-18T16:16:03.189+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:03.189+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Committing epoch 1 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:16:03.211+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.238+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:03.351+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v65.metadata.json
[2025-07-18T16:16:03.372+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/1 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.1.2c0d3e81-cbc7-4b4e-aee4-8b8803e71bc8.tmp
[2025-07-18T16:16:03.419+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.1.2c0d3e81-cbc7-4b4e-aee4-8b8803e71bc8.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/1
[2025-07-18T16:16:03.419+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855363357,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:03.430+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SnapshotProducer: Committed snapshot 479541063519893066 (FastAppend)
[2025-07-18T16:16:03.438+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.439+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.440+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.443+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.447+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.493+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.494+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.498+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.499+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.506+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.507+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 WARN SnapshotProducer: Failed to load committed snapshot, skipping manifest clean-up
[2025-07-18T16:16:03.510+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 WARN SnapshotProducer: Failed to notify listeners
[2025-07-18T16:16:03.514+0000] {subprocess.py:93} INFO - java.lang.NullPointerException: Cannot invoke "org.apache.iceberg.Snapshot.sequenceNumber()" because "snapshot" is null
[2025-07-18T16:16:03.516+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.FastAppend.updateEvent(FastAppend.java:174)
[2025-07-18T16:16:03.519+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.notifyListeners(SnapshotProducer.java:449)
[2025-07-18T16:16:03.521+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:441)
[2025-07-18T16:16:03.523+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:16:03.524+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:16:03.524+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:16:03.524+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:16:03.524+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:16:03.525+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:16:03.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:16:03.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:16:03.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:16:03.528+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:16:03.529+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:16:03.530+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:16:03.531+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:16:03.533+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:16:03.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:16:03.538+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:16:03.539+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:16:03.540+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:16:03.542+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:03.542+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:03.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:03.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:03.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:03.543+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:16:03.544+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:16:03.544+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:16:03.545+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:03.546+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:03.547+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:03.547+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:03.549+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:03.550+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:16:03.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:03.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:03.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:03.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:16:03.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:16:03.552+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:03.552+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:03.552+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:03.553+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:03.554+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:16:03.555+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:16:03.555+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:16:03.556+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:16:03.556+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:03.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:03.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:16:03.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:16:03.559+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:03.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:16:03.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:16:03.560+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Committed in 294 ms
[2025-07-18T16:16:03.561+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:16:03.563+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.565+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.566+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.570+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/1 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.1.d934a258-c989-4e86-8632-8f332b951446.tmp
[2025-07-18T16:16:03.572+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.574+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.575+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:16:03.576+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.3 MiB)
[2025-07-18T16:16:03.577+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:16:03.579+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkContext: Created broadcast 8 from start at <unknown>:0
[2025-07-18T16:16:03.580+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:03.582+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:03.585+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:03.585+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
[2025-07-18T16:16:03.587+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:03.588+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:03.589+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:03.589+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:03.589+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 28.0 KiB, free 434.3 MiB)
[2025-07-18T16:16:03.590+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.3 MiB)
[2025-07-18T16:16:03.600+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.4 MiB)
[2025-07-18T16:16:03.601+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:03.603+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:03.604+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-18T16:16:03.607+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:16:03.608+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-07-18T16:16:03.647+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.1.d934a258-c989-4e86-8632-8f332b951446.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/1
[2025-07-18T16:16:03.651+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:03.653+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:16:03.654+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:16:03.655+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:03.656+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:02.750Z",
[2025-07-18T16:16:03.656+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:16:03.657+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:16:03.657+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:16:03.657+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.1185682326621924,
[2025-07-18T16:16:03.657+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:03.657+0000] {subprocess.py:93} INFO -     "addBatch" : 638,
[2025-07-18T16:16:03.657+0000] {subprocess.py:93} INFO -     "commitOffsets" : 139,
[2025-07-18T16:16:03.657+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:16:03.659+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:16:03.659+0000] {subprocess.py:93} INFO -     "queryPlanning" : 49,
[2025-07-18T16:16:03.659+0000] {subprocess.py:93} INFO -     "triggerExecution" : 894,
[2025-07-18T16:16:03.659+0000] {subprocess.py:93} INFO -     "walCommit" : 65
[2025-07-18T16:16:03.659+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:03.659+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:03.660+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:03.661+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:16:03.663+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:16:03.666+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:03.666+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:16:03.667+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:03.667+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:03.667+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:03.668+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:03.668+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:03.669+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:03.670+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:03.671+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:03.672+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:03.673+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:03.674+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:03.676+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:03.677+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:16:03.680+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:16:03.682+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.1185682326621924,
[2025-07-18T16:16:03.685+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:03.687+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:03.687+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:03.688+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:03.689+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:03.691+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:03.692+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:03.694+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:16:03.696+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:16:03.697+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:03.698+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:03.698+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:03.698+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=123 untilOffset=124, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=1 taskId=4 partitionId=0
[2025-07-18T16:16:03.699+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/2 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.2.34c1164d-f907-4017-9cff-ec759ccdcd04.tmp
[2025-07-18T16:16:03.701+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 123 for partition checkins-0
[2025-07-18T16:16:03.706+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:16:03.745+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.2.34c1164d-f907-4017-9cff-ec759ccdcd04.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/2
[2025-07-18T16:16:03.750+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855363655,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:03.756+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.759+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.760+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.760+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:16:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:03.766+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:16:03.773+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.773+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.774+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.777+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.778+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.788+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.789+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.789+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:03.791+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:16:03.792+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 1 records through 1 polls (polled  out 2 records), taking 66460458 nanos, during time span of 101994125 nanos.
[2025-07-18T16:16:03.798+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.799+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4864 bytes result sent to driver
[2025-07-18T16:16:03.800+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 201 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:03.801+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-18T16:16:03.803+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:03.807+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.231 s
[2025-07-18T16:16:03.809+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:03.809+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-18T16:16:03.814+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.238593 s
[2025-07-18T16:16:03.822+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:03.825+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Committing epoch 1 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:16:03.828+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:16:03.841+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:16:03.841+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:03.842+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkContext: Created broadcast 10 from start at <unknown>:0
[2025-07-18T16:16:03.844+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:03.845+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:03.846+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:03.846+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Final stage: ResultStage 5 (start at <unknown>:0)
[2025-07-18T16:16:03.847+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:03.847+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:03.847+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:03.847+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:03.852+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T16:16:03.858+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T16:16:03.860+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:03.860+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:03.861+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:03.861+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-07-18T16:16:03.861+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:16:03.862+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-07-18T16:16:03.868+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:03.870+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=124 untilOffset=126, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=2 taskId=5 partitionId=0
[2025-07-18T16:16:03.880+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to offset 125 for partition reservations-0
[2025-07-18T16:16:03.881+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:16:03.979+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/1 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.1.33533e8f-4c21-4e04-9b72-83ec58dd28fd.tmp
[2025-07-18T16:16:03.994+0000] {subprocess.py:93} INFO - 25/07/18 16:16:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v74.metadata.json
[2025-07-18T16:16:04.010+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.1.33533e8f-4c21-4e04-9b72-83ec58dd28fd.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/1
[2025-07-18T16:16:04.011+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855363968,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:04.027+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.029+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.031+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.038+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.042+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.054+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SnapshotProducer: Committed snapshot 4262906092616583668 (FastAppend)
[2025-07-18T16:16:04.058+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.059+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.060+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.064+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.073+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.079+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.080+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.082+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.085+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.086+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.098+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:16:04.099+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:16:04.100+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:04.101+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkContext: Created broadcast 12 from start at <unknown>:0
[2025-07-18T16:16:04.101+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:04.102+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4262906092616583668, sequenceNumber=73, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.257054625S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=73}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=464}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2903}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=226519}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:04.102+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Committed in 257 ms
[2025-07-18T16:16:04.103+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:16:04.106+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:04.106+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:04.107+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
[2025-07-18T16:16:04.107+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:04.108+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:04.108+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:04.109+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:16:04.109+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:16:04.109+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:16:04.109+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:04.109+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:04.109+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-18T16:16:04.110+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/1 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.1.74ac8db0-b25d-4e36-b333-468f0f7d9928.tmp
[2025-07-18T16:16:04.115+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:16:04.116+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-07-18T16:16:04.125+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:04.125+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=123 untilOffset=124, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=1 taskId=6 partitionId=0
[2025-07-18T16:16:04.126+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 123 for partition feedback-0
[2025-07-18T16:16:04.128+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:16:04.132+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.1.74ac8db0-b25d-4e36-b333-468f0f7d9928.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/1
[2025-07-18T16:16:04.133+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:04.133+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:16:04.133+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:16:04.133+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:04.133+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:03.354Z",
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.287001287001287,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -     "addBatch" : 651,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -     "commitOffsets" : 31,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -     "queryPlanning" : 31,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -     "triggerExecution" : 777,
[2025-07-18T16:16:04.134+0000] {subprocess.py:93} INFO -     "walCommit" : 59
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:16:04.135+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:04.136+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.287001287001287,
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:04.137+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:04.138+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:04.138+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:04.138+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:16:04.138+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:16:04.138+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:04.138+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:04.139+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/2 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.2.62fc8e8b-cebc-4216-a7d2-033be95bd5be.tmp
[2025-07-18T16:16:04.147+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:16:04.150+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:16:04.157+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.2.62fc8e8b-cebc-4216-a7d2-033be95bd5be.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/2
[2025-07-18T16:16:04.158+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855364134,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:04.166+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:04.166+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:16:04.167+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.167+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.168+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.168+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=125, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:04.168+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:16:04.169+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.169+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.178+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.178+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.178+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.179+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.180+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.187+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.187+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.188+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:04.189+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:16:04.191+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 1 records through 1 polls (polled  out 1 records), taking 38212667 nanos, during time span of 60694917 nanos.
[2025-07-18T16:16:04.191+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4822 bytes result sent to driver
[2025-07-18T16:16:04.192+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.193+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 77 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:04.194+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-18T16:16:04.194+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.086 s
[2025-07-18T16:16:04.194+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:04.195+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-07-18T16:16:04.196+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.088271 s
[2025-07-18T16:16:04.197+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:04.197+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Committing epoch 1 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:16:04.197+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:04.217+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:04.221+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:16:04.233+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:16:04.235+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:04.238+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkContext: Created broadcast 14 from start at <unknown>:0
[2025-07-18T16:16:04.239+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:04.240+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:04.241+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:04.244+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
[2025-07-18T16:16:04.246+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:04.246+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:04.250+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:04.253+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T16:16:04.258+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:16:04.261+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:16:04.262+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:04.263+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:04.263+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-18T16:16:04.267+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:16:04.268+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-07-18T16:16:04.268+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:04.269+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=124 untilOffset=126, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=2 taskId=7 partitionId=0
[2025-07-18T16:16:04.269+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 125 for partition checkins-0
[2025-07-18T16:16:04.270+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:16:04.369+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:16:04.386+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:04.387+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:16:04.388+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:04.389+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:16:04.468+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:16:04.470+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 2 records through 1 polls (polled  out 1 records), taking 507743834 nanos, during time span of 592725625 nanos.
[2025-07-18T16:16:04.481+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4784 bytes result sent to driver
[2025-07-18T16:16:04.529+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 666 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:04.530+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-07-18T16:16:04.530+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: ResultStage 5 (start at <unknown>:0) finished in 0.677 s
[2025-07-18T16:16:04.530+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:04.530+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-07-18T16:16:04.530+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.687747 s
[2025-07-18T16:16:04.531+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:04.531+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Committing epoch 2 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:16:04.576+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 WARN Tasks: Retrying task after failure: Failed to commit changes using rename: s3a://warehouse/bronze/Feedback_raw/metadata/v75.metadata.json
[2025-07-18T16:16:04.578+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Failed to commit changes using rename: s3a://warehouse/bronze/Feedback_raw/metadata/v75.metadata.json
[2025-07-18T16:16:04.579+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:378)
[2025-07-18T16:16:04.580+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:16:04.581+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:16:04.583+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:16:04.584+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:16:04.584+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:16:04.585+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:16:04.585+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:16:04.586+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:16:04.587+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:16:04.591+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:16:04.592+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:16:04.592+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:16:04.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:16:04.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:16:04.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:16:04.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:16:04.592+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:16:04.593+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:04.594+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:04.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:04.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:04.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:16:04.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:04.595+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:04.597+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:04.598+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:16:04.599+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:16:04.600+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:04.600+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:04.601+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:04.601+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:04.601+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:16:04.602+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:16:04.604+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:16:04.604+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:16:04.605+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:04.605+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:04.605+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:16:04.605+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:16:04.606+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:04.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:16:04.607+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:16:04.607+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Failed to rename s3a://warehouse/bronze/Feedback_raw/metadata/cfb770c1-59a7-44fb-8fe5-26afc3fdb215.metadata.json to s3a://warehouse/bronze/Feedback_raw/metadata/v75.metadata.json; destination file exists
[2025-07-18T16:16:04.607+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initiateRename(S3AFileSystem.java:1920)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerRename(S3AFileSystem.java:1988)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$rename$7(S3AFileSystem.java:1846)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:1844)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:368)
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 	... 59 more
[2025-07-18T16:16:04.608+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:16:04.773+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:04.774+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:16:04.780+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:04.782+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:16:04.845+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:16:04.846+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 2 records through 1 polls (polled  out 1 records), taking 505724834 nanos, during time span of 573863375 nanos.
[2025-07-18T16:16:04.858+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4817 bytes result sent to driver
[2025-07-18T16:16:04.868+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 615 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:04.868+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-18T16:16:04.879+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 WARN Tasks: Retrying task after failure: Version 66 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v66.metadata.json
[2025-07-18T16:16:04.881+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 66 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v66.metadata.json
[2025-07-18T16:16:04.883+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:16:04.884+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:16:04.885+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:16:04.886+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:16:04.890+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:16:04.891+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:16:04.891+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:16:04.891+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:16:04.892+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:16:04.892+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:16:04.892+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:16:04.894+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:16:04.895+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:16:04.897+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:16:04.900+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:16:04.900+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:16:04.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:16:04.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:16:04.907+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:16:04.908+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:16:04.909+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:16:04.909+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:16:04.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:16:04.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:16:04.912+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:16:04.917+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:16:04.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:04.919+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:04.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:04.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:04.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:04.924+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:16:04.924+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:16:04.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:16:04.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:04.927+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:04.929+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:04.930+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:04.931+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:04.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:16:04.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:04.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:04.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:04.934+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:16:04.935+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:16:04.938+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:04.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:04.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:04.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:04.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:16:04.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:16:04.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:16:04.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:16:04.943+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:04.943+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:04.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:16:04.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:16:04.945+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:04.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:16:04.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:16:04.949+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.629 s
[2025-07-18T16:16:04.950+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:04.950+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-18T16:16:04.950+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 0.638221 s
[2025-07-18T16:16:04.951+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:04.951+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Committing epoch 2 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:16:04.951+0000] {subprocess.py:93} INFO - 25/07/18 16:16:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:16:05.025+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v76.metadata.json
[2025-07-18T16:16:05.132+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SnapshotProducer: Committed snapshot 8349965594316082349 (FastAppend)
[2025-07-18T16:16:05.234+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=8349965594316082349, sequenceNumber=75, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.019653459S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=75}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=464}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2897}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=235220}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:05.238+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Committed in 1020 ms
[2025-07-18T16:16:05.239+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:16:05.242+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 WARN Tasks: Retrying task after failure: Version 75 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v75.metadata.json
[2025-07-18T16:16:05.243+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 75 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v75.metadata.json
[2025-07-18T16:16:05.250+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:16:05.251+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:16:05.252+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:16:05.252+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:16:05.252+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:16:05.254+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:16:05.255+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:16:05.264+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:16:05.265+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:16:05.266+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:16:05.267+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:16:05.268+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:16:05.270+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:16:05.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:16:05.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:16:05.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:16:05.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:16:05.277+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:16:05.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:16:05.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:16:05.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:16:05.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:16:05.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:16:05.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:16:05.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:16:05.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:16:05.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:05.292+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:05.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:05.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:05.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:05.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:16:05.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:16:05.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:16:05.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:16:05.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:16:05.309+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:16:05.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:05.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:16:05.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:16:05.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:05.312+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:05.317+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:05.318+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:16:05.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:16:05.322+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:05.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:16:05.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:16:05.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:16:05.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:16:05.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:16:05.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:16:05.328+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:16:05.329+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:05.331+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:16:05.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:16:05.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:16:05.342+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:16:05.345+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:16:05.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:16:05.348+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/1 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.1.77471740-1349-4b4d-bdae-b41db3d74de2.tmp
[2025-07-18T16:16:05.349+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v67.metadata.json
[2025-07-18T16:16:05.402+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.1.77471740-1349-4b4d-bdae-b41db3d74de2.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/1
[2025-07-18T16:16:05.402+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:05.403+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:16:05.404+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:16:05.405+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:05.406+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:03.963Z",
[2025-07-18T16:16:05.406+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:16:05.407+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:16:05.407+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T16:16:05.407+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6968641114982578,
[2025-07-18T16:16:05.408+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:05.408+0000] {subprocess.py:93} INFO -     "addBatch" : 1188,
[2025-07-18T16:16:05.409+0000] {subprocess.py:93} INFO -     "commitOffsets" : 166,
[2025-07-18T16:16:05.409+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:16:05.409+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:16:05.410+0000] {subprocess.py:93} INFO -     "queryPlanning" : 38,
[2025-07-18T16:16:05.410+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1435,
[2025-07-18T16:16:05.410+0000] {subprocess.py:93} INFO -     "walCommit" : 37
[2025-07-18T16:16:05.411+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:05.411+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:05.411+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:05.414+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:16:05.414+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:16:05.416+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:05.417+0000] {subprocess.py:93} INFO -         "0" : 123
[2025-07-18T16:16:05.418+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.422+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.423+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:05.424+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:05.425+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:05.425+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.426+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.426+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:05.427+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:05.427+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:05.427+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.427+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.427+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:16:05.428+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T16:16:05.428+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6968641114982578,
[2025-07-18T16:16:05.429+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:05.429+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:05.430+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:05.431+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:05.431+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:05.432+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:05.433+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:05.433+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:16:05.434+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:16:05.434+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:05.435+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:05.436+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/2 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.2.07144bea-a25b-4d1b-96b4-c0d09e85ae01.tmp
[2025-07-18T16:16:05.456+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SnapshotProducer: Committed snapshot 2523538951336066496 (FastAppend)
[2025-07-18T16:16:05.458+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:16:05.473+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.2.07144bea-a25b-4d1b-96b4-c0d09e85ae01.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/2
[2025-07-18T16:16:05.474+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855365408,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:16:05.482+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.485+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.485+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.486+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:05.488+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:05.493+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.494+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.494+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.509+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:05.510+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:05.517+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=2523538951336066496, sequenceNumber=66, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.938152751S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=66}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=467}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3056}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=206101}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:05.517+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Committed in 938 ms
[2025-07-18T16:16:05.517+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:16:05.519+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.524+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.524+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:05.524+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:05.525+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/2 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.2.ad49601c-20e8-40f3-aa1f-bda9ac0034aa.tmp
[2025-07-18T16:16:05.526+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:16:05.542+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:16:05.549+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:16:05.551+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:05.554+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkContext: Created broadcast 16 from start at <unknown>:0
[2025-07-18T16:16:05.555+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:16:05.556+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:16:05.557+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:16:05.558+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
[2025-07-18T16:16:05.560+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:16:05.561+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:16:05.561+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v76.metadata.json
[2025-07-18T16:16:05.565+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:16:05.565+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:16:05.566+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:16:05.566+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:16:05.566+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:16:05.567+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:16:05.567+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-18T16:16:05.567+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:16:05.568+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-07-18T16:16:05.568+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.2.ad49601c-20e8-40f3-aa1f-bda9ac0034aa.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/2
[2025-07-18T16:16:05.568+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:05.569+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:16:05.571+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:16:05.571+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:05.572+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:03.650Z",
[2025-07-18T16:16:05.573+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:16:05.573+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:16:05.573+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.2222222222222223,
[2025-07-18T16:16:05.574+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.0460251046025104,
[2025-07-18T16:16:05.574+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:05.575+0000] {subprocess.py:93} INFO -     "addBatch" : 1752,
[2025-07-18T16:16:05.575+0000] {subprocess.py:93} INFO -     "commitOffsets" : 47,
[2025-07-18T16:16:05.575+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:16:05.576+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:16:05.578+0000] {subprocess.py:93} INFO -     "queryPlanning" : 18,
[2025-07-18T16:16:05.579+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1912,
[2025-07-18T16:16:05.579+0000] {subprocess.py:93} INFO -     "walCommit" : 88
[2025-07-18T16:16:05.579+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:05.580+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:05.580+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:05.581+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:16:05.581+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:16:05.582+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:05.582+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:05.582+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.583+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.583+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:05.583+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:05.584+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:05.584+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.585+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.585+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:05.586+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:16:05.586+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:05.587+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.588+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.588+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:16:05.588+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.2222222222222223,
[2025-07-18T16:16:05.589+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.0460251046025104,
[2025-07-18T16:16:05.589+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:05.589+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:05.590+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:05.590+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:05.590+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:05.590+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:05.591+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:05.592+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:16:05.592+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:16:05.593+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:05.593+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:05.593+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:16:05.593+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=124 untilOffset=126, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=2 taskId=8 partitionId=0
[2025-07-18T16:16:05.593+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 124 for partition feedback-0
[2025-07-18T16:16:05.594+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:16:05.594+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:05.594+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:16:05.596+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:05.596+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 125 for partition feedback-0
[2025-07-18T16:16:05.597+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:16:05.609+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:05.617+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:05.618+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SnapshotProducer: Committed snapshot 366877833217079921 (FastAppend)
[2025-07-18T16:16:05.622+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:16:05.653+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=366877833217079921, sequenceNumber=75, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.738063667S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=75}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=468}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2938}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=232395}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:05.653+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO SparkWrite: Committed in 738 ms
[2025-07-18T16:16:05.653+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:16:05.657+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/2 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.2.2bb3364a-0883-45fe-be06-5cfb0f2579fa.tmp
[2025-07-18T16:16:05.673+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.2.2bb3364a-0883-45fe-be06-5cfb0f2579fa.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/2
[2025-07-18T16:16:05.673+0000] {subprocess.py:93} INFO - 25/07/18 16:16:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:04.132Z",
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.570694087403599,
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.2987012987012987,
[2025-07-18T16:16:05.674+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:05.675+0000] {subprocess.py:93} INFO -     "addBatch" : 1483,
[2025-07-18T16:16:05.675+0000] {subprocess.py:93} INFO -     "commitOffsets" : 19,
[2025-07-18T16:16:05.675+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:16:05.675+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:16:05.675+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T16:16:05.675+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1540,
[2025-07-18T16:16:05.675+0000] {subprocess.py:93} INFO -     "walCommit" : 22
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:05.676+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.570694087403599,
[2025-07-18T16:16:05.677+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.2987012987012987,
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:16:05.678+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:16:05.679+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:05.679+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:06.082+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:06.082+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:16:06.085+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=126, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:16:06.088+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:16:06.115+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:16:06.115+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 2 records through 2 polls (polled  out 2 records), taking 510663792 nanos, during time span of 542276333 nanos.
[2025-07-18T16:16:06.116+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4759 bytes result sent to driver
[2025-07-18T16:16:06.117+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 558 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:16:06.118+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-18T16:16:06.119+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.567 s
[2025-07-18T16:16:06.120+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:16:06.120+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-18T16:16:06.121+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.569199 s
[2025-07-18T16:16:06.121+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:16:06.122+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO SparkWrite: Committing epoch 2 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:16:06.174+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:16:06.287+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v78.metadata.json
[2025-07-18T16:16:06.326+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO SnapshotProducer: Committed snapshot 8668870170769188592 (FastAppend)
[2025-07-18T16:16:06.360+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=8668870170769188592, sequenceNumber=77, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.190383542S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=77}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=468}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2876}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=240972}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:16:06.361+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO SparkWrite: Committed in 191 ms
[2025-07-18T16:16:06.361+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:16:06.365+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/2 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.2.73d871ce-98a5-4c30-ba45-9e0aa1f773d2.tmp
[2025-07-18T16:16:06.394+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.2.73d871ce-98a5-4c30-ba45-9e0aa1f773d2.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/2
[2025-07-18T16:16:06.398+0000] {subprocess.py:93} INFO - 25/07/18 16:16:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:16:06.398+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:16:06.398+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:16:06.399+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:16:06.399+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:16:05.401Z",
[2025-07-18T16:16:06.399+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:16:06.399+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:16:06.400+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 1.3908205841446455,
[2025-07-18T16:16:06.400+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.0181634712411705,
[2025-07-18T16:16:06.400+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:16:06.400+0000] {subprocess.py:93} INFO -     "addBatch" : 869,
[2025-07-18T16:16:06.400+0000] {subprocess.py:93} INFO -     "commitOffsets" : 34,
[2025-07-18T16:16:06.401+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:16:06.401+0000] {subprocess.py:93} INFO -     "latestOffset" : 7,
[2025-07-18T16:16:06.401+0000] {subprocess.py:93} INFO -     "queryPlanning" : 15,
[2025-07-18T16:16:06.401+0000] {subprocess.py:93} INFO -     "triggerExecution" : 991,
[2025-07-18T16:16:06.401+0000] {subprocess.py:93} INFO -     "walCommit" : 64
[2025-07-18T16:16:06.401+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:16:06.402+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:16:06.402+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:16:06.402+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:16:06.402+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:16:06.402+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:06.402+0000] {subprocess.py:93} INFO -         "0" : 124
[2025-07-18T16:16:06.402+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:16:06.403+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:16:06.404+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 1.3908205841446455,
[2025-07-18T16:16:06.404+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.0181634712411705,
[2025-07-18T16:16:06.404+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:16:06.404+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:16:06.404+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:16:06.404+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:16:06.404+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:16:06.405+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:16:06.405+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:16:06.405+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:16:06.405+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:16:06.405+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:16:06.405+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:16:09.350+0000] {subprocess.py:93} INFO - 25/07/18 16:16:09 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:16:09.358+0000] {subprocess.py:93} INFO - 25/07/18 16:16:09 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:15.567+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:15.684+0000] {subprocess.py:93} INFO - 25/07/18 16:16:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:16.396+0000] {subprocess.py:93} INFO - 25/07/18 16:16:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:16.430+0000] {subprocess.py:93} INFO - 25/07/18 16:16:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:16:25.575+0000] {subprocess.py:93} INFO - 25/07/18 16:16:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:25.687+0000] {subprocess.py:93} INFO - 25/07/18 16:16:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:26.409+0000] {subprocess.py:93} INFO - 25/07/18 16:16:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:35.576+0000] {subprocess.py:93} INFO - 25/07/18 16:16:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:35.692+0000] {subprocess.py:93} INFO - 25/07/18 16:16:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:36.418+0000] {subprocess.py:93} INFO - 25/07/18 16:16:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:45.581+0000] {subprocess.py:93} INFO - 25/07/18 16:16:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:45.693+0000] {subprocess.py:93} INFO - 25/07/18 16:16:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:46.419+0000] {subprocess.py:93} INFO - 25/07/18 16:16:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:55.586+0000] {subprocess.py:93} INFO - 25/07/18 16:16:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:55.698+0000] {subprocess.py:93} INFO - 25/07/18 16:16:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:16:56.427+0000] {subprocess.py:93} INFO - 25/07/18 16:16:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:05.591+0000] {subprocess.py:93} INFO - 25/07/18 16:17:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:05.700+0000] {subprocess.py:93} INFO - 25/07/18 16:17:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:06.433+0000] {subprocess.py:93} INFO - 25/07/18 16:17:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:15.599+0000] {subprocess.py:93} INFO - 25/07/18 16:17:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:15.710+0000] {subprocess.py:93} INFO - 25/07/18 16:17:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:16.446+0000] {subprocess.py:93} INFO - 25/07/18 16:17:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:25.607+0000] {subprocess.py:93} INFO - 25/07/18 16:17:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:25.715+0000] {subprocess.py:93} INFO - 25/07/18 16:17:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:26.450+0000] {subprocess.py:93} INFO - 25/07/18 16:17:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:35.617+0000] {subprocess.py:93} INFO - 25/07/18 16:17:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:35.718+0000] {subprocess.py:93} INFO - 25/07/18 16:17:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:36.449+0000] {subprocess.py:93} INFO - 25/07/18 16:17:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:45.631+0000] {subprocess.py:93} INFO - 25/07/18 16:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:45.721+0000] {subprocess.py:93} INFO - 25/07/18 16:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:46.461+0000] {subprocess.py:93} INFO - 25/07/18 16:17:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:55.639+0000] {subprocess.py:93} INFO - 25/07/18 16:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:55.730+0000] {subprocess.py:93} INFO - 25/07/18 16:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:17:56.471+0000] {subprocess.py:93} INFO - 25/07/18 16:17:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:02.179+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/3 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.3.69e58d5e-aafe-429b-8530-430d4f15975f.tmp
[2025-07-18T16:18:02.219+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.3.69e58d5e-aafe-429b-8530-430d4f15975f.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/3
[2025-07-18T16:18:02.220+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855482163,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:02.269+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.270+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.271+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.280+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.284+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.295+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.296+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.296+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.297+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.299+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.310+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.314+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.317+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.321+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.325+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.387+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:18:02.421+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:18:02.427+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:02.441+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkContext: Created broadcast 18 from start at <unknown>:0
[2025-07-18T16:18:02.442+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:02.444+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:02.449+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:02.450+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Final stage: ResultStage 9 (start at <unknown>:0)
[2025-07-18T16:18:02.450+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:02.450+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:02.451+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:02.459+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:18:02.468+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:18:02.469+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:02.469+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:02.470+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:02.470+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-18T16:18:02.470+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:18:02.473+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2025-07-18T16:18:02.524+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:02.529+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=126 untilOffset=127, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=3 taskId=9 partitionId=0
[2025-07-18T16:18:02.562+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to offset 126 for partition reservations-0
[2025-07-18T16:18:02.569+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:18:02.605+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:02.608+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:18:02.609+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:02.621+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T16:18:02.743+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T16:18:02.744+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 1 records through 1 polls (polled  out 2 records), taking 42031959 nanos, during time span of 179393417 nanos.
[2025-07-18T16:18:02.744+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4745 bytes result sent to driver
[2025-07-18T16:18:02.753+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 279 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:02.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-18T16:18:02.761+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: ResultStage 9 (start at <unknown>:0) finished in 0.299 s
[2025-07-18T16:18:02.762+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:02.763+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-18T16:18:02.763+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.311748 s
[2025-07-18T16:18:02.764+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:02.764+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Committing epoch 3 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:18:02.819+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/3 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.3.196dbf39-0f30-4790-a4d9-7dce5552a35f.tmp
[2025-07-18T16:18:02.834+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:02.964+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.3.196dbf39-0f30-4790-a4d9-7dce5552a35f.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/3
[2025-07-18T16:18:02.965+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855482779,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:02.972+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.981+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.982+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:02.984+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:02.997+0000] {subprocess.py:93} INFO - 25/07/18 16:18:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.023+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.025+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.026+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.033+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.036+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.051+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.053+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.054+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.058+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.058+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.068+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:18:03.075+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.077+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.082+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 20 from start at <unknown>:0
[2025-07-18T16:18:03.083+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:03.085+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:03.087+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:03.091+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
[2025-07-18T16:18:03.092+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:03.099+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:03.106+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:03.111+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.115+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.118+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.120+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:03.121+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:03.124+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-18T16:18:03.124+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:18:03.124+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2025-07-18T16:18:03.151+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:03.152+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=126 untilOffset=127, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=3 taskId=10 partitionId=0
[2025-07-18T16:18:03.171+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 126 for partition checkins-0
[2025-07-18T16:18:03.186+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:18:03.193+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.193+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:18:03.200+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.207+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T16:18:03.252+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T16:18:03.258+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 1 records through 1 polls (polled  out 2 records), taking 29323833 nanos, during time span of 77396083 nanos.
[2025-07-18T16:18:03.260+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 4822 bytes result sent to driver
[2025-07-18T16:18:03.261+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 168 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:03.264+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-18T16:18:03.270+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.177 s
[2025-07-18T16:18:03.272+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:03.273+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-18T16:18:03.274+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.188402 s
[2025-07-18T16:18:03.276+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:03.276+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing epoch 3 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:18:03.417+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:03.439+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/3 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.3.ae20c2f2-177f-48dd-a4df-3850d56aedcd.tmp
[2025-07-18T16:18:03.452+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v69.metadata.json
[2025-07-18T16:18:03.545+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SnapshotProducer: Committed snapshot 6585740963032581634 (FastAppend)
[2025-07-18T16:18:03.558+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.3.ae20c2f2-177f-48dd-a4df-3850d56aedcd.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/3
[2025-07-18T16:18:03.559+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1752855483406,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:03.640+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.643+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.650+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.655+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.660+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.662+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.664+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.665+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.671+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.673+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.676+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.677+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.677+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:03.682+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6585740963032581634, sequenceNumber=68, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.829323376S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=68}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=594}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3004}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=215009}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:03.688+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committed in 830 ms
[2025-07-18T16:18:03.690+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:18:03.692+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.698+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:03.700+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:03.721+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:03.727+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/3 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.3.97aa87a7-c064-47fb-9528-2b69d6023904.tmp
[2025-07-18T16:18:03.729+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.730+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 22 from start at <unknown>:0
[2025-07-18T16:18:03.732+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:03.733+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:03.740+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:03.741+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
[2025-07-18T16:18:03.742+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:03.745+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:03.748+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:03.755+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:03.757+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:18:03.759+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.760+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:03.761+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:03.763+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-18T16:18:03.764+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:18:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2025-07-18T16:18:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=126 untilOffset=127, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=3 taskId=11 partitionId=0
[2025-07-18T16:18:03.765+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 126 for partition feedback-0
[2025-07-18T16:18:03.766+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:18:03.770+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v78.metadata.json
[2025-07-18T16:18:03.828+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.830+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:18:03.832+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:03.833+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.3.97aa87a7-c064-47fb-9528-2b69d6023904.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/3
[2025-07-18T16:18:03.838+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.853+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:03.872+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:18:03.873+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:18:03.875+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:03.880+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:02.161Z",
[2025-07-18T16:18:03.880+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:18:03.881+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:18:03.883+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:18:03.884+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6009615384615384,
[2025-07-18T16:18:03.885+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:03.888+0000] {subprocess.py:93} INFO -     "addBatch" : 1370,
[2025-07-18T16:18:03.890+0000] {subprocess.py:93} INFO -     "commitOffsets" : 172,
[2025-07-18T16:18:03.890+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:03.893+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:18:03.894+0000] {subprocess.py:93} INFO -     "queryPlanning" : 63,
[2025-07-18T16:18:03.895+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1664,
[2025-07-18T16:18:03.896+0000] {subprocess.py:93} INFO -     "walCommit" : 53
[2025-07-18T16:18:03.896+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:03.897+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:03.902+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:03.902+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:18:03.904+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:03.904+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:03.905+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:18:03.905+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:03.905+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:03.906+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:03.906+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:03.906+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:03.907+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:03.907+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:03.908+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:03.908+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:03.908+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:03.909+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:03.909+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:03.909+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:18:03.910+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:18:03.910+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6009615384615384,
[2025-07-18T16:18:03.910+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:03.911+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:03.912+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:03.912+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:03.914+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:03.915+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:03.916+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:03.916+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:18:03.919+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:18:03.919+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:03.920+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:03.920+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T16:18:03.921+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.921+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/4 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.4.777a15d5-0afc-497d-9e6f-965ddd3b15d0.tmp
[2025-07-18T16:18:03.922+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:03.922+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T16:18:03.922+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 1 records through 1 polls (polled  out 2 records), taking 66848458 nanos, during time span of 132267416 nanos.
[2025-07-18T16:18:03.924+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 4820 bytes result sent to driver
[2025-07-18T16:18:03.925+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 160 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:03.928+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-18T16:18:03.929+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.175 s
[2025-07-18T16:18:03.929+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:03.929+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-07-18T16:18:03.930+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.182113 s
[2025-07-18T16:18:03.930+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:03.930+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing epoch 3 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:18:03.936+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.4.777a15d5-0afc-497d-9e6f-965ddd3b15d0.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/4
[2025-07-18T16:18:03.936+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855483846,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:03.948+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SnapshotProducer: Committed snapshot 2544579344389546561 (FastAppend)
[2025-07-18T16:18:03.978+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:03.983+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:03.987+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:03.987+0000] {subprocess.py:93} INFO - 25/07/18 16:18:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.018+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.020+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.043+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:04.044+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:04.044+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:04.044+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.052+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.057+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:04.058+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:04.058+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:04.058+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.083+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.084+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2544579344389546561, sequenceNumber=77, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.656444542S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=77}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=595}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2896}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=242791}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:04.084+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committed in 658 ms
[2025-07-18T16:18:04.085+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:18:04.092+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/3 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.3.c4fb39a5-999a-42a0-bf7f-2d67f71f80ee.tmp
[2025-07-18T16:18:04.102+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:04.112+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:18:04.117+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:04.118+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Created broadcast 24 from start at <unknown>:0
[2025-07-18T16:18:04.118+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:04.119+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:04.119+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:04.119+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Final stage: ResultStage 12 (start at <unknown>:0)
[2025-07-18T16:18:04.120+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:04.120+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:04.121+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:04.122+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T16:18:04.133+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:04.136+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:04.139+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:04.140+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:04.146+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-07-18T16:18:04.149+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:18:04.150+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2025-07-18T16:18:04.159+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:04.162+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=127 untilOffset=129, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=4 taskId=12 partitionId=0
[2025-07-18T16:18:04.190+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to offset 128 for partition reservations-0
[2025-07-18T16:18:04.196+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:18:04.222+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.3.c4fb39a5-999a-42a0-bf7f-2d67f71f80ee.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/3
[2025-07-18T16:18:04.228+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:04.229+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:18:04.230+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:18:04.230+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:04.231+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:02.770Z",
[2025-07-18T16:18:04.231+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:18:04.232+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:18:04.233+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T16:18:04.233+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6915629322268326,
[2025-07-18T16:18:04.233+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:04.233+0000] {subprocess.py:93} INFO -     "addBatch" : 1071,
[2025-07-18T16:18:04.234+0000] {subprocess.py:93} INFO -     "commitOffsets" : 145,
[2025-07-18T16:18:04.235+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:04.236+0000] {subprocess.py:93} INFO -     "latestOffset" : 9,
[2025-07-18T16:18:04.236+0000] {subprocess.py:93} INFO -     "queryPlanning" : 37,
[2025-07-18T16:18:04.237+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1446,
[2025-07-18T16:18:04.238+0000] {subprocess.py:93} INFO -     "walCommit" : 184
[2025-07-18T16:18:04.240+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:04.240+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:04.241+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:04.241+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:18:04.241+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:04.242+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:04.243+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:18:04.244+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.244+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.244+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:04.244+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:04.244+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:04.244+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.245+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.245+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:04.245+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:04.245+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:04.245+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.245+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.245+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:18:04.246+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T16:18:04.246+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6915629322268326,
[2025-07-18T16:18:04.246+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:04.246+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:04.246+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:04.246+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:04.246+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:04.247+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:04.247+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:04.248+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:18:04.249+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:18:04.250+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:04.251+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:04.251+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:04.261+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:18:04.273+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/4 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.4.707714f3-2b65-4715-8810-7a252ca7e853.tmp
[2025-07-18T16:18:04.330+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v80.metadata.json
[2025-07-18T16:18:04.340+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.4.707714f3-2b65-4715-8810-7a252ca7e853.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/4
[2025-07-18T16:18:04.342+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855484222,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:04.356+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.360+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.368+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.370+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.373+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.386+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.387+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.387+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.387+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.387+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.389+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.390+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.398+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:04.403+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.404+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.416+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:04.418+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:18:04.419+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:04.421+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Created broadcast 26 from start at <unknown>:0
[2025-07-18T16:18:04.423+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:04.423+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:04.424+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:04.433+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
[2025-07-18T16:18:04.434+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:04.450+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:04.452+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:04.456+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:18:04.458+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:18:04.464+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:04.465+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:04.466+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:04.467+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-07-18T16:18:04.472+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:18:04.476+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2025-07-18T16:18:04.501+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:04.504+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=127 untilOffset=129, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=4 taskId=13 partitionId=0
[2025-07-18T16:18:04.526+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SnapshotProducer: Committed snapshot 4154303294774086704 (FastAppend)
[2025-07-18T16:18:04.546+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 128 for partition checkins-0
[2025-07-18T16:18:04.548+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:18:04.625+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=4154303294774086704, sequenceNumber=79, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.636547125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=79}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=595}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=252906}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:04.625+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committed in 637 ms
[2025-07-18T16:18:04.626+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:18:04.639+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/3 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.3.e4f7625b-608c-451b-9752-5e0867989612.tmp
[2025-07-18T16:18:04.695+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.3.e4f7625b-608c-451b-9752-5e0867989612.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/3
[2025-07-18T16:18:04.696+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:04.697+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:18:04.698+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:18:04.699+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:04.699+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:03.402Z",
[2025-07-18T16:18:04.700+0000] {subprocess.py:93} INFO -   "batchId" : 3,
[2025-07-18T16:18:04.700+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:18:04.701+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:18:04.701+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.774593338497289,
[2025-07-18T16:18:04.702+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:04.704+0000] {subprocess.py:93} INFO -     "addBatch" : 989,
[2025-07-18T16:18:04.705+0000] {subprocess.py:93} INFO -     "commitOffsets" : 70,
[2025-07-18T16:18:04.718+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:04.719+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:18:04.719+0000] {subprocess.py:93} INFO -     "queryPlanning" : 67,
[2025-07-18T16:18:04.720+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1291,
[2025-07-18T16:18:04.721+0000] {subprocess.py:93} INFO -     "walCommit" : 153
[2025-07-18T16:18:04.723+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:04.724+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:04.724+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:04.725+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:18:04.725+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:04.725+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:04.726+0000] {subprocess.py:93} INFO -         "0" : 126
[2025-07-18T16:18:04.726+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.726+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.727+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:04.728+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:04.728+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:04.729+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.730+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.730+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:04.730+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:04.731+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:04.731+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:04.737+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:04.738+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:18:04.738+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:18:04.739+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.774593338497289,
[2025-07-18T16:18:04.739+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:04.741+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:04.742+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:04.742+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:04.742+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:04.743+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:04.743+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:04.746+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:18:04.747+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:18:04.747+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:04.749+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:04.749+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:04.750+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:18:04.750+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:04.752+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T16:18:04.753+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/4 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.4.db51e3da-2261-48ad-9422-e5a8647c180a.tmp
[2025-07-18T16:18:04.780+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T16:18:04.781+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 2 records through 1 polls (polled  out 1 records), taking 511711917 nanos, during time span of 581081750 nanos.
[2025-07-18T16:18:04.781+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 4692 bytes result sent to driver
[2025-07-18T16:18:04.782+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 645 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:04.786+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-07-18T16:18:04.793+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: ResultStage 12 (start at <unknown>:0) finished in 0.664 s
[2025-07-18T16:18:04.797+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:04.799+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-07-18T16:18:04.800+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.4.db51e3da-2261-48ad-9422-e5a8647c180a.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/4
[2025-07-18T16:18:04.800+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1752855484698,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:18:04.801+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.674306 s
[2025-07-18T16:18:04.802+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:04.803+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committing epoch 4 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:18:04.805+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.806+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.807+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.810+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.811+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.852+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.853+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.854+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.865+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.873+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.890+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.891+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.892+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:04.898+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.899+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:18:04.919+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:18:04.923+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:18:04.924+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:04.925+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:04.933+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Created broadcast 28 from start at <unknown>:0
[2025-07-18T16:18:04.940+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:18:04.942+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:18:04.943+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:18:04.947+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Final stage: ResultStage 14 (start at <unknown>:0)
[2025-07-18T16:18:04.948+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:18:04.954+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:18:04.956+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:18:04.960+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:18:04.961+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.0 MiB)
[2025-07-18T16:18:04.961+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 77cb57a6bd53:41117 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T16:18:04.963+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:18:04.965+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:18:04.966+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2025-07-18T16:18:04.971+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:18:04.971+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2025-07-18T16:18:04.972+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:18:04.972+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=127 untilOffset=129, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=4 taskId=14 partitionId=0
[2025-07-18T16:18:04.973+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 128 for partition feedback-0
[2025-07-18T16:18:04.978+0000] {subprocess.py:93} INFO - 25/07/18 16:18:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:18:05.058+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:05.076+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:18:05.077+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:05.079+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T16:18:05.127+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T16:18:05.129+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 2 records through 1 polls (polled  out 1 records), taking 510340792 nanos, during time span of 592643376 nanos.
[2025-07-18T16:18:05.130+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 4808 bytes result sent to driver
[2025-07-18T16:18:05.137+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 667 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:05.141+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-18T16:18:05.142+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.711 s
[2025-07-18T16:18:05.143+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:05.143+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-07-18T16:18:05.144+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.717151 s
[2025-07-18T16:18:05.144+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:05.145+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing epoch 4 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:18:05.180+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:18:05.237+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v72.metadata.json
[2025-07-18T16:18:05.320+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SnapshotProducer: Committed snapshot 6004173062481861507 (FastAppend)
[2025-07-18T16:18:05.337+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.345+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.442+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6004173062481861507, sequenceNumber=71, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.52686175S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=71}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=598}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3013}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=224030}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:05.445+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committed in 527 ms
[2025-07-18T16:18:05.446+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:18:05.448+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 WARN Tasks: Retrying task after failure: Version 81 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v81.metadata.json
[2025-07-18T16:18:05.448+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 81 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v81.metadata.json
[2025-07-18T16:18:05.449+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:18:05.450+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:18:05.452+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:18:05.455+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:18:05.457+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:18:05.460+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:18:05.461+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:18:05.462+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:18:05.464+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:18:05.464+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:18:05.465+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:18:05.467+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:18:05.467+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:18:05.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:18:05.468+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:18:05.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:18:05.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:18:05.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:18:05.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:18:05.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:18:05.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:18:05.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:18:05.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:18:05.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:18:05.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:18:05.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:18:05.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.494+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:18:05.497+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:18:05.500+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:18:05.501+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:18:05.502+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:18:05.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:18:05.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:18:05.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:18:05.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:18:05.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:18:05.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:18:05.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:18:05.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:18:05.514+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:05.514+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:18:05.514+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=129, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:18:05.515+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T16:18:05.516+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/4 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.4.9abdda5a-c2bf-49e5-8595-00844d78e5e7.tmp
[2025-07-18T16:18:05.539+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.4.9abdda5a-c2bf-49e5-8595-00844d78e5e7.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/4
[2025-07-18T16:18:05.541+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T16:18:05.541+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 2 records through 1 polls (polled  out 1 records), taking 507178458 nanos, during time span of 571094959 nanos.
[2025-07-18T16:18:05.556+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 4859 bytes result sent to driver
[2025-07-18T16:18:05.558+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:05.559+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:18:05.560+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:18:05.561+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:05.562+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:03.843Z",
[2025-07-18T16:18:05.562+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:18:05.563+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:18:05.564+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 1.1890606420927468,
[2025-07-18T16:18:05.565+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.1806375442739079,
[2025-07-18T16:18:05.565+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:05.566+0000] {subprocess.py:93} INFO -     "addBatch" : 1436,
[2025-07-18T16:18:05.566+0000] {subprocess.py:93} INFO -     "commitOffsets" : 97,
[2025-07-18T16:18:05.567+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:05.567+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:18:05.568+0000] {subprocess.py:93} INFO -     "queryPlanning" : 71,
[2025-07-18T16:18:05.569+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1694,
[2025-07-18T16:18:05.569+0000] {subprocess.py:93} INFO -     "walCommit" : 87
[2025-07-18T16:18:05.569+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:05.570+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:05.570+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:05.571+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:18:05.572+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:05.573+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:05.573+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:05.574+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.578+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.579+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:05.579+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:05.579+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:05.580+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.580+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.580+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:05.581+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:18:05.581+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:05.581+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:05.582+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:05.582+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:18:05.583+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 1.1890606420927468,
[2025-07-18T16:18:05.583+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.1806375442739079,
[2025-07-18T16:18:05.584+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:05.584+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:05.584+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:05.584+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:05.585+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:05.585+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:05.585+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:05.586+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:18:05.588+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:18:05.588+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:05.589+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:05.590+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 618 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:18:05.591+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-07-18T16:18:05.591+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: ResultStage 14 (start at <unknown>:0) finished in 0.643 s
[2025-07-18T16:18:05.591+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:18:05.591+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2025-07-18T16:18:05.592+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.646872 s
[2025-07-18T16:18:05.592+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:18:05.592+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing epoch 4 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:18:05.667+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.682+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:18:05.684+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:18:05.830+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v82.metadata.json
[2025-07-18T16:18:05.883+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v83.metadata.json
[2025-07-18T16:18:05.913+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SnapshotProducer: Committed snapshot 2187821811283834849 (FastAppend)
[2025-07-18T16:18:05.947+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SnapshotProducer: Committed snapshot 8355780183204368080 (FastAppend)
[2025-07-18T16:18:05.969+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2187821811283834849, sequenceNumber=81, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.784767542S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=81}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=601}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2979}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=254541}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:05.972+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committed in 786 ms
[2025-07-18T16:18:05.972+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:18:05.982+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/4 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.4.2014d8d3-eaf9-494f-965d-9c41447987f2.tmp
[2025-07-18T16:18:05.990+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=8355780183204368080, sequenceNumber=82, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.311305917S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=82}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=599}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2922}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=261726}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:18:05.991+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO SparkWrite: Committed in 312 ms
[2025-07-18T16:18:05.991+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:18:06.003+0000] {subprocess.py:93} INFO - 25/07/18 16:18:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/4 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.4.6d610ea2-29d9-467a-921e-7bdb594ba1b2.tmp
[2025-07-18T16:18:06.011+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.4.2014d8d3-eaf9-494f-965d-9c41447987f2.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/4
[2025-07-18T16:18:06.013+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:06.013+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:18:06.014+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:18:06.016+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:06.017+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:04.219Z",
[2025-07-18T16:18:06.017+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:18:06.017+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:18:06.018+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 1.3802622498274673,
[2025-07-18T16:18:06.018+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.119194180190263,
[2025-07-18T16:18:06.019+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:06.021+0000] {subprocess.py:93} INFO -     "addBatch" : 1603,
[2025-07-18T16:18:06.022+0000] {subprocess.py:93} INFO -     "commitOffsets" : 42,
[2025-07-18T16:18:06.024+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:18:06.025+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:18:06.025+0000] {subprocess.py:93} INFO -     "queryPlanning" : 21,
[2025-07-18T16:18:06.025+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1787,
[2025-07-18T16:18:06.026+0000] {subprocess.py:93} INFO -     "walCommit" : 115
[2025-07-18T16:18:06.026+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:06.026+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:06.026+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:06.027+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:18:06.027+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:06.028+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:06.031+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:06.033+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.035+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.036+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:06.037+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:06.038+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.038+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.039+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.040+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:06.041+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:18:06.041+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.041+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.042+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.042+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:18:06.042+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 1.3802622498274673,
[2025-07-18T16:18:06.043+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.119194180190263,
[2025-07-18T16:18:06.043+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:06.044+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:06.045+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:06.046+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:06.046+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:06.047+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:06.047+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:06.048+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:18:06.048+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:18:06.048+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:06.049+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:06.056+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.4.6d610ea2-29d9-467a-921e-7bdb594ba1b2.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/4
[2025-07-18T16:18:06.060+0000] {subprocess.py:93} INFO - 25/07/18 16:18:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:18:06.061+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:18:06.063+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:18:06.065+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:18:06.066+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:18:04.694Z",
[2025-07-18T16:18:06.067+0000] {subprocess.py:93} INFO -   "batchId" : 4,
[2025-07-18T16:18:06.068+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:18:06.068+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 1.547987616099071,
[2025-07-18T16:18:06.069+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.4727540500736376,
[2025-07-18T16:18:06.069+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:18:06.069+0000] {subprocess.py:93} INFO -     "addBatch" : 1178,
[2025-07-18T16:18:06.071+0000] {subprocess.py:93} INFO -     "commitOffsets" : 61,
[2025-07-18T16:18:06.072+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:18:06.072+0000] {subprocess.py:93} INFO -     "latestOffset" : 4,
[2025-07-18T16:18:06.073+0000] {subprocess.py:93} INFO -     "queryPlanning" : 24,
[2025-07-18T16:18:06.074+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1358,
[2025-07-18T16:18:06.075+0000] {subprocess.py:93} INFO -     "walCommit" : 89
[2025-07-18T16:18:06.075+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:18:06.075+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:18:06.076+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:18:06.077+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:18:06.078+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:18:06.082+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:06.083+0000] {subprocess.py:93} INFO -         "0" : 127
[2025-07-18T16:18:06.084+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.084+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.084+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:18:06.084+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:06.085+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.085+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.086+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.087+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:18:06.087+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:18:06.088+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 1.547987616099071,
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.4727540500736376,
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:18:06.089+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:18:06.090+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:18:06.090+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:18:06.091+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:18:06.092+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:18:06.092+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:18:06.095+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:18:06.097+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:18:06.097+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:18:06.097+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:18:15.574+0000] {subprocess.py:93} INFO - 25/07/18 16:18:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:16.024+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:16.062+0000] {subprocess.py:93} INFO - 25/07/18 16:18:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:17.205+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:17.212+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:18:17.218+0000] {subprocess.py:93} INFO - 25/07/18 16:18:17 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 77cb57a6bd53:41117 in memory (size: 12.1 KiB, free: 434.4 MiB)
[2025-07-18T16:18:25.546+0000] {subprocess.py:93} INFO - 25/07/18 16:18:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:26.021+0000] {subprocess.py:93} INFO - 25/07/18 16:18:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:26.067+0000] {subprocess.py:93} INFO - 25/07/18 16:18:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:35.557+0000] {subprocess.py:93} INFO - 25/07/18 16:18:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:36.033+0000] {subprocess.py:93} INFO - 25/07/18 16:18:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:36.071+0000] {subprocess.py:93} INFO - 25/07/18 16:18:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:45.555+0000] {subprocess.py:93} INFO - 25/07/18 16:18:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:46.039+0000] {subprocess.py:93} INFO - 25/07/18 16:18:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:46.076+0000] {subprocess.py:93} INFO - 25/07/18 16:18:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:55.569+0000] {subprocess.py:93} INFO - 25/07/18 16:18:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:56.050+0000] {subprocess.py:93} INFO - 25/07/18 16:18:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:18:56.086+0000] {subprocess.py:93} INFO - 25/07/18 16:18:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:05.567+0000] {subprocess.py:93} INFO - 25/07/18 16:19:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:06.050+0000] {subprocess.py:93} INFO - 25/07/18 16:19:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:06.090+0000] {subprocess.py:93} INFO - 25/07/18 16:19:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:15.571+0000] {subprocess.py:93} INFO - 25/07/18 16:19:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:16.050+0000] {subprocess.py:93} INFO - 25/07/18 16:19:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:16.102+0000] {subprocess.py:93} INFO - 25/07/18 16:19:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:25.571+0000] {subprocess.py:93} INFO - 25/07/18 16:19:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:26.050+0000] {subprocess.py:93} INFO - 25/07/18 16:19:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:26.111+0000] {subprocess.py:93} INFO - 25/07/18 16:19:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:35.578+0000] {subprocess.py:93} INFO - 25/07/18 16:19:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:36.063+0000] {subprocess.py:93} INFO - 25/07/18 16:19:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:36.124+0000] {subprocess.py:93} INFO - 25/07/18 16:19:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:41.375+0000] {subprocess.py:93} INFO - 25/07/18 16:19:41 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2025-07-18T16:19:41.376+0000] {subprocess.py:93} INFO - 25/07/18 16:19:41 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
[2025-07-18T16:19:41.376+0000] {subprocess.py:93} INFO - 25/07/18 16:19:41 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
[2025-07-18T16:19:45.583+0000] {subprocess.py:93} INFO - 25/07/18 16:19:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:46.066+0000] {subprocess.py:93} INFO - 25/07/18 16:19:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:46.126+0000] {subprocess.py:93} INFO - 25/07/18 16:19:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:55.593+0000] {subprocess.py:93} INFO - 25/07/18 16:19:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:56.066+0000] {subprocess.py:93} INFO - 25/07/18 16:19:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:19:56.129+0000] {subprocess.py:93} INFO - 25/07/18 16:19:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:04.196+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/5 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.5.b733d3df-664c-4de4-b802-6cec4905fc24.tmp
[2025-07-18T16:20:04.236+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.5.b733d3df-664c-4de4-b802-6cec4905fc24.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/5
[2025-07-18T16:20:04.242+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752855604183,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:04.297+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.301+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.304+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.306+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.308+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.332+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.336+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.338+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.339+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.357+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.360+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.362+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:04.364+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.371+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.410+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:20:04.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.438+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:04.439+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Created broadcast 30 from start at <unknown>:0
[2025-07-18T16:20:04.439+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:04.440+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:04.446+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:04.446+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
[2025-07-18T16:20:04.447+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:04.448+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:04.448+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:04.458+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.464+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:20:04.524+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:04.526+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:04.528+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:04.528+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-07-18T16:20:04.530+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:20:04.538+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2025-07-18T16:20:04.638+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:04.648+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=129 untilOffset=130, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=5 taskId=15 partitionId=0
[2025-07-18T16:20:04.676+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to offset 129 for partition reservations-0
[2025-07-18T16:20:04.685+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:20:04.809+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/5 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.5.fe0c8263-37e7-4b90-b78a-4d47c8a6decc.tmp
[2025-07-18T16:20:04.863+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.5.fe0c8263-37e7-4b90-b78a-4d47c8a6decc.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/5
[2025-07-18T16:20:04.869+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752855604791,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:04.905+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.907+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.911+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.917+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.925+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.927+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.928+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.933+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.936+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.956+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.957+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.957+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:04.957+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.966+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:04.993+0000] {subprocess.py:93} INFO - 25/07/18 16:20:04 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:20:05.003+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.004+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 32 from start at <unknown>:0
[2025-07-18T16:20:05.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:05.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:05.008+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:05.008+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Final stage: ResultStage 16 (start at <unknown>:0)
[2025-07-18T16:20:05.009+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:05.009+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:05.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:05.011+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.014+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.020+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.021+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:05.021+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:05.022+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-07-18T16:20:05.022+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:20:05.024+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2025-07-18T16:20:05.037+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:05.042+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=129 untilOffset=130, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=5 taskId=16 partitionId=0
[2025-07-18T16:20:05.043+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 129 for partition checkins-0
[2025-07-18T16:20:05.061+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:20:05.214+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.220+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:20:05.221+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.223+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.224+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:20:05.225+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:05.227+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T16:20:05.229+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T16:20:05.325+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T16:20:05.327+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 1 records through 1 polls (polled  out 3 records), taking 541443292 nanos, during time span of 658669708 nanos.
[2025-07-18T16:20:05.333+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T16:20:05.336+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 1 records through 1 polls (polled  out 2 records), taking 158612542 nanos, during time span of 284394875 nanos.
[2025-07-18T16:20:05.341+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 4697 bytes result sent to driver
[2025-07-18T16:20:05.342+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 4811 bytes result sent to driver
[2025-07-18T16:20:05.343+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 314 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:05.344+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: ResultStage 16 (start at <unknown>:0) finished in 0.324 s
[2025-07-18T16:20:05.346+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-07-18T16:20:05.348+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 822 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:05.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:05.352+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-07-18T16:20:05.352+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-07-18T16:20:05.353+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.889 s
[2025-07-18T16:20:05.355+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:05.356+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2025-07-18T16:20:05.358+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.899342 s
[2025-07-18T16:20:05.358+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:05.359+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing epoch 5 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:20:05.359+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 0.335833 s
[2025-07-18T16:20:05.359+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:05.360+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing epoch 5 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:20:05.436+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:05.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:05.462+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/5 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.5.32f5bc32-112a-4a69-8fc7-381891b9955d.tmp
[2025-07-18T16:20:05.543+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.5.32f5bc32-112a-4a69-8fc7-381891b9955d.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/5
[2025-07-18T16:20:05.544+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1752855605418,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:05.570+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.571+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.583+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.583+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.585+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.596+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.598+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.605+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.606+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.609+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.619+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.622+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:05.629+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.632+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:05.660+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:05.664+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:05.667+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.671+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 34 from start at <unknown>:0
[2025-07-18T16:20:05.675+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:05.676+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:05.682+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:05.683+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
[2025-07-18T16:20:05.685+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:05.687+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:05.688+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:05.698+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:05.701+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:20:05.704+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:05.707+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:05.717+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:05.718+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-07-18T16:20:05.718+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:05.718+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2025-07-18T16:20:05.787+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v85.metadata.json
[2025-07-18T16:20:05.820+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:05.824+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=129 untilOffset=130, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=5 taskId=17 partitionId=0
[2025-07-18T16:20:05.826+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v76.metadata.json
[2025-07-18T16:20:05.829+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 129 for partition feedback-0
[2025-07-18T16:20:05.832+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:20:05.955+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SnapshotProducer: Committed snapshot 4949958554216890375 (FastAppend)
[2025-07-18T16:20:05.978+0000] {subprocess.py:93} INFO - 25/07/18 16:20:05 INFO SnapshotProducer: Committed snapshot 7260724556969351326 (FastAppend)
[2025-07-18T16:20:06.132+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4949958554216890375, sequenceNumber=84, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.706526709S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=84}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=733}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2876}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=267983}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:06.133+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committed in 707 ms
[2025-07-18T16:20:06.134+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:20:06.137+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=7260724556969351326, sequenceNumber=75, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.701031751S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=75}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=732}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2969}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=238982}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:06.139+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committed in 705 ms
[2025-07-18T16:20:06.142+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:20:06.200+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.202+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/5 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.5.e5661cd2-eaf0-4f29-8c0d-1a74eeccb945.tmp
[2025-07-18T16:20:06.216+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.225+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.238+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/5 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.5.6e1319e9-aad7-47a3-9262-d39214bcd715.tmp
[2025-07-18T16:20:06.240+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:06.343+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:06.346+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:20:06.355+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.5.e5661cd2-eaf0-4f29-8c0d-1a74eeccb945.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/5
[2025-07-18T16:20:06.360+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:06.366+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:06.368+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:20:06.371+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:20:06.372+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:06.375+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:04.182Z",
[2025-07-18T16:20:06.377+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T16:20:06.385+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:06.388+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T16:20:06.390+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.46382189239332094,
[2025-07-18T16:20:06.394+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:06.396+0000] {subprocess.py:93} INFO -     "addBatch" : 1817,
[2025-07-18T16:20:06.403+0000] {subprocess.py:93} INFO -     "commitOffsets" : 205,
[2025-07-18T16:20:06.405+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:20:06.406+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:20:06.409+0000] {subprocess.py:93} INFO -     "queryPlanning" : 68,
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2156,
[2025-07-18T16:20:06.410+0000] {subprocess.py:93} INFO -     "walCommit" : 54
[2025-07-18T16:20:06.412+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:06.417+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:06.419+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:06.421+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:20:06.422+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:06.424+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:06.425+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:06.425+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.426+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.427+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:06.428+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:06.429+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:06.429+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.430+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.430+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:06.431+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:06.432+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:06.433+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.435+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.436+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:06.438+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T16:20:06.439+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.46382189239332094,
[2025-07-18T16:20:06.439+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:06.439+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:06.440+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:06.440+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:06.440+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:06.441+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:06.441+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:06.442+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:20:06.443+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:06.444+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:06.445+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:06.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T16:20:06.447+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.5.6e1319e9-aad7-47a3-9262-d39214bcd715.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/5
[2025-07-18T16:20:06.448+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:06.449+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:20:06.450+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:20:06.450+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:06.450+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:04.787Z",
[2025-07-18T16:20:06.450+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T16:20:06.451+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:06.451+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 62.5,
[2025-07-18T16:20:06.452+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6345177664974619,
[2025-07-18T16:20:06.452+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:06.452+0000] {subprocess.py:93} INFO -     "addBatch" : 1275,
[2025-07-18T16:20:06.453+0000] {subprocess.py:93} INFO -     "commitOffsets" : 172,
[2025-07-18T16:20:06.453+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:06.453+0000] {subprocess.py:93} INFO -     "latestOffset" : 4,
[2025-07-18T16:20:06.453+0000] {subprocess.py:93} INFO -     "queryPlanning" : 60,
[2025-07-18T16:20:06.453+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1576,
[2025-07-18T16:20:06.454+0000] {subprocess.py:93} INFO -     "walCommit" : 64
[2025-07-18T16:20:06.454+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:06.454+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:06.454+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:06.455+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:20:06.455+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:06.455+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:06.456+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:06.456+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.456+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.456+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:06.456+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:06.456+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:06.457+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.457+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.458+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:06.458+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:06.459+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:06.460+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:06.460+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:06.461+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:06.461+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 62.5,
[2025-07-18T16:20:06.461+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6345177664974619,
[2025-07-18T16:20:06.462+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:06.462+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:06.462+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:06.463+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:06.463+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:06.463+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:06.463+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:06.464+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:20:06.464+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:06.464+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:06.464+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:06.465+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/6 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.6.d98e290f-6a53-40e1-a6f2-065deeb70843.tmp
[2025-07-18T16:20:06.465+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/6 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.6.02cb8de8-ad9e-431d-89cd-2f285c78f555.tmp
[2025-07-18T16:20:06.465+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T16:20:06.465+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 1 records through 1 polls (polled  out 3 records), taking 527344291 nanos, during time span of 629116125 nanos.
[2025-07-18T16:20:06.466+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 4810 bytes result sent to driver
[2025-07-18T16:20:06.480+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 763 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:06.482+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-07-18T16:20:06.489+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.798 s
[2025-07-18T16:20:06.496+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:06.497+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-07-18T16:20:06.501+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.819226 s
[2025-07-18T16:20:06.503+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:06.512+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committing epoch 5 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:20:06.513+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.6.d98e290f-6a53-40e1-a6f2-065deeb70843.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/6
[2025-07-18T16:20:06.514+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752855606356,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:06.535+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.540+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.540+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.542+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.543+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.559+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.561+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.578+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.582+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.584+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.589+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.594+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.595+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:06.596+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.600+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.607+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:20:06.612+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:20:06.616+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.617+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Created broadcast 36 from start at <unknown>:0
[2025-07-18T16:20:06.617+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:06.617+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:06.617+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Got job 18 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:06.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
[2025-07-18T16:20:06.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:06.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:06.619+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:06.620+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.6.02cb8de8-ad9e-431d-89cd-2f285c78f555.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/6
[2025-07-18T16:20:06.621+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752855606371,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:06.625+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T16:20:06.628+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:06.629+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.632+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:06.635+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:06.636+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-07-18T16:20:06.638+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:20:06.638+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2025-07-18T16:20:06.639+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.645+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.645+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.646+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.647+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.649+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:06.649+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=130 untilOffset=132, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=6 taskId=18 partitionId=0
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.650+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.653+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.654+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.663+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:06.664+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.666+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:06.680+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:06.688+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T16:20:06.715+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T16:20:06.717+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 64527959 nanos.
[2025-07-18T16:20:06.718+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:06.744+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:06.745+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.745+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 4696 bytes result sent to driver
[2025-07-18T16:20:06.746+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Created broadcast 38 from start at <unknown>:0
[2025-07-18T16:20:06.746+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:06.746+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:06.746+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Got job 19 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:06.747+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Final stage: ResultStage 19 (start at <unknown>:0)
[2025-07-18T16:20:06.747+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:06.747+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:06.747+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:06.748+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 105 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:06.749+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-07-18T16:20:06.786+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T16:20:06.787+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:20:06.797+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:06.803+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:06.803+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:06.804+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-07-18T16:20:06.804+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.183 s
[2025-07-18T16:20:06.805+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:06.806+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-07-18T16:20:06.807+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO DAGScheduler: Job 18 finished: start at <unknown>:0, took 0.190907 s
[2025-07-18T16:20:06.807+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:20:06.808+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2025-07-18T16:20:06.817+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:06.819+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committing epoch 6 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:20:06.846+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:06.852+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=130 untilOffset=132, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=6 taskId=19 partitionId=0
[2025-07-18T16:20:06.867+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 131 for partition checkins-0
[2025-07-18T16:20:06.868+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:20:06.953+0000] {subprocess.py:93} INFO - 25/07/18 16:20:06 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:07.210+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v87.metadata.json
[2025-07-18T16:20:07.333+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 WARN Tasks: Retrying task after failure: Version 78 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v78.metadata.json
[2025-07-18T16:20:07.339+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 78 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v78.metadata.json
[2025-07-18T16:20:07.341+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:20:07.345+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:07.351+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:07.353+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:07.366+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:07.369+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:07.370+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:07.371+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:07.373+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:07.374+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:07.376+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:07.380+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:07.382+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:07.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:07.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:07.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:07.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:07.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:07.389+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:07.389+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:07.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:07.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:07.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:07.394+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:07.395+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:07.396+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:07.397+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:07.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:07.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:07.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:07.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:07.408+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:07.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:07.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:07.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:07.412+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:07.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:07.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:07.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:07.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:07.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:07.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:07.413+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:07.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:07.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:07.415+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:07.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:07.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:07.417+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:07.418+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:20:07.418+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:07.418+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SnapshotProducer: Committed snapshot 7859709553281448494 (FastAppend)
[2025-07-18T16:20:07.419+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T16:20:07.641+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DataWritingSparkTask: Committed partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T16:20:07.642+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 2 records through 1 polls (polled  out 1 records), taking 527315250 nanos, during time span of 778021458 nanos.
[2025-07-18T16:20:07.659+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 4905 bytes result sent to driver
[2025-07-18T16:20:07.680+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:07.683+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 879 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:07.688+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7859709553281448494, sequenceNumber=86, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.009421917S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=86}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=733}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2934}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=279617}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:07.694+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committed in 1019 ms
[2025-07-18T16:20:07.701+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:20:07.703+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: ResultStage 19 (start at <unknown>:0) finished in 0.954 s
[2025-07-18T16:20:07.707+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:07.710+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-07-18T16:20:07.713+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2025-07-18T16:20:07.714+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO DAGScheduler: Job 19 finished: start at <unknown>:0, took 0.980662 s
[2025-07-18T16:20:07.715+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:07.718+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committing epoch 6 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:20:07.722+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/5 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.5.2769318b-6be6-49e7-a427-ac8301d850dd.tmp
[2025-07-18T16:20:07.887+0000] {subprocess.py:93} INFO - 25/07/18 16:20:07 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:08.029+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.5.2769318b-6be6-49e7-a427-ac8301d850dd.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/5
[2025-07-18T16:20:08.033+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:08.034+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:20:08.034+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:20:08.034+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:08.038+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:05.416Z",
[2025-07-18T16:20:08.039+0000] {subprocess.py:93} INFO -   "batchId" : 5,
[2025-07-18T16:20:08.039+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:20:08.039+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 45.45454545454546,
[2025-07-18T16:20:08.039+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.3834355828220859,
[2025-07-18T16:20:08.040+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:08.040+0000] {subprocess.py:93} INFO -     "addBatch" : 2115,
[2025-07-18T16:20:08.041+0000] {subprocess.py:93} INFO -     "commitOffsets" : 327,
[2025-07-18T16:20:08.045+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:08.045+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:20:08.046+0000] {subprocess.py:93} INFO -     "queryPlanning" : 42,
[2025-07-18T16:20:08.046+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2608,
[2025-07-18T16:20:08.047+0000] {subprocess.py:93} INFO -     "walCommit" : 122
[2025-07-18T16:20:08.047+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:08.047+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:08.048+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:08.048+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:20:08.048+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -         "0" : 129
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.049+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.050+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:08.050+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:08.050+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:08.050+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.050+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.050+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:20:08.050+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 45.45454545454546,
[2025-07-18T16:20:08.054+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.3834355828220859,
[2025-07-18T16:20:08.059+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:08.059+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:08.060+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:08.061+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:08.061+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:08.062+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:08.063+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:08.064+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:20:08.064+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:20:08.064+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:08.065+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:08.123+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v79.metadata.json
[2025-07-18T16:20:08.129+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/6 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.6.63a90ced-acc6-4d60-a730-e4a9ca00fa67.tmp
[2025-07-18T16:20:08.202+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.6.63a90ced-acc6-4d60-a730-e4a9ca00fa67.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/6
[2025-07-18T16:20:08.205+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MicroBatchExecution: Committed offsets for batch 6. Metadata OffsetSeqMetadata(0,1752855608059,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:08.236+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.236+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.236+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.260+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.275+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.294+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SnapshotProducer: Committed snapshot 8130715669450131571 (FastAppend)
[2025-07-18T16:20:08.305+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.309+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.310+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.315+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.377+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.379+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.386+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:08.405+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.410+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:08.413+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:20:08.476+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:08.479+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.486+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkContext: Created broadcast 40 from start at <unknown>:0
[2025-07-18T16:20:08.488+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:08.488+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.489+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:08.489+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Got job 20 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:08.490+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
[2025-07-18T16:20:08.491+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:08.491+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:08.491+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:08.492+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:08.498+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:20:08.501+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.502+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:08.504+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:08.506+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2025-07-18T16:20:08.510+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:20:08.580+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2025-07-18T16:20:08.581+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.592+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:08.638+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=8130715669450131571, sequenceNumber=78, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.683183293S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=78}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=736}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3050}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=247970}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:08.639+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Committed in 1687 ms
[2025-07-18T16:20:08.644+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:20:08.672+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 WARN Tasks: Retrying task after failure: Failed to commit changes using rename: s3a://warehouse/bronze/Checkins_raw/metadata/v88.metadata.json
[2025-07-18T16:20:08.672+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Failed to commit changes using rename: s3a://warehouse/bronze/Checkins_raw/metadata/v88.metadata.json
[2025-07-18T16:20:08.672+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:378)
[2025-07-18T16:20:08.673+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:20:08.677+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:20:08.679+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:20:08.683+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:20:08.684+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:20:08.689+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:20:08.692+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:20:08.699+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:20:08.704+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:20:08.704+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:20:08.704+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:20:08.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:20:08.706+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:20:08.706+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:20:08.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:20:08.709+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:20:08.710+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:20:08.716+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:20:08.719+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.720+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.720+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.722+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.725+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.727+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:20:08.729+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:20:08.730+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:20:08.735+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:20:08.737+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:20:08.742+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:20:08.744+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.749+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:20:08.750+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.750+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.750+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.751+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.752+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:20:08.753+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:20:08.754+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.758+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:20:08.762+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:20:08.764+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:20:08.765+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:20:08.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:20:08.767+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:20:08.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:20:08.771+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.771+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:20:08.772+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:20:08.773+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:20:08.773+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:20:08.774+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:20:08.775+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:20:08.776+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Failed to rename s3a://warehouse/bronze/Checkins_raw/metadata/0f83c2f5-c878-485b-90a3-c6c738e4f76d.metadata.json to s3a://warehouse/bronze/Checkins_raw/metadata/v88.metadata.json; destination file exists
[2025-07-18T16:20:08.777+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initiateRename(S3AFileSystem.java:1920)
[2025-07-18T16:20:08.777+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerRename(S3AFileSystem.java:1988)
[2025-07-18T16:20:08.777+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$rename$7(S3AFileSystem.java:1846)
[2025-07-18T16:20:08.778+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2025-07-18T16:20:08.782+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2025-07-18T16:20:08.782+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2025-07-18T16:20:08.783+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:1844)
[2025-07-18T16:20:08.784+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:368)
[2025-07-18T16:20:08.785+0000] {subprocess.py:93} INFO - 	... 59 more
[2025-07-18T16:20:08.786+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/6 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.6.43e37307-60a1-4e56-8a1e-a3f826a080b3.tmp
[2025-07-18T16:20:08.787+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:08.787+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=130 untilOffset=132, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=6 taskId=20 partitionId=0
[2025-07-18T16:20:08.788+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T16:20:08.810+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T16:20:08.812+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 86981708 nanos.
[2025-07-18T16:20:08.823+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 4768 bytes result sent to driver
[2025-07-18T16:20:08.828+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 323 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:08.831+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.343 s
[2025-07-18T16:20:08.832+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:08.832+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-07-18T16:20:08.833+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2025-07-18T16:20:08.833+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO DAGScheduler: Job 20 finished: start at <unknown>:0, took 0.345265 s
[2025-07-18T16:20:08.834+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:08.834+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Committing epoch 6 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:20:08.845+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.6.43e37307-60a1-4e56-8a1e-a3f826a080b3.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/6
[2025-07-18T16:20:08.850+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:08.851+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:20:08.852+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:20:08.853+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:08.853+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:06.349Z",
[2025-07-18T16:20:08.854+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T16:20:08.854+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:08.855+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.9229349330872174,
[2025-07-18T16:20:08.856+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.8025682182985554,
[2025-07-18T16:20:08.857+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:08.858+0000] {subprocess.py:93} INFO -     "addBatch" : 2101,
[2025-07-18T16:20:08.861+0000] {subprocess.py:93} INFO -     "commitOffsets" : 200,
[2025-07-18T16:20:08.864+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:08.865+0000] {subprocess.py:93} INFO -     "latestOffset" : 7,
[2025-07-18T16:20:08.865+0000] {subprocess.py:93} INFO -     "queryPlanning" : 27,
[2025-07-18T16:20:08.866+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2492,
[2025-07-18T16:20:08.866+0000] {subprocess.py:93} INFO -     "walCommit" : 156
[2025-07-18T16:20:08.866+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:08.867+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:08.868+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:08.869+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:20:08.871+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:08.873+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:08.873+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:08.874+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.875+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.877+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:08.878+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:08.881+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:08.884+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.885+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.886+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:08.887+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:08.888+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:08.888+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:08.890+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:08.891+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:08.895+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.9229349330872174,
[2025-07-18T16:20:08.900+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.8025682182985554,
[2025-07-18T16:20:08.905+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:08.908+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:08.909+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:08.909+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:08.910+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:08.910+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:08.911+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:08.911+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:20:08.911+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:08.912+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:08.916+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:08.938+0000] {subprocess.py:93} INFO - 25/07/18 16:20:08 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:09.183+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v89.metadata.json
[2025-07-18T16:20:09.270+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v90.metadata.json
[2025-07-18T16:20:09.285+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SnapshotProducer: Committed snapshot 2288240132200309506 (FastAppend)
[2025-07-18T16:20:09.371+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SnapshotProducer: Committed snapshot 951894329848557320 (FastAppend)
[2025-07-18T16:20:09.394+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2288240132200309506, sequenceNumber=88, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.511055376S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=88}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=738}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2894}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=279505}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:09.403+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committed in 1512 ms
[2025-07-18T16:20:09.408+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:20:09.420+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/6 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.6.5675d8e5-f4b3-4a58-bfee-deb63c0e1a40.tmp
[2025-07-18T16:20:09.425+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=951894329848557320, sequenceNumber=89, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.485486876S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=89}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=737}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3052}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=288537}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:09.426+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO SparkWrite: Committed in 485 ms
[2025-07-18T16:20:09.426+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 6, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:20:09.476+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/6 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.6.25dcd0c2-2402-417b-bfe3-ecce44c6ab95.tmp
[2025-07-18T16:20:09.519+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.6.5675d8e5-f4b3-4a58-bfee-deb63c0e1a40.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/6
[2025-07-18T16:20:09.525+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:09.527+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:20:09.531+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:20:09.533+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:09.537+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:06.366Z",
[2025-07-18T16:20:09.540+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T16:20:09.544+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:09.547+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 1.266624445851805,
[2025-07-18T16:20:09.549+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.6345177664974619,
[2025-07-18T16:20:09.552+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:09.553+0000] {subprocess.py:93} INFO -     "addBatch" : 2755,
[2025-07-18T16:20:09.557+0000] {subprocess.py:93} INFO -     "commitOffsets" : 123,
[2025-07-18T16:20:09.558+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:09.558+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:20:09.559+0000] {subprocess.py:93} INFO -     "queryPlanning" : 21,
[2025-07-18T16:20:09.559+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3152,
[2025-07-18T16:20:09.559+0000] {subprocess.py:93} INFO -     "walCommit" : 248
[2025-07-18T16:20:09.559+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:09.560+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:09.560+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:09.561+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:20:09.561+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:09.562+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:09.562+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:09.562+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.563+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.563+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:09.563+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:09.564+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:09.564+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.565+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.567+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:09.569+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:09.569+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:09.569+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.569+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.569+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:09.569+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 1.266624445851805,
[2025-07-18T16:20:09.570+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.6345177664974619,
[2025-07-18T16:20:09.570+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:09.571+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:09.571+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:09.572+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:09.575+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:09.576+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:09.578+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:09.578+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:20:09.578+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:09.579+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:09.580+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:09.583+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.6.25dcd0c2-2402-417b-bfe3-ecce44c6ab95.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/6
[2025-07-18T16:20:09.584+0000] {subprocess.py:93} INFO - 25/07/18 16:20:09 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:09.586+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:20:09.589+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:20:09.590+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:09.591+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:08.029Z",
[2025-07-18T16:20:09.592+0000] {subprocess.py:93} INFO -   "batchId" : 6,
[2025-07-18T16:20:09.592+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:20:09.592+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.7654037504783774,
[2025-07-18T16:20:09.593+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.3020833333333333,
[2025-07-18T16:20:09.594+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:09.595+0000] {subprocess.py:93} INFO -     "addBatch" : 1139,
[2025-07-18T16:20:09.597+0000] {subprocess.py:93} INFO -     "commitOffsets" : 140,
[2025-07-18T16:20:09.600+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:20:09.602+0000] {subprocess.py:93} INFO -     "latestOffset" : 30,
[2025-07-18T16:20:09.604+0000] {subprocess.py:93} INFO -     "queryPlanning" : 82,
[2025-07-18T16:20:09.605+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1536,
[2025-07-18T16:20:09.606+0000] {subprocess.py:93} INFO -     "walCommit" : 143
[2025-07-18T16:20:09.607+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:09.608+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:09.610+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:09.611+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:20:09.612+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:20:09.616+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:09.617+0000] {subprocess.py:93} INFO -         "0" : 130
[2025-07-18T16:20:09.617+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.618+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.619+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:09.620+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:09.621+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:09.624+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.625+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.627+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:09.628+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:09.628+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:09.629+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:09.630+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:09.630+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:20:09.631+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.7654037504783774,
[2025-07-18T16:20:09.632+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.3020833333333333,
[2025-07-18T16:20:09.632+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:09.633+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:09.633+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:09.633+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:09.633+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:09.634+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:09.634+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:09.636+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:20:09.636+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:20:09.639+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:09.640+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:13.972+0000] {subprocess.py:93} INFO - 25/07/18 16:20:13 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:14.022+0000] {subprocess.py:93} INFO - 25/07/18 16:20:14 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:20:14.100+0000] {subprocess.py:93} INFO - 25/07/18 16:20:14 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:20:14.114+0000] {subprocess.py:93} INFO - 25/07/18 16:20:14 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:18.849+0000] {subprocess.py:93} INFO - 25/07/18 16:20:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:19.529+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:19.580+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:28.848+0000] {subprocess.py:93} INFO - 25/07/18 16:20:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:29.545+0000] {subprocess.py:93} INFO - 25/07/18 16:20:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:29.589+0000] {subprocess.py:93} INFO - 25/07/18 16:20:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:38.858+0000] {subprocess.py:93} INFO - 25/07/18 16:20:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:39.543+0000] {subprocess.py:93} INFO - 25/07/18 16:20:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:39.583+0000] {subprocess.py:93} INFO - 25/07/18 16:20:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:48.867+0000] {subprocess.py:93} INFO - 25/07/18 16:20:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:49.545+0000] {subprocess.py:93} INFO - 25/07/18 16:20:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:49.590+0000] {subprocess.py:93} INFO - 25/07/18 16:20:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:58.870+0000] {subprocess.py:93} INFO - 25/07/18 16:20:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:59.548+0000] {subprocess.py:93} INFO - 25/07/18 16:20:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:59.598+0000] {subprocess.py:93} INFO - 25/07/18 16:20:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:08.876+0000] {subprocess.py:93} INFO - 25/07/18 16:21:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:09.547+0000] {subprocess.py:93} INFO - 25/07/18 16:21:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:09.607+0000] {subprocess.py:93} INFO - 25/07/18 16:21:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:18.885+0000] {subprocess.py:93} INFO - 25/07/18 16:21:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:19.547+0000] {subprocess.py:93} INFO - 25/07/18 16:21:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:19.611+0000] {subprocess.py:93} INFO - 25/07/18 16:21:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:28.886+0000] {subprocess.py:93} INFO - 25/07/18 16:21:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:29.556+0000] {subprocess.py:93} INFO - 25/07/18 16:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:29.618+0000] {subprocess.py:93} INFO - 25/07/18 16:21:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:38.890+0000] {subprocess.py:93} INFO - 25/07/18 16:21:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:39.562+0000] {subprocess.py:93} INFO - 25/07/18 16:21:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:39.624+0000] {subprocess.py:93} INFO - 25/07/18 16:21:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:48.889+0000] {subprocess.py:93} INFO - 25/07/18 16:21:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:49.570+0000] {subprocess.py:93} INFO - 25/07/18 16:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:49.634+0000] {subprocess.py:93} INFO - 25/07/18 16:21:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:58.901+0000] {subprocess.py:93} INFO - 25/07/18 16:21:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:59.586+0000] {subprocess.py:93} INFO - 25/07/18 16:21:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:59.640+0000] {subprocess.py:93} INFO - 25/07/18 16:21:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:02.919+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/7 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.7.09ccd833-5203-4e47-8943-66e8f36cb840.tmp
[2025-07-18T16:22:02.972+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.7.09ccd833-5203-4e47-8943-66e8f36cb840.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/7
[2025-07-18T16:22:02.974+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1752855722865,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.109+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.112+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.114+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.133+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.157+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.201+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.202+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.202+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.202+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.212+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.253+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.254+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.256+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.267+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.270+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.365+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:22:03.384+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.385+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:03.386+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 42 from start at <unknown>:0
[2025-07-18T16:22:03.387+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:03.388+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:03.396+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Got job 21 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:03.397+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Final stage: ResultStage 21 (start at <unknown>:0)
[2025-07-18T16:22:03.398+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:03.400+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:03.414+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[87] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:03.443+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 28.6 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.461+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.463+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:03.473+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:03.479+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[87] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:03.483+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2025-07-18T16:22:03.484+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:22:03.485+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2025-07-18T16:22:03.535+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/7 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.7.1bfd4565-72f8-43ce-9829-42084254e0c2.tmp
[2025-07-18T16:22:03.548+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:03.557+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=132 untilOffset=133, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=7 taskId=21 partitionId=0
[2025-07-18T16:22:03.571+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to offset 132 for partition reservations-0
[2025-07-18T16:22:03.588+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:22:03.630+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.7.1bfd4565-72f8-43ce-9829-42084254e0c2.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/7
[2025-07-18T16:22:03.631+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1752855723492,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.656+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.657+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.657+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.662+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.669+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.685+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.687+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.689+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.701+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.706+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.738+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.738+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.739+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.745+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.753+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.812+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:22:03.821+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.826+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.828+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 44 from start at <unknown>:0
[2025-07-18T16:22:03.832+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:03.837+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:03.838+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Got job 22 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:03.839+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Final stage: ResultStage 22 (start at <unknown>:0)
[2025-07-18T16:22:03.843+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:03.845+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:03.846+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[91] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:03.846+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.853+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.856+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.866+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:03.871+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[91] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:03.873+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-07-18T16:22:03.875+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:22:03.880+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2025-07-18T16:22:03.897+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:03.899+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=132 untilOffset=133, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=7 taskId=22 partitionId=0
[2025-07-18T16:22:03.901+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to offset 132 for partition checkins-0
[2025-07-18T16:22:03.908+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:22:04.091+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.092+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:22:04.097+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor-2, groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.107+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 21, attempt 0, stage 21.0)
[2025-07-18T16:22:04.165+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/7 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.7.abc8ae62-4211-40b4-80f6-94b2abaf4b91.tmp
[2025-07-18T16:22:04.247+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.7.abc8ae62-4211-40b4-80f6-94b2abaf4b91.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/7
[2025-07-18T16:22:04.250+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1752855724097,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:04.277+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.278+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.278+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.279+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 21, attempt 0, stage 21.0)
[2025-07-18T16:22:04.280+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 1 records through 1 polls (polled  out 3 records), taking 526637917 nanos, during time span of 710751292 nanos.
[2025-07-18T16:22:04.284+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 4721 bytes result sent to driver
[2025-07-18T16:22:04.286+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.291+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.293+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 815 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.293+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.294+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 21 (start at <unknown>:0) finished in 0.867 s
[2025-07-18T16:22:04.294+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.294+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2025-07-18T16:22:04.295+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 21 finished: start at <unknown>:0, took 0.903369 s
[2025-07-18T16:22:04.296+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.296+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 7 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:22:04.299+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.300+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.301+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.308+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.310+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.334+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.334+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.340+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.342+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.385+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:04.427+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:04.446+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.446+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:22:04.447+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor-3, groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.457+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:22:04.474+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 22, attempt 0, stage 22.0)
[2025-07-18T16:22:04.486+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:04.518+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:04.553+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 46 from start at <unknown>:0
[2025-07-18T16:22:04.577+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:04.586+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:04.605+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Got job 23 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:04.608+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Final stage: ResultStage 23 (start at <unknown>:0)
[2025-07-18T16:22:04.611+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:04.612+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:04.631+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[95] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:04.687+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:22:04.722+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T16:22:04.728+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:04.739+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 22, attempt 0, stage 22.0)
[2025-07-18T16:22:04.740+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 1 records through 1 polls (polled  out 3 records), taking 545517751 nanos, during time span of 838408292 nanos.
[2025-07-18T16:22:04.741+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 4865 bytes result sent to driver
[2025-07-18T16:22:04.772+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:04.778+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[95] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:04.780+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 900 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.783+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.783+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2025-07-18T16:22:04.784+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 22 (start at <unknown>:0) finished in 0.952 s
[2025-07-18T16:22:04.784+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.790+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:04.819+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 WARN S3AInstrumentation: Closing output stream statistics while data is still marked as pending upload in OutputStreamStatistics{counters=((object_multipart_aborted=0) (object_multipart_aborted.failures=0) (multipart_upload_completed.failures=0) (op_abort.failures=0) (stream_write_total_time=0) (stream_write_exceptions_completing_upload=0) (stream_write_bytes=7356) (stream_write_exceptions=0) (action_executor_acquired.failures=0) (action_executor_acquired=0) (op_hsync=0) (stream_write_block_uploads=1) (op_abort=0) (stream_write_queue_duration=0) (multipart_upload_completed=0) (stream_write_total_data=0) (op_hflush=0));
[2025-07-18T16:22:04.825+0000] {subprocess.py:93} INFO - gauges=((stream_write_block_uploads_pending=1) (stream_write_block_uploads_data_pending=7356));
[2025-07-18T16:22:04.830+0000] {subprocess.py:93} INFO - minimums=((multipart_upload_completed.min=-1) (action_executor_acquired.min=-1) (multipart_upload_completed.failures.min=-1) (op_abort.min=-1) (action_executor_acquired.failures.min=-1) (op_abort.failures.min=-1) (object_multipart_aborted.failures.min=-1) (object_multipart_aborted.min=-1));
[2025-07-18T16:22:04.836+0000] {subprocess.py:93} INFO - maximums=((op_abort.max=-1) (multipart_upload_completed.max=-1) (op_abort.failures.max=-1) (multipart_upload_completed.failures.max=-1) (action_executor_acquired.failures.max=-1) (object_multipart_aborted.failures.max=-1) (object_multipart_aborted.max=-1) (action_executor_acquired.max=-1));
[2025-07-18T16:22:04.838+0000] {subprocess.py:93} INFO - means=((action_executor_acquired.failures.mean=(samples=0, sum=0, mean=0.0000)) (action_executor_acquired.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.failures.mean=(samples=0, sum=0, mean=0.0000)) (op_abort.mean=(samples=0, sum=0, mean=0.0000)) (op_abort.failures.mean=(samples=0, sum=0, mean=0.0000)) (multipart_upload_completed.failures.mean=(samples=0, sum=0, mean=0.0000)) (multipart_upload_completed.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.mean=(samples=0, sum=0, mean=0.0000)));
[2025-07-18T16:22:04.840+0000] {subprocess.py:93} INFO - , blocksActive=0, blockUploadsCompleted=0, blocksAllocated=1, blocksReleased=1, blocksActivelyAllocated=0, transferDuration=0 ms, totalUploadDuration=0 ms, effectiveBandwidth=0.0 bytes/s}
[2025-07-18T16:22:04.843+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-07-18T16:22:04.848+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2025-07-18T16:22:04.856+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 22 finished: start at <unknown>:0, took 1.025360 s
[2025-07-18T16:22:04.857+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.857+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 7 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:22:04.904+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:04.910+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=132 untilOffset=133, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=7 taskId=23 partitionId=0
[2025-07-18T16:22:05.044+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to offset 132 for partition feedback-0
[2025-07-18T16:22:05.054+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:22:05.165+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 WARN Tasks: Retrying task after failure: Version 85 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v85.metadata.json
[2025-07-18T16:22:05.166+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 85 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v85.metadata.json
[2025-07-18T16:22:05.166+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:05.167+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:05.167+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:05.167+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:05.168+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:05.168+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.168+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.169+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.169+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.169+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.169+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.169+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.170+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:05.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:05.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:05.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:05.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:05.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:05.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:05.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:05.238+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:05.567+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:05.568+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:22:05.580+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor-1, groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:05.592+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 23, attempt 0, stage 23.0)
[2025-07-18T16:22:05.669+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 23.0)
[2025-07-18T16:22:05.671+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 1 records through 1 polls (polled  out 3 records), taking 527059958 nanos, during time span of 627074251 nanos.
[2025-07-18T16:22:05.672+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 4759 bytes result sent to driver
[2025-07-18T16:22:05.676+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 889 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:05.678+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-07-18T16:22:05.678+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: ResultStage 23 (start at <unknown>:0) finished in 1.030 s
[2025-07-18T16:22:05.680+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:05.682+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2025-07-18T16:22:05.685+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 23 finished: start at <unknown>:0, took 1.078220 s
[2025-07-18T16:22:05.688+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:05.696+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing epoch 7 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:22:05.831+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:05.834+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 WARN Tasks: Retrying task after failure: Version 86 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json
[2025-07-18T16:22:05.839+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 86 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json
[2025-07-18T16:22:05.840+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:05.840+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:05.841+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:05.842+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:05.842+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:05.843+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.843+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.844+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.845+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.846+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.847+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.848+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.849+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:05.850+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:05.850+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:05.851+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:05.853+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:05.854+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:05.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:05.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:05.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:05.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:05.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.860+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.860+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.863+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:05.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:05.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:05.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:05.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:05.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:05.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:05.873+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:05.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:05.874+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:05.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:05.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:05.938+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 WARN Tasks: Retrying task after failure: Version 94 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.941+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 94 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.948+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:05.951+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:05.953+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:05.954+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:05.955+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:05.956+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.959+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.964+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.965+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.966+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.968+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.971+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.972+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:05.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:05.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:05.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:05.976+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:05.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:05.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:05.979+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:05.980+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:05.982+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:05.983+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:05.989+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:05.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:05.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:05.995+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:05.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:05.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:05.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:05.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.010+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.015+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.021+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.030+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.042+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.044+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.047+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.048+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.051+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.052+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.416+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v87.metadata.json
[2025-07-18T16:22:06.423+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Failed to commit changes using rename: s3a://warehouse/bronze/Feedback_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.427+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Failed to commit changes using rename: s3a://warehouse/bronze/Feedback_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.429+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:378)
[2025-07-18T16:22:06.438+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.442+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.443+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.447+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.452+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.455+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.456+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.460+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.463+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.465+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.467+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.468+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.470+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.471+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.472+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.473+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.499+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.500+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.502+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.503+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.504+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.509+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.516+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.527+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.529+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.531+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.532+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.533+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.534+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.536+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.537+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.538+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.539+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.539+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.540+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.540+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.540+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.540+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.540+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.fs.FileAlreadyExistsException: Failed to rename s3a://warehouse/bronze/Feedback_raw/metadata/6492321d-ea93-4042-9576-dc74ac30deb4.metadata.json to s3a://warehouse/bronze/Feedback_raw/metadata/v95.metadata.json; destination file exists
[2025-07-18T16:22:06.540+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.initiateRename(S3AFileSystem.java:1920)
[2025-07-18T16:22:06.541+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.innerRename(S3AFileSystem.java:1988)
[2025-07-18T16:22:06.541+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$rename$7(S3AFileSystem.java:1846)
[2025-07-18T16:22:06.541+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
[2025-07-18T16:22:06.541+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
[2025-07-18T16:22:06.542+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
[2025-07-18T16:22:06.542+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.fs.s3a.S3AFileSystem.rename(S3AFileSystem.java:1844)
[2025-07-18T16:22:06.542+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:368)
[2025-07-18T16:22:06.542+0000] {subprocess.py:93} INFO - 	... 59 more
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Version 95 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 95 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.543+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.544+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.545+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.545+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.546+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.546+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.546+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.547+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.547+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.548+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.548+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.550+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.551+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.552+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.552+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.553+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.554+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.554+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.555+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.556+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.557+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.557+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.561+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.561+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.561+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.564+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.566+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.568+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.568+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SnapshotProducer: Committed snapshot 1292352206791920120 (FastAppend)
[2025-07-18T16:22:06.658+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=1292352206791920120, sequenceNumber=86, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT2.22018525S, count=1}, attempts=CounterResult{unit=COUNT, value=3}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=86}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=878}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2920}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=274865}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:06.669+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Committed in 2225 ms
[2025-07-18T16:22:06.671+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:06.673+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/7 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.7.9f7de4b5-4a65-4c3a-9b1c-645f3eeedc5a.tmp
[2025-07-18T16:22:06.728+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.7.9f7de4b5-4a65-4c3a-9b1c-645f3eeedc5a.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/7
[2025-07-18T16:22:06.729+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:06.729+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:22:06.730+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:22:06.731+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:06.732+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:02.861Z",
[2025-07-18T16:22:06.732+0000] {subprocess.py:93} INFO -   "batchId" : 7,
[2025-07-18T16:22:06.732+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:06.733+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:06.736+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.25900025900025897,
[2025-07-18T16:22:06.738+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:06.739+0000] {subprocess.py:93} INFO -     "addBatch" : 3490,
[2025-07-18T16:22:06.743+0000] {subprocess.py:93} INFO -     "commitOffsets" : 71,
[2025-07-18T16:22:06.746+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:06.756+0000] {subprocess.py:93} INFO -     "latestOffset" : 4,
[2025-07-18T16:22:06.757+0000] {subprocess.py:93} INFO -     "queryPlanning" : 189,
[2025-07-18T16:22:06.758+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3861,
[2025-07-18T16:22:06.760+0000] {subprocess.py:93} INFO -     "walCommit" : 104
[2025-07-18T16:22:06.764+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:06.766+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:06.768+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:06.809+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:06.812+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:06.813+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:06.813+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:06.814+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.814+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.815+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:06.815+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:06.816+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.817+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.818+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.820+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:06.821+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:06.822+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:06.823+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:06.825+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:06.826+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:06.827+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:06.828+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.25900025900025897,
[2025-07-18T16:22:06.829+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:06.830+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:06.831+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:06.832+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:06.833+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:06.833+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:06.834+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:06.835+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:06.835+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:06.836+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:06.837+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:06.837+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/8 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.8.b6180025-c81b-4a77-a7aa-06ac245f279d.tmp
[2025-07-18T16:22:06.992+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/.8.b6180025-c81b-4a77-a7aa-06ac245f279d.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/offsets/8
[2025-07-18T16:22:07.000+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1752855726730,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:07.119+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.127+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.135+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.153+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.166+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.167+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.168+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.188+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.189+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.199+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.201+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.210+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.212+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.214+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.215+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.224+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.227+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.229+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.230+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.230+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.310+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:07.318+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:22:07.320+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 77cb57a6bd53:41117 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.321+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 48 from start at <unknown>:0
[2025-07-18T16:22:07.326+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:07.326+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:07.329+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Got job 24 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:07.330+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Final stage: ResultStage 24 (start at <unknown>:0)
[2025-07-18T16:22:07.331+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:07.334+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:07.335+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[99] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:07.336+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SnapshotProducer: Committed snapshot 6550966426307780865 (FastAppend)
[2025-07-18T16:22:07.347+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T16:22:07.356+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.2 MiB)
[2025-07-18T16:22:07.364+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 77cb57a6bd53:41117 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.369+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SnapshotProducer: Committed snapshot 3450767356993074716 (FastAppend)
[2025-07-18T16:22:07.376+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:07.377+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[99] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:07.381+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2025-07-18T16:22:07.385+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:22:07.387+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2025-07-18T16:22:07.413+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:07.423+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=133 untilOffset=135, for query queryId=91de1f1c-31ac-4205-a078-ccc8a3f415c2 batchId=8 taskId=24 partitionId=0
[2025-07-18T16:22:07.459+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 24, attempt 0, stage 24.0)
[2025-07-18T16:22:07.469+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN SnapshotProducer: Failed to load committed snapshot, skipping manifest clean-up
[2025-07-18T16:22:07.470+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN SnapshotProducer: Failed to notify listeners
[2025-07-18T16:22:07.471+0000] {subprocess.py:93} INFO - java.lang.NullPointerException: Cannot invoke "org.apache.iceberg.Snapshot.sequenceNumber()" because "snapshot" is null
[2025-07-18T16:22:07.471+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.FastAppend.updateEvent(FastAppend.java:174)
[2025-07-18T16:22:07.472+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.notifyListeners(SnapshotProducer.java:449)
[2025-07-18T16:22:07.472+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:441)
[2025-07-18T16:22:07.473+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:07.473+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:07.474+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:07.474+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:07.474+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:07.475+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:07.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:07.476+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:07.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:07.477+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:07.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:07.478+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:07.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:07.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:07.479+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:07.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:07.480+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:07.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:07.481+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.482+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.483+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:07.484+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:07.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:07.485+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.486+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.487+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.488+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.489+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.490+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:07.491+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.492+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.492+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.493+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.493+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:07.494+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:07.494+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:07.495+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:07.495+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.495+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:07.496+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:07.497+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.497+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:07.498+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:07.498+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committed in 2234 ms
[2025-07-18T16:22:07.498+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:07.499+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=6550966426307780865, sequenceNumber=95, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.664761666S, count=1}, attempts=CounterResult{unit=COUNT, value=2}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=95}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=876}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2713}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=312020}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:07.499+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committed in 1662 ms
[2025-07-18T16:22:07.500+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 7, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:07.504+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/7 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.7.43d0e200-91eb-4bed-b6f7-c2ac66c51601.tmp
[2025-07-18T16:22:07.529+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/7 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.7.d421cb93-17bf-4d9b-a570-3df4a0c21f2f.tmp
[2025-07-18T16:22:07.532+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DataWritingSparkTask: Committed partition 0 (task 24, attempt 0, stage 24.0)
[2025-07-18T16:22:07.533+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-4b39dc10-4d80-4483-b25c-8a9809c49016--386263662-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 85995667 nanos.
[2025-07-18T16:22:07.534+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 4699 bytes result sent to driver
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 149 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: ResultStage 24 (start at <unknown>:0) finished in 0.199 s
[2025-07-18T16:22:07.536+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:07.536+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2025-07-18T16:22:07.536+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Job 24 finished: start at <unknown>:0, took 0.207544 s
[2025-07-18T16:22:07.537+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:07.537+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committing epoch 8 for query 91de1f1c-31ac-4205-a078-ccc8a3f415c2 in append mode
[2025-07-18T16:22:07.578+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.7.43d0e200-91eb-4bed-b6f7-c2ac66c51601.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/7
[2025-07-18T16:22:07.578+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:03.487Z",
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "batchId" : 7,
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.2445585717779408,
[2025-07-18T16:22:07.579+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:07.580+0000] {subprocess.py:93} INFO -     "addBatch" : 3798,
[2025-07-18T16:22:07.580+0000] {subprocess.py:93} INFO -     "commitOffsets" : 106,
[2025-07-18T16:22:07.580+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:07.581+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:22:07.582+0000] {subprocess.py:93} INFO -     "queryPlanning" : 50,
[2025-07-18T16:22:07.582+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4089,
[2025-07-18T16:22:07.582+0000] {subprocess.py:93} INFO -     "walCommit" : 129
[2025-07-18T16:22:07.583+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:07.583+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:07.584+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:07.584+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:07.584+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:07.584+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:07.585+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:07.585+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.585+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.585+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:07.585+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:07.586+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.587+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.588+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.590+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:07.591+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:07.594+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.594+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.596+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.597+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:07.599+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:22:07.600+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.2445585717779408,
[2025-07-18T16:22:07.602+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:07.603+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:07.604+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:07.605+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:07.606+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:07.606+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:07.608+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:07.609+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:07.611+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:07.612+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:07.612+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:07.612+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/8 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.8.e13d9522-dd25-4b55-aba3-ff9fc50895a2.tmp
[2025-07-18T16:22:07.635+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.7.d421cb93-17bf-4d9b-a570-3df4a0c21f2f.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/7
[2025-07-18T16:22:07.636+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:07.637+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:22:07.637+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:22:07.637+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:07.638+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:04.075Z",
[2025-07-18T16:22:07.638+0000] {subprocess.py:93} INFO -   "batchId" : 7,
[2025-07-18T16:22:07.640+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:07.642+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:22:07.643+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.28224668360146765,
[2025-07-18T16:22:07.643+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:07.644+0000] {subprocess.py:93} INFO -     "addBatch" : 3199,
[2025-07-18T16:22:07.644+0000] {subprocess.py:93} INFO -     "commitOffsets" : 124,
[2025-07-18T16:22:07.644+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:07.644+0000] {subprocess.py:93} INFO -     "latestOffset" : 22,
[2025-07-18T16:22:07.645+0000] {subprocess.py:93} INFO -     "queryPlanning" : 46,
[2025-07-18T16:22:07.645+0000] {subprocess.py:93} INFO -     "triggerExecution" : 3543,
[2025-07-18T16:22:07.646+0000] {subprocess.py:93} INFO -     "walCommit" : 151
[2025-07-18T16:22:07.647+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:07.649+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:07.650+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:07.652+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:07.653+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:07.654+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:07.655+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:07.657+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.660+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.662+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:07.662+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:07.663+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.663+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.664+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.664+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:07.665+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:07.665+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:07.666+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:07.667+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:07.668+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:07.669+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:22:07.669+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.28224668360146765,
[2025-07-18T16:22:07.669+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:07.670+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:07.670+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:07.671+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:07.671+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:07.672+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:07.672+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:07.674+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:07.675+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:07.676+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:07.677+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:07.677+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:07.678+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/8 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.8.9526f6cb-0b00-46cf-a722-dedcc329e22f.tmp
[2025-07-18T16:22:07.679+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/.8.e13d9522-dd25-4b55-aba3-ff9fc50895a2.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/offsets/8
[2025-07-18T16:22:07.679+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1752855727580,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:07.756+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/.8.9526f6cb-0b00-46cf-a722-dedcc329e22f.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/offsets/8
[2025-07-18T16:22:07.764+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MicroBatchExecution: Committed offsets for batch 8. Metadata OffsetSeqMetadata(0,1752855727645,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:07.765+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.765+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.765+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.770+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.776+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.778+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.779+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.779+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.798+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.807+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.822+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.826+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.828+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.869+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.884+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.910+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.911+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.913+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:07.914+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.915+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.916+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.931+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.931+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:07.944+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:07.947+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.951+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:07.958+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:07.986+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:07.987+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Created broadcast 50 from start at <unknown>:0
[2025-07-18T16:22:07.987+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:07.988+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:07.992+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Got job 25 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:07.994+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Final stage: ResultStage 25 (start at <unknown>:0)
[2025-07-18T16:22:07.994+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:07.997+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:08.007+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[103] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:08.010+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:08.011+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:08.011+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:08.012+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:08.013+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.016+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:22:08.017+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 77cb57a6bd53:41117 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.019+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:08.022+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:08.025+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[103] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:08.027+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2025-07-18T16:22:08.032+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:22:08.038+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:08.044+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
[2025-07-18T16:22:08.045+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:08.046+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 77cb57a6bd53:41117 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.047+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Created broadcast 52 from start at <unknown>:0
[2025-07-18T16:22:08.047+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:08.056+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:08.058+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Got job 26 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:08.064+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Final stage: ResultStage 26 (start at <unknown>:0)
[2025-07-18T16:22:08.074+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:08.077+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:08.081+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:08.086+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 27.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:08.088+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:08.089+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.0 MiB)
[2025-07-18T16:22:08.093+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=133 untilOffset=135, for query queryId=c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 batchId=8 taskId=25 partitionId=0
[2025-07-18T16:22:08.104+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 77cb57a6bd53:41117 (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.105+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 25, attempt 0, stage 25.0)
[2025-07-18T16:22:08.113+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:08.114+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:08.114+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-07-18T16:22:08.118+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:22:08.121+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
[2025-07-18T16:22:08.147+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:08.156+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=133 untilOffset=135, for query queryId=ec415088-91b0-434b-9242-7a8791fccbbd batchId=8 taskId=26 partitionId=0
[2025-07-18T16:22:08.165+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 25.0)
[2025-07-18T16:22:08.165+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-d79d2690-0a77-4566-a9ab-161294ee3a12-445867251-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 67708834 nanos.
[2025-07-18T16:22:08.167+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 4813 bytes result sent to driver
[2025-07-18T16:22:08.169+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 137 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:08.172+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2025-07-18T16:22:08.177+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: ResultStage 25 (start at <unknown>:0) finished in 0.176 s
[2025-07-18T16:22:08.185+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:08.185+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2025-07-18T16:22:08.186+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 25 finished: start at <unknown>:0, took 0.195962 s
[2025-07-18T16:22:08.186+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:08.187+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing epoch 8 for query c9c7dd2e-34f6-40c1-860c-fbb9629e8b19 in append mode
[2025-07-18T16:22:08.205+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 26, attempt 0, stage 26.0)
[2025-07-18T16:22:08.306+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v89.metadata.json
[2025-07-18T16:22:08.336+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DataWritingSparkTask: Committed partition 0 (task 26, attempt 0, stage 26.0)
[2025-07-18T16:22:08.338+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-54fc1d49-c87f-45d4-8315-366986cef8be--502894348-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 141816542 nanos.
[2025-07-18T16:22:08.345+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 4760 bytes result sent to driver
[2025-07-18T16:22:08.348+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 234 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:08.349+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-07-18T16:22:08.355+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: ResultStage 26 (start at <unknown>:0) finished in 0.292 s
[2025-07-18T16:22:08.356+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:08.357+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2025-07-18T16:22:08.360+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO DAGScheduler: Job 26 finished: start at <unknown>:0, took 0.305092 s
[2025-07-18T16:22:08.360+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:08.360+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing epoch 8 for query ec415088-91b0-434b-9242-7a8791fccbbd in append mode
[2025-07-18T16:22:08.410+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:08.412+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SnapshotProducer: Committed snapshot 1379664932513305734 (FastAppend)
[2025-07-18T16:22:08.441+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:08.536+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 77cb57a6bd53:41117 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.561+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 77cb57a6bd53:41117 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=1379664932513305734, sequenceNumber=88, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.938698459S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=88}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=881}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3052}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=280837}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO SparkWrite: Committed in 939 ms
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:08.577+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 77cb57a6bd53:41117 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T16:22:08.602+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/8 using temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.8.3251ddb8-a202-4d7f-8c94-60f573b42857.tmp
[2025-07-18T16:22:08.700+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/.8.3251ddb8-a202-4d7f-8c94-60f573b42857.tmp to file:/tmp/checkpoints/reservations/manual__2025-07-18T16:14:30.001261+00:00/commits/8
[2025-07-18T16:22:08.705+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:08.709+0000] {subprocess.py:93} INFO -   "id" : "91de1f1c-31ac-4205-a078-ccc8a3f415c2",
[2025-07-18T16:22:08.711+0000] {subprocess.py:93} INFO -   "runId" : "d7826a15-f048-4c6f-ac0e-1c932a7da334",
[2025-07-18T16:22:08.712+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:08.715+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:06.727Z",
[2025-07-18T16:22:08.718+0000] {subprocess.py:93} INFO -   "batchId" : 8,
[2025-07-18T16:22:08.722+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:08.722+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.5173305742369374,
[2025-07-18T16:22:08.726+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.0141987829614605,
[2025-07-18T16:22:08.728+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:08.728+0000] {subprocess.py:93} INFO -     "addBatch" : 1378,
[2025-07-18T16:22:08.729+0000] {subprocess.py:93} INFO -     "commitOffsets" : 130,
[2025-07-18T16:22:08.733+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:22:08.735+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:22:08.736+0000] {subprocess.py:93} INFO -     "queryPlanning" : 167,
[2025-07-18T16:22:08.738+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1972,
[2025-07-18T16:22:08.739+0000] {subprocess.py:93} INFO -     "walCommit" : 282
[2025-07-18T16:22:08.740+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:08.743+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:08.747+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:08.749+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:08.750+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:08.752+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:08.754+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:08.757+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.758+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.759+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:08.759+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:08.761+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:08.764+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.766+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.768+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:08.768+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:08.768+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:08.768+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:08.768+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:08.769+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:08.770+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.5173305742369374,
[2025-07-18T16:22:08.770+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.0141987829614605,
[2025-07-18T16:22:08.770+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:08.771+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:08.772+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:08.773+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:08.773+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:08.775+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:08.777+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:08.778+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:08.779+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:08.780+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:08.781+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:08.835+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN Tasks: Retrying task after failure: Version 98 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v98.metadata.json
[2025-07-18T16:22:08.847+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 98 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v98.metadata.json
[2025-07-18T16:22:08.849+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:08.856+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:08.860+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:08.862+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:08.863+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:08.869+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:08.874+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:08.877+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:08.884+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:08.886+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:08.891+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:08.897+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:08.911+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:08.916+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:08.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:08.919+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:08.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:08.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:08.923+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:08.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:08.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:08.927+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:08.931+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:08.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:08.932+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:08.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:08.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.934+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.934+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.935+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.935+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.935+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:08.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:08.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:08.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.939+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:08.940+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:08.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:08.942+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:08.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:08.952+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v98.metadata.json
[2025-07-18T16:22:09.289+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SnapshotProducer: Committed snapshot 2180727796700507044 (FastAppend)
[2025-07-18T16:22:09.453+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2180727796700507044, sequenceNumber=97, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT1.043691834S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=97}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=883}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=310402}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855278040, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:09.454+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SparkWrite: Committed in 1045 ms
[2025-07-18T16:22:09.454+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:09.504+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/8 using temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.8.1e83aa20-8b3d-415b-8f5c-7287cd6a92fd.tmp
[2025-07-18T16:22:09.517+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v99.metadata.json
[2025-07-18T16:22:09.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/.8.1e83aa20-8b3d-415b-8f5c-7287cd6a92fd.tmp to file:/tmp/checkpoints/checkins/manual__2025-07-18T16:14:30.001261+00:00/commits/8
[2025-07-18T16:22:09.607+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:09.609+0000] {subprocess.py:93} INFO -   "id" : "c9c7dd2e-34f6-40c1-860c-fbb9629e8b19",
[2025-07-18T16:22:09.610+0000] {subprocess.py:93} INFO -   "runId" : "7d171787-b562-405b-afd9-54b53d58d80e",
[2025-07-18T16:22:09.612+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:09.612+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:07.577Z",
[2025-07-18T16:22:09.612+0000] {subprocess.py:93} INFO -   "batchId" : 8,
[2025-07-18T16:22:09.612+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.48899755501222497,
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.991571641051066,
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO -     "addBatch" : 1673,
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO -     "commitOffsets" : 136,
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:09.614+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T16:22:09.615+0000] {subprocess.py:93} INFO -     "queryPlanning" : 105,
[2025-07-18T16:22:09.618+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2017,
[2025-07-18T16:22:09.618+0000] {subprocess.py:93} INFO -     "walCommit" : 95
[2025-07-18T16:22:09.619+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:09.619+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:09.625+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:09.627+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:09.628+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:09.630+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:09.632+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:09.636+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.638+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.638+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:09.641+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:09.644+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:09.646+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.648+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.648+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:09.648+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:09.649+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:09.649+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:09.649+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:09.649+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:09.649+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.48899755501222497,
[2025-07-18T16:22:09.649+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.991571641051066,
[2025-07-18T16:22:09.650+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:09.651+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:09.652+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:09.652+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:09.653+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:09.653+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:09.653+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:09.654+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:09.654+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:09.655+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:09.655+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:09.702+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SnapshotProducer: Committed snapshot 4791302983079194721 (FastAppend)
[2025-07-18T16:22:09.828+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 WARN SnapshotProducer: Failed to load committed snapshot, skipping manifest clean-up
[2025-07-18T16:22:09.829+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 WARN SnapshotProducer: Failed to notify listeners
[2025-07-18T16:22:09.829+0000] {subprocess.py:93} INFO - java.lang.NullPointerException: Cannot invoke "org.apache.iceberg.Snapshot.sequenceNumber()" because "snapshot" is null
[2025-07-18T16:22:09.830+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.FastAppend.updateEvent(FastAppend.java:174)
[2025-07-18T16:22:09.830+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.notifyListeners(SnapshotProducer.java:449)
[2025-07-18T16:22:09.831+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:441)
[2025-07-18T16:22:09.831+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:09.832+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:09.835+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:09.838+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:09.841+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:09.841+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:09.845+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:09.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:09.848+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:09.848+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:09.848+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:09.850+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.866+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:09.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:09.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:09.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:09.880+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.884+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:09.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:09.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:09.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:09.887+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.888+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.888+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:09.889+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:09.890+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.891+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:09.892+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:09.894+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO SparkWrite: Committed in 1385 ms
[2025-07-18T16:22:09.896+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:09.919+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/8 using temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.8.6bb37824-3471-4a23-b8ca-e7a9d9b5d1ec.tmp
[2025-07-18T16:22:10.054+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/.8.6bb37824-3471-4a23-b8ca-e7a9d9b5d1ec.tmp to file:/tmp/checkpoints/feedback/manual__2025-07-18T16:14:30.001261+00:00/commits/8
[2025-07-18T16:22:10.065+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:10.065+0000] {subprocess.py:93} INFO -   "id" : "ec415088-91b0-434b-9242-7a8791fccbbd",
[2025-07-18T16:22:10.069+0000] {subprocess.py:93} INFO -   "runId" : "125cb6b4-915e-4720-aa51-b7c4bad8f8f7",
[2025-07-18T16:22:10.071+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:10.073+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:07.622Z",
[2025-07-18T16:22:10.076+0000] {subprocess.py:93} INFO -   "batchId" : 8,
[2025-07-18T16:22:10.076+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.563856780377784,
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.8237232289950577,
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -     "addBatch" : 2020,
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -     "commitOffsets" : 217,
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -     "latestOffset" : 23,
[2025-07-18T16:22:10.077+0000] {subprocess.py:93} INFO -     "queryPlanning" : 53,
[2025-07-18T16:22:10.078+0000] {subprocess.py:93} INFO -     "triggerExecution" : 2428,
[2025-07-18T16:22:10.079+0000] {subprocess.py:93} INFO -     "walCommit" : 110
[2025-07-18T16:22:10.083+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:10.086+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:10.087+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:10.089+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:10.097+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:10.097+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:10.098+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:10.098+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.098+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.099+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:10.099+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:10.103+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:10.105+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.105+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.106+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:10.107+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:10.108+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:10.110+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.113+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.114+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:10.118+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.563856780377784,
[2025-07-18T16:22:10.120+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.8237232289950577,
[2025-07-18T16:22:10.123+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:10.129+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:10.130+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:10.135+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:10.138+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:10.142+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:10.145+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:10.149+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:10.150+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:10.154+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:10.154+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:13.203+0000] {subprocess.py:93} INFO - 25/07/18 16:22:13 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:13.209+0000] {subprocess.py:93} INFO - 25/07/18 16:22:13 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:13.213+0000] {subprocess.py:93} INFO - 25/07/18 16:22:13 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:18.704+0000] {subprocess.py:93} INFO - 25/07/18 16:22:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:19.603+0000] {subprocess.py:93} INFO - 25/07/18 16:22:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:20.078+0000] {subprocess.py:93} INFO - 25/07/18 16:22:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:20.375+0000] {subprocess.py:93} INFO - 25/07/18 16:22:20 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 77cb57a6bd53:41117 in memory (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T16:22:20.383+0000] {subprocess.py:93} INFO - 25/07/18 16:22:20 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 77cb57a6bd53:41117 in memory (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:22:28.803+0000] {subprocess.py:93} INFO - 25/07/18 16:22:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:29.613+0000] {subprocess.py:93} INFO - 25/07/18 16:22:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:30.093+0000] {subprocess.py:93} INFO - 25/07/18 16:22:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.823+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.838+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.846+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:55.700+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:58.853+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:02.267+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.170+0000] {subprocess.py:93} INFO - 25/07/18 16:23:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.336+0000] {subprocess.py:93} INFO - 25/07/18 16:23:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.357+0000] {subprocess.py:93} INFO - 25/07/18 16:23:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:12.740+0000] {subprocess.py:93} INFO - 25/07/18 16:23:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:13.842+0000] {subprocess.py:93} INFO - 25/07/18 16:23:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:14.265+0000] {subprocess.py:93} INFO - 25/07/18 16:23:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:22.778+0000] {subprocess.py:93} INFO - 25/07/18 16:23:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:23.791+0000] {subprocess.py:93} INFO - 25/07/18 16:23:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:24.306+0000] {subprocess.py:93} INFO - 25/07/18 16:23:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:32.991+0000] {subprocess.py:93} INFO - 25/07/18 16:23:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:34.733+0000] {subprocess.py:93} INFO - 25/07/18 16:23:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:37.238+0000] {subprocess.py:93} INFO - 25/07/18 16:23:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:51.532+0000] {subprocess.py:93} INFO - 25/07/18 16:23:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:58.215+0000] {subprocess.py:93} INFO - 25/07/18 16:23:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:06.829+0000] {subprocess.py:93} INFO - 25/07/18 16:23:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:11.526+0000] {subprocess.py:93} INFO - 25/07/18 16:23:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:06.112+0000] {subprocess.py:93} INFO - 25/07/18 16:23:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:08.803+0000] {subprocess.py:93} INFO - 25/07/18 16:24:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:10.209+0000] {subprocess.py:93} INFO - 25/07/18 16:24:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:12.883+0000] {subprocess.py:93} INFO - 25/07/18 16:24:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:33.582+0000] {subprocess.py:93} INFO - 25/07/18 16:25:20 INFO NetworkClient: [AdminClient clientId=adminclient-2] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:33.763+0000] {subprocess.py:93} INFO - 25/07/18 16:25:20 INFO NetworkClient: [AdminClient clientId=adminclient-3] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:33.858+0000] {subprocess.py:93} INFO - 25/07/18 16:25:22 INFO NetworkClient: [AdminClient clientId=adminclient-3] Cancelled in-flight METADATA request with correlation id 108629 due to node 1 being disconnected (elapsed time since creation: 18267ms, elapsed time since send: 18267ms, request timeout: 17108ms)
[2025-07-18T16:25:33.881+0000] {subprocess.py:93} INFO - 25/07/18 16:25:23 INFO NetworkClient: [AdminClient clientId=adminclient-2] Cancelled in-flight METADATA request with correlation id 108048 due to node 1 being disconnected (elapsed time since creation: 17821ms, elapsed time since send: 17821ms, request timeout: 17252ms)
[2025-07-18T16:25:33.883+0000] {subprocess.py:93} INFO - 25/07/18 16:25:32 INFO AdminMetadataManager: [AdminClient clientId=adminclient-3] Metadata update failed
[2025-07-18T16:25:33.883+0000] {subprocess.py:93} INFO - org.apache.kafka.common.errors.TimeoutException: Call(callName=fetchMetadata, deadlineMs=1752855911555, tries=1, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 1 attempt(s)
[2025-07-18T16:25:33.883+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled fetchMetadata request with correlation id 108629 due to node 1 being disconnected
[2025-07-18T16:25:33.883+0000] {subprocess.py:93} INFO - 25/07/18 16:25:32 INFO AdminMetadataManager: [AdminClient clientId=adminclient-2] Metadata update failed
[2025-07-18T16:25:33.884+0000] {subprocess.py:93} INFO - org.apache.kafka.common.errors.TimeoutException: Call(callName=fetchMetadata, deadlineMs=1752855911569, tries=1, nextAllowedTryMs=1752855924123) timed out at 1752855924023 after 1 attempt(s)
[2025-07-18T16:25:33.900+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled fetchMetadata request with correlation id 108048 due to node 1 being disconnected
[2025-07-18T16:25:33.927+0000] {subprocess.py:93} INFO - 25/07/18 16:25:32 INFO NetworkClient: [AdminClient clientId=adminclient-1] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:33.952+0000] {subprocess.py:93} INFO - 25/07/18 16:25:32 INFO NetworkClient: [AdminClient clientId=adminclient-1] Cancelled in-flight METADATA request with correlation id 108255 due to node 1 being disconnected (elapsed time since creation: 49626ms, elapsed time since send: 49626ms, request timeout: 30000ms)
[2025-07-18T16:25:33.963+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 INFO AdminMetadataManager: [AdminClient clientId=adminclient-1] Metadata update failed
[2025-07-18T16:25:33.980+0000] {subprocess.py:93} INFO - org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: fetchMetadata
[2025-07-18T16:25:34.027+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.035+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855931474, tries=4, nextAllowedTryMs=1752855932696) timed out at 1752855932596 after 4 attempt(s)
[2025-07-18T16:25:34.039+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.039+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.055+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.065+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.079+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.081+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.083+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.083+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.084+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.084+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.090+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.091+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.092+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.092+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.096+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.120+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.121+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.123+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.140+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.149+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.152+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.164+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.225+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.237+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.274+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.305+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.315+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.318+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.326+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855931474, tries=4, nextAllowedTryMs=1752855932696) timed out at 1752855932596 after 4 attempt(s)
[2025-07-18T16:25:34.338+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 108255 due to node 1 being disconnected
[2025-07-18T16:25:34.359+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.359+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: metadata
[2025-07-18T16:25:34.367+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.369+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.372+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.432+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.439+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.454+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.455+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.456+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.457+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.460+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.462+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.465+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.467+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.470+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.472+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.477+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.488+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.491+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.519+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.524+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.538+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.539+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.539+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.540+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.546+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.560+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.583+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.595+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.602+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.609+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.614+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.623+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.639+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting for a node assignment. Call: metadata
[2025-07-18T16:25:34.662+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.669+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: metadata
[2025-07-18T16:25:34.678+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.682+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.699+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.711+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.713+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.719+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.725+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.727+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.733+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.737+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.754+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.755+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.757+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.765+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.769+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.775+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.789+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.798+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.802+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.804+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.812+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.827+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.829+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.833+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.839+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.850+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.852+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.858+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.861+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.864+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.865+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.883+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.890+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.895+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.896+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.897+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.898+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.900+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: metadata
[2025-07-18T16:25:34.931+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-07-18T16:25:34.934+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-07-18T16:25:34.982+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:34.985+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:34.990+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:34.994+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:34.996+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:34.998+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.012+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.013+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.015+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.017+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.019+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.043+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.052+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.063+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.071+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.075+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.079+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.081+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.084+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.088+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.094+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.096+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.102+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.107+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.109+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.114+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.153+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.179+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.180+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.180+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.182+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.183+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.186+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.198+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.201+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.204+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.205+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.205+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.208+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.210+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.211+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.213+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.215+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.216+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.216+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.226+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.232+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.237+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.247+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.248+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.259+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.263+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.263+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.263+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.264+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.264+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.265+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.270+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.273+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.273+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.285+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.287+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.288+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.288+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.289+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.290+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.291+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.291+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.292+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.293+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.295+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.296+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.296+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.297+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.298+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.298+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.299+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.301+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.301+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.304+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.305+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.307+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.308+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.309+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.311+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.313+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.314+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.317+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.328+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.330+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.331+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.333+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.335+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.336+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.337+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.338+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.339+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.343+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.345+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.348+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.350+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.353+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.354+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.354+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.357+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.358+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.361+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.362+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.363+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.366+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.370+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.371+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.434+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.438+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.442+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.444+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.447+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.448+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.456+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.457+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.459+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.468+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.469+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.469+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.469+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.474+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.476+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.477+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.479+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.481+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.481+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.483+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.484+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.485+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.486+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.487+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.488+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.489+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.491+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.492+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.493+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.494+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.495+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.495+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.496+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.497+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.498+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.498+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
[2025-07-18T16:25:35.499+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.499+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.499+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.499+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.500+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935102
[2025-07-18T16:25:35.500+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.501+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.512+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935102
[2025-07-18T16:25:35.517+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:35.523+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:35.527+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.528+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.538+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.544+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.544+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.552+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.553+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.561+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.577+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.580+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.588+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.591+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.595+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.595+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.596+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.596+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.600+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.601+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.605+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.606+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.608+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.613+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.613+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.613+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.614+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.616+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.618+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.618+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.618+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.629+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.629+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.630+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.631+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.637+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.638+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.638+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.639+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.640+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.641+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.643+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.647+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.648+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.648+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.648+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.649+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.649+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.650+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.651+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.653+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.654+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.655+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.656+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.658+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.659+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.660+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.661+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.662+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.663+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.664+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.664+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.664+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.667+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.667+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.669+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.671+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.672+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.673+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.678+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.680+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.687+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.688+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.689+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.719+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:35.724+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:35.727+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:35.738+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935186
[2025-07-18T16:25:35.741+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:35.753+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:35.757+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:44.857+0000] {local_task_job_runner.py:294} WARNING - State of this instance has been externally set to scheduled. Terminating instance.
[2025-07-18T16:25:44.950+0000] {process_utils.py:131} INFO - Sending 15 to group 1250. PIDs of all processes in the group: [1251, 1250]
[2025-07-18T16:25:44.952+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 1250
[2025-07-18T16:25:44.961+0000] {taskinstance.py:1632} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-07-18T16:25:44.969+0000] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2025-07-18T16:25:45.831+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/subprocess.py", line 91, in run_command
    for raw_line in iter(self.sub_process.stdout.readline, b""):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1634, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2025-07-18T16:25:46.056+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=restaurant_pipeline, task_id=stream_to_bronze, execution_date=20250718T161430, start_date=20250718T161435, end_date=20250718T162545
[2025-07-18T16:25:46.623+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 180 for task stream_to_bronze (Task received SIGTERM signal; 1250)
[2025-07-18T16:25:46.815+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1251, status='terminated', started='16:14:35') (1251) terminated with exit code None
[2025-07-18T16:25:46.820+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1250, status='terminated', exitcode=1, started='16:14:35') (1250) terminated with exit code 1
