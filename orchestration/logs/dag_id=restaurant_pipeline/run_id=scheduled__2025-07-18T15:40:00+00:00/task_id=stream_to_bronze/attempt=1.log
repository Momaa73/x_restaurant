[2025-07-18T15:42:05.206+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T15:40:00+00:00 [queued]>
[2025-07-18T15:42:05.210+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T15:40:00+00:00 [queued]>
[2025-07-18T15:42:05.210+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-07-18T15:42:05.216+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): stream_to_bronze> on 2025-07-18 15:40:00+00:00
[2025-07-18T15:42:05.219+0000] {standard_task_runner.py:57} INFO - Started process 618 to run task
[2025-07-18T15:42:05.221+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'restaurant_pipeline', 'stream_to_bronze', 'scheduled__2025-07-18T15:40:00+00:00', '--job-id', '126', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_pipeline.py', '--cfg-path', '/tmp/tmp0_8gd72y']
[2025-07-18T15:42:05.223+0000] {standard_task_runner.py:85} INFO - Job 126: Subtask stream_to_bronze
[2025-07-18T15:42:05.253+0000] {task_command.py:416} INFO - Running <TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T15:40:00+00:00 [running]> on host 9bcfb43e0ab7
[2025-07-18T15:42:05.291+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='moran' AIRFLOW_CTX_DAG_ID='restaurant_pipeline' AIRFLOW_CTX_TASK_ID='stream_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-07-18T15:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T15:40:00+00:00'
[2025-07-18T15:42:05.292+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-07-18T15:42:05.293+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec spark-iceberg spark-submit /home/iceberg/spark/stream_to_bronze.py']
[2025-07-18T15:42:05.297+0000] {subprocess.py:86} INFO - Output:
[2025-07-18T15:42:07.568+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SparkContext: Running Spark version 3.5.6
[2025-07-18T15:42:07.585+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T15:42:07.585+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SparkContext: Java version 17.0.15
[2025-07-18T15:42:07.614+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO ResourceUtils: ==============================================================
[2025-07-18T15:42:07.615+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-18T15:42:07.618+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO ResourceUtils: ==============================================================
[2025-07-18T15:42:07.619+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SparkContext: Submitted application: StreamToBronze
[2025-07-18T15:42:07.637+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-18T15:42:07.640+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO ResourceProfile: Limiting resource is cpu
[2025-07-18T15:42:07.643+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-18T15:42:07.710+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SecurityManager: Changing view acls to: root,spark
[2025-07-18T15:42:07.714+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SecurityManager: Changing modify acls to: root,spark
[2025-07-18T15:42:07.717+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SecurityManager: Changing view acls groups to:
[2025-07-18T15:42:07.718+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SecurityManager: Changing modify acls groups to:
[2025-07-18T15:42:07.720+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
[2025-07-18T15:42:07.825+0000] {subprocess.py:93} INFO - 25/07/18 15:42:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-18T15:42:08.278+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Utils: Successfully started service 'sparkDriver' on port 41615.
[2025-07-18T15:42:08.353+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO SparkEnv: Registering MapOutputTracker
[2025-07-18T15:42:08.444+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-18T15:42:08.457+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-18T15:42:08.459+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-18T15:42:08.464+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-18T15:42:08.483+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c4c3294c-064f-4b92-b456-6f4939f8d621
[2025-07-18T15:42:08.488+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-18T15:42:08.501+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-18T15:42:08.607+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-18T15:42:08.660+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-07-18T15:42:08.734+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Executor: Starting executor ID driver on host 77cb57a6bd53
[2025-07-18T15:42:08.738+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T15:42:08.739+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Executor: Java version 17.0.15
[2025-07-18T15:42:08.747+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-18T15:42:08.748+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@756ecb14 for default.
[2025-07-18T15:42:08.765+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38973.
[2025-07-18T15:42:08.765+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO NettyBlockTransferService: Server created on 77cb57a6bd53:38973
[2025-07-18T15:42:08.774+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-18T15:42:08.779+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 77cb57a6bd53, 38973, None)
[2025-07-18T15:42:08.782+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO BlockManagerMasterEndpoint: Registering block manager 77cb57a6bd53:38973 with 434.4 MiB RAM, BlockManagerId(driver, 77cb57a6bd53, 38973, None)
[2025-07-18T15:42:08.785+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 77cb57a6bd53, 38973, None)
[2025-07-18T15:42:08.785+0000] {subprocess.py:93} INFO - 25/07/18 15:42:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 77cb57a6bd53, 38973, None)
[2025-07-18T15:42:09.091+0000] {subprocess.py:93} INFO - 25/07/18 15:42:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-18T15:42:09.095+0000] {subprocess.py:93} INFO - 25/07/18 15:42:09 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-07-18T15:42:10.055+0000] {subprocess.py:93} INFO - 25/07/18 15:42:10 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-07-18T15:42:10.070+0000] {subprocess.py:93} INFO - 25/07/18 15:42:10 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-07-18T15:42:10.070+0000] {subprocess.py:93} INFO - 25/07/18 15:42:10 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-07-18T15:42:10.933+0000] {subprocess.py:93} INFO - 25/07/18 15:42:10 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:10.965+0000] {subprocess.py:93} INFO - 25/07/18 15:42:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-07-18T15:42:10.996+0000] {subprocess.py:93} INFO - 25/07/18 15:42:10 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/reservations resolved to file:/tmp/checkpoints/reservations.
[2025-07-18T15:42:10.996+0000] {subprocess.py:93} INFO - 25/07/18 15:42:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:42:11.066+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Starting [id = 0314df7c-5598-4928-8d91-374ee67989d1, runId = af558342-e931-459f-b082-cde32c42e687]. Use file:/tmp/checkpoints/reservations to store the query checkpoint.
[2025-07-18T15:42:11.073+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@54f1fef4] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@7471fdea]
[2025-07-18T15:42:11.113+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22
[2025-07-18T15:42:11.122+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: Getting latest batch 22
[2025-07-18T15:42:11.237+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22
[2025-07-18T15:42:11.238+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: Getting latest batch 22
[2025-07-18T15:42:11.247+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO CommitLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21
[2025-07-18T15:42:11.248+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO CommitLog: Getting latest batch 21
[2025-07-18T15:42:11.249+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.252+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/checkins resolved to file:/tmp/checkpoints/checkins.
[2025-07-18T15:42:11.252+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:42:11.252+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Resuming at batch 22 with committed offsets {KafkaV2[Subscribe[reservations]]: {"reservations":{"0":67}}} and available offsets {KafkaV2[Subscribe[reservations]]: {"reservations":{"0":69}}}
[2025-07-18T15:42:11.252+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[reservations]]: {"reservations":{"0":67}}}
[2025-07-18T15:42:11.258+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Starting [id = a9a9b3af-d3c2-4704-81e0-4163831ae683, runId = 01143f57-ab8a-4afe-9039-32fa7b4eca3f]. Use file:/tmp/checkpoints/checkins to store the query checkpoint.
[2025-07-18T15:42:11.260+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1ed0e249] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@10170e3b]
[2025-07-18T15:42:11.273+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
[2025-07-18T15:42:11.278+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: Getting latest batch 24
[2025-07-18T15:42:11.279+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24
[2025-07-18T15:42:11.279+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: Getting latest batch 24
[2025-07-18T15:42:11.286+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO CommitLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23
[2025-07-18T15:42:11.287+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO CommitLog: Getting latest batch 23
[2025-07-18T15:42:11.287+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Resuming at batch 24 with committed offsets {KafkaV2[Subscribe[checkins]]: {"checkins":{"0":67}}} and available offsets {KafkaV2[Subscribe[checkins]]: {"checkins":{"0":69}}}
[2025-07-18T15:42:11.287+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[checkins]]: {"checkins":{"0":67}}}
[2025-07-18T15:42:11.368+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.368+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/feedback resolved to file:/tmp/checkpoints/feedback.
[2025-07-18T15:42:11.369+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T15:42:11.376+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Starting [id = d3aff090-24bc-4a1c-938f-fc839231598c, runId = 1d90f249-7c40-4a15-bd2f-2ed3427fbacd]. Use file:/tmp/checkpoints/feedback to store the query checkpoint.
[2025-07-18T15:42:11.377+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@320fe8d7] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3cc8113e]
[2025-07-18T15:42:11.379+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21
[2025-07-18T15:42:11.379+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: Getting latest batch 21
[2025-07-18T15:42:11.382+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21
[2025-07-18T15:42:11.382+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO OffsetSeqLog: Getting latest batch 21
[2025-07-18T15:42:11.390+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO CommitLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20
[2025-07-18T15:42:11.390+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO CommitLog: Getting latest batch 20
[2025-07-18T15:42:11.391+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Resuming at batch 21 with committed offsets {KafkaV2[Subscribe[feedback]]: {"feedback":{"0":66}}} and available offsets {KafkaV2[Subscribe[feedback]]: {"feedback":{"0":67}}}
[2025-07-18T15:42:11.391+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[feedback]]: {"feedback":{"0":66}}}
[2025-07-18T15:42:11.519+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.520+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.521+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.522+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.523+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.525+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.526+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.526+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.527+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.669+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.671+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.672+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.721+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.721+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.721+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.784+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.784+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.786+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.788+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.801+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.806+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.819+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.821+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.824+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.829+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.830+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.830+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.830+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.830+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.830+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.874+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.877+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.878+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:11.880+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.882+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.883+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.883+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:11.884+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.889+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.896+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.897+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.899+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.902+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:11.910+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:11.921+0000] {subprocess.py:93} INFO - 25/07/18 15:42:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:12.582+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO CodeGenerator: Code generated in 317.685459 ms
[2025-07-18T15:42:12.583+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO CodeGenerator: Code generated in 319.328792 ms
[2025-07-18T15:42:12.585+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO CodeGenerator: Code generated in 314.222167 ms
[2025-07-18T15:42:12.703+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:42:12.704+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:42:12.704+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T15:42:12.743+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:42:12.744+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:42:12.744+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:42:12.744+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.4 MiB)
[2025-07-18T15:42:12.755+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:42:12.756+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Created broadcast 1 from start at <unknown>:0
[2025-07-18T15:42:12.757+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:42:12.757+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Created broadcast 0 from start at <unknown>:0
[2025-07-18T15:42:12.758+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:42:12.758+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 21, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:42:12.758+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Created broadcast 2 from start at <unknown>:0
[2025-07-18T15:42:12.758+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:42:12.776+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:42:12.776+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:42:12.776+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:42:12.789+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:42:12.790+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-07-18T15:42:12.790+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:42:12.791+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:42:12.794+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:42:12.857+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T15:42:12.865+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.2 MiB)
[2025-07-18T15:42:12.867+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 77cb57a6bd53:38973 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:42:12.875+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:42:12.888+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:42:12.917+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-18T15:42:12.938+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:42:12.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-07-18T15:42:12.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:42:12.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:42:12.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:42:12.944+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.6 KiB, free 434.2 MiB)
[2025-07-18T15:42:12.961+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:42:12.961+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:42:12.961+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:42:12.961+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:42:12.962+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-18T15:42:12.990+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:42:12.995+0000] {subprocess.py:93} INFO - 25/07/18 15:42:12 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:42:13.002+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
[2025-07-18T15:42:13.002+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:42:13.003+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:42:13.003+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:42:13.006+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T15:42:13.007+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:42:13.029+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T15:42:13.030+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:42:13.031+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-18T15:42:13.033+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-18T15:42:13.033+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:42:13.035+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:42:13.036+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-18T15:42:13.041+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:42:13.041+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-18T15:42:13.183+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 28.799583 ms
[2025-07-18T15:42:13.187+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 28.816542 ms
[2025-07-18T15:42:13.188+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 28.384125 ms
[2025-07-18T15:42:13.217+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 27.886333 ms
[2025-07-18T15:42:13.232+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 43.031542 ms
[2025-07-18T15:42:13.233+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 45.71525 ms
[2025-07-18T15:42:13.311+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:42:13.313+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:42:13.313+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:42:13.518+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=67 untilOffset=69, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=24 taskId=2 partitionId=0
[2025-07-18T15:42:13.518+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=66 untilOffset=67, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=21 taskId=0 partitionId=0
[2025-07-18T15:42:13.523+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=67 untilOffset=69, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=22 taskId=1 partitionId=0
[2025-07-18T15:42:13.556+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 13.227375 ms
[2025-07-18T15:42:13.590+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO CodeGenerator: Code generated in 16.854583 ms
[2025-07-18T15:42:13.668+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:42:13.668+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:42:13.669+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:42:13.670+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:42:13.675+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:42:13.675+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:42:13.675+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:42:13.675+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:42:13.676+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:42:13.677+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:42:13.678+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:42:13.678+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:42:13.678+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:42:13.678+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:42:13.678+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:42:13.678+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:42:13.678+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:42:13.679+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:42:13.680+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:42:13.680+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:42:13.680+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:42:13.680+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:42:13.680+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:42:13.680+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:42:13.681+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:42:13.682+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3
[2025-07-18T15:42:13.683+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:42:13.684+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:42:13.685+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:42:13.686+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:42:13.687+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:42:13.688+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:42:13.689+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:42:13.690+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T15:42:13.691+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T15:42:13.692+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T15:42:13.693+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:42:13.694+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:42:13.695+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:42:13.696+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:42:13.697+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:42:13.698+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:42:13.699+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:42:13.699+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:42:13.699+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:42:13.699+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:42:13.699+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T15:42:13.699+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:42:13.746+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:42:13.747+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:42:13.747+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka startTimeMs: 1752853333742
[2025-07-18T15:42:13.749+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:42:13.749+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:42:13.750+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka startTimeMs: 1752853333743
[2025-07-18T15:42:13.750+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:42:13.750+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:42:13.750+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO AppInfoParser: Kafka startTimeMs: 1752853333743
[2025-07-18T15:42:13.750+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Assigned to partition(s): reservations-0
[2025-07-18T15:42:13.750+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Assigned to partition(s): feedback-0
[2025-07-18T15:42:13.750+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Assigned to partition(s): checkins-0
[2025-07-18T15:42:13.755+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 67 for partition checkins-0
[2025-07-18T15:42:13.756+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 66 for partition feedback-0
[2025-07-18T15:42:13.756+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 67 for partition reservations-0
[2025-07-18T15:42:13.915+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:42:13.916+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:42:13.916+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T15:42:13.942+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:42:13.943+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:42:13.944+0000] {subprocess.py:93} INFO - 25/07/18 15:42:13 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:42:14.450+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=69, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=69, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:42:14.451+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=69, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:42:14.603+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T15:42:14.605+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T15:42:14.605+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T15:42:14.897+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T15:42:14.897+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T15:42:14.898+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T15:42:14.899+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 2 records through 1 polls (polled  out 2 records), taking 694209334 nanos, during time span of 1141929292 nanos.
[2025-07-18T15:42:14.899+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 3 records), taking 694482709 nanos, during time span of 1142102584 nanos.
[2025-07-18T15:42:14.899+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 1 polls (polled  out 2 records), taking 694391709 nanos, during time span of 1142246292 nanos.
[2025-07-18T15:42:14.916+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4903 bytes result sent to driver
[2025-07-18T15:42:14.916+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4858 bytes result sent to driver
[2025-07-18T15:42:14.916+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4780 bytes result sent to driver
[2025-07-18T15:42:14.926+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1891 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:42:14.929+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-18T15:42:14.930+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1963 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:42:14.930+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-18T15:42:14.930+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1923 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:42:14.931+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-18T15:42:14.935+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 1.931 s
[2025-07-18T15:42:14.938+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:42:14.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-18T15:42:14.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 2.136 s
[2025-07-18T15:42:14.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:42:14.939+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-18T15:42:14.941+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 2.166358 s
[2025-07-18T15:42:14.942+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 2.166311 s
[2025-07-18T15:42:14.947+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 2.007 s
[2025-07-18T15:42:14.948+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:42:14.948+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-18T15:42:14.948+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 2.168195 s
[2025-07-18T15:42:14.949+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:42:14.949+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SparkWrite: Committing epoch 24 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:42:14.949+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:42:14.949+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SparkWrite: Committing epoch 21 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:42:14.949+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:42:14.949+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SparkWrite: Committing epoch 22 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:42:14.984+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:42:14.985+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:14.985+0000] {subprocess.py:93} INFO - 25/07/18 15:42:14 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:42:15.123+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 77cb57a6bd53:38973 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:42:15.127+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:42:15.134+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:42:15.264+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v23.metadata.json
[2025-07-18T15:42:15.264+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v26.metadata.json
[2025-07-18T15:42:15.264+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v24.metadata.json
[2025-07-18T15:42:15.322+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SnapshotProducer: Committed snapshot 6458529827389566121 (FastAppend)
[2025-07-18T15:42:15.322+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SnapshotProducer: Committed snapshot 8948082618437397135 (FastAppend)
[2025-07-18T15:42:15.322+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SnapshotProducer: Committed snapshot 7311416523552213174 (FastAppend)
[2025-07-18T15:42:15.379+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7311416523552213174, sequenceNumber=22, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.382713751S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=22}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=67}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2892}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=66072}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:42:15.380+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=8948082618437397135, sequenceNumber=23, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.3846455S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=23}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=69}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3034}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=69662}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:42:15.381+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Committed in 393 ms
[2025-07-18T15:42:15.381+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Committed in 393 ms
[2025-07-18T15:42:15.381+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 21, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:42:15.381+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:42:15.387+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=6458529827389566121, sequenceNumber=25, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.395005583S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=25}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=69}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2893}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=74373}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:42:15.388+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Committed in 399 ms
[2025-07-18T15:42:15.389+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:42:15.405+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/24 using temp file file:/tmp/checkpoints/checkins/commits/.24.248a9817-d2bd-4b08-8eb7-80fe3333080b.tmp
[2025-07-18T15:42:15.405+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/22 using temp file file:/tmp/checkpoints/reservations/commits/.22.77967884-9ab4-4052-b29f-9dc1c5217dc1.tmp
[2025-07-18T15:42:15.405+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/21 using temp file file:/tmp/checkpoints/feedback/commits/.21.41e522f0-068c-4549-9061-fb739c020e24.tmp
[2025-07-18T15:42:15.458+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.22.77967884-9ab4-4052-b29f-9dc1c5217dc1.tmp to file:/tmp/checkpoints/reservations/commits/22
[2025-07-18T15:42:15.463+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.21.41e522f0-068c-4549-9061-fb739c020e24.tmp to file:/tmp/checkpoints/feedback/commits/21
[2025-07-18T15:42:15.464+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.24.248a9817-d2bd-4b08-8eb7-80fe3333080b.tmp to file:/tmp/checkpoints/checkins/commits/24
[2025-07-18T15:42:15.496+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:42:15.497+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:42:15.498+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:42:15.499+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:42:15.499+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:42:11.090Z",
[2025-07-18T15:42:15.499+0000] {subprocess.py:93} INFO -   "batchId" : 22,
[2025-07-18T15:42:15.501+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:42:15.501+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:42:15.501+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.4579803068468056,
[2025-07-18T15:42:15.501+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:42:15.502+0000] {subprocess.py:93} INFO -     "addBatch" : 3639,
[2025-07-18T15:42:15.502+0000] {subprocess.py:93} INFO -     "commitOffsets" : 73,
[2025-07-18T15:42:15.503+0000] {subprocess.py:93} INFO -     "getBatch" : 13,
[2025-07-18T15:42:15.503+0000] {subprocess.py:93} INFO -     "queryPlanning" : 464,
[2025-07-18T15:42:15.503+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4366
[2025-07-18T15:42:15.504+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:42:15.504+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:42:15.505+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:42:15.505+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:42:15.506+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:42:15.506+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:42:15.507+0000] {subprocess.py:93} INFO -         "0" : 67
[2025-07-18T15:42:15.507+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:15.507+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:15.507+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:42:15.507+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:42:15.508+0000] {subprocess.py:93} INFO -         "0" : 69
[2025-07-18T15:42:15.508+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:15.508+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:15.508+0000] {subprocess.py:93} INFO -     "latestOffset" : null,
[2025-07-18T15:42:15.508+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:42:15.508+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:42:15.508+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.4579803068468056
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:42:15.509+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:42:11.261Z",
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "batchId" : 24,
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.47619047619047616,
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -     "addBatch" : 3643,
[2025-07-18T15:42:15.510+0000] {subprocess.py:93} INFO -     "commitOffsets" : 77,
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -     "queryPlanning" : 454,
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4200
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -         "0" : 67
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:15.511+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -         "0" : 69
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -     "latestOffset" : null,
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.47619047619047616
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:42:15.512+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:42:11.376Z",
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "batchId" : 21,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.2448579823702253,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -     "addBatch" : 3641,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -     "commitOffsets" : 76,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:42:15.513+0000] {subprocess.py:93} INFO -     "queryPlanning" : 353,
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4084
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -         "0" : 66
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:42:15.514+0000] {subprocess.py:93} INFO -         "0" : 67
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -     "latestOffset" : null,
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.2448579823702253
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:42:15.515+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:42:15.516+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:42:15.516+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:42:15.516+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:42:15.516+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:42:15.516+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:42:15.516+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:42:15.516+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:42:15.517+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:42:15.517+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:42:15.517+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:42:15.517+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:42:15.517+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:42:15.517+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:42:15.517+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:42:15.518+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:42:15.518+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:42:15.518+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:42:15.518+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:42:15.518+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:42:15.519+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:42:15.519+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:42:15.519+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:42:15.519+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:42:15.520+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:42:15.521+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:42:15.521+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:42:15.521+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:42:15.521+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:42:15.522+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:42:15.522+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:42:15.522+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:42:15.523+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:42:15.523+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:42:15.523+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:42:15.523+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:42:15.524+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:42:15.525+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:42:15.525+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:42:15.525+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:42:15.526+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:42:15.527+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:42:15.527+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:42:15.528+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:42:15.529+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:42:15.530+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:42:15.532+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:42:15.534+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:42:15.535+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:42:15.535+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:42:15.536+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:42:15.537+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:42:15.538+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:42:15.539+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:42:15.539+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:42:15.541+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:42:15.542+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:42:15.542+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:42:15.543+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:42:15.543+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:42:15.544+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:42:15.544+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:42:15.545+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:42:15.545+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:42:15.545+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:42:15.545+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:42:15.545+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:42:15.545+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:42:15.546+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:42:15.546+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:42:15.546+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:42:15.546+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:42:15.546+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:42:15.546+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:42:15.546+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:42:15.547+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:42:15.547+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:42:15.547+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:42:15.548+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:42:15.548+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:42:15.549+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:42:15.549+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:42:15.549+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:42:15.550+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:42:15.550+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:42:15.552+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:42:15.552+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:42:15.553+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:42:15.553+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:42:15.554+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:42:15.554+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:42:15.555+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:42:15.555+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:42:15.555+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:42:15.556+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:42:15.557+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:42:15.557+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:42:15.558+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:42:15.559+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:42:15.559+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:42:15.560+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:42:15.560+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:42:15.560+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:42:15.561+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:42:15.561+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:42:15.562+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:42:15.563+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:42:15.563+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:42:15.563+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:42:15.564+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:42:15.564+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:42:15.564+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:42:15.565+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:42:15.565+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:42:15.566+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:42:15.566+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:42:15.566+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:42:15.569+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:42:15.570+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:42:15.571+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:42:15.572+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:42:15.572+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:42:15.573+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:42:15.573+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:42:15.573+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:42:15.574+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:42:15.574+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:42:15.574+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:42:15.575+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:42:15.575+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:42:15.576+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:42:15.576+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:42:15.576+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:42:15.576+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:42:15.577+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:42:15.577+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:42:15.578+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T15:42:15.579+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T15:42:15.579+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T15:42:15.579+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T15:42:15.580+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T15:42:15.581+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T15:42:15.581+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T15:42:15.581+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T15:42:15.582+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T15:42:15.582+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T15:42:15.582+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T15:42:15.582+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T15:42:15.583+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T15:42:15.583+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T15:42:15.583+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T15:42:15.584+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T15:42:15.585+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T15:42:15.585+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T15:42:15.585+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T15:42:15.586+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T15:42:15.586+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T15:42:15.586+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T15:42:15.586+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T15:42:15.587+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T15:42:15.587+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T15:42:15.587+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T15:42:15.588+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T15:42:15.588+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T15:42:15.588+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T15:42:15.589+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T15:42:15.589+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T15:42:15.589+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T15:42:15.589+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T15:42:15.589+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T15:42:15.590+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T15:42:15.590+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T15:42:15.591+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T15:42:15.591+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T15:42:15.592+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T15:42:15.593+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T15:42:15.593+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T15:42:15.593+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T15:42:15.594+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T15:42:15.594+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T15:42:15.594+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T15:42:15.595+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T15:42:15.595+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T15:42:15.596+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T15:42:15.596+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T15:42:15.596+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T15:42:15.596+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T15:42:15.597+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T15:42:15.597+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T15:42:15.597+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T15:42:15.598+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T15:42:15.599+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T15:42:15.599+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T15:42:15.600+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T15:42:15.600+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T15:42:15.600+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T15:42:15.600+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T15:42:15.601+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T15:42:15.601+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T15:42:15.601+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T15:42:15.602+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T15:42:15.602+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T15:42:15.602+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T15:42:15.602+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T15:42:15.602+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T15:42:15.603+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T15:42:15.603+0000] {subprocess.py:93} INFO - 
[2025-07-18T15:42:15.603+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:42:15.604+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:42:15.604+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:42:15.604+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:42:15.604+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T15:42:15.604+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka startTimeMs: 1752853335524
[2025-07-18T15:42:15.604+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:42:15.604+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:42:15.605+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka startTimeMs: 1752853335524
[2025-07-18T15:42:15.605+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T15:42:15.605+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T15:42:15.605+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO AppInfoParser: Kafka startTimeMs: 1752853335524
[2025-07-18T15:42:15.605+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/22 using temp file file:/tmp/checkpoints/feedback/offsets/.22.cb3254b2-8e6d-4fdb-8d79-8662f34c8c10.tmp
[2025-07-18T15:42:15.614+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.22.cb3254b2-8e6d-4fdb-8d79-8662f34c8c10.tmp to file:/tmp/checkpoints/feedback/offsets/22
[2025-07-18T15:42:15.615+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MicroBatchExecution: Committed offsets for batch 22. Metadata OffsetSeqMetadata(0,1752853335558,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:42:15.629+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.629+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.630+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.631+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:15.646+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:15.661+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.661+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.662+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.673+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:15.677+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:15.715+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.715+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.722+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:15.724+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:15.732+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:42:15.761+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:42:15.766+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:42:15.766+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:42:15.766+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkContext: Created broadcast 6 from start at <unknown>:0
[2025-07-18T15:42:15.767+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 22, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:42:15.767+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:42:15.768+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:42:15.770+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Final stage: ResultStage 3 (start at <unknown>:0)
[2025-07-18T15:42:15.771+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:42:15.771+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:42:15.773+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:42:15.783+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T15:42:15.799+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T15:42:15.801+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 77cb57a6bd53:38973 (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:42:15.803+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:42:15.804+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:42:15.804+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-07-18T15:42:15.806+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:42:15.810+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-07-18T15:42:15.828+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:42:15.831+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=67 untilOffset=69, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=22 taskId=3 partitionId=0
[2025-07-18T15:42:15.844+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T15:42:15.883+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T15:42:15.883+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 42403792 nanos.
[2025-07-18T15:42:15.885+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4771 bytes result sent to driver
[2025-07-18T15:42:15.889+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 84 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:42:15.889+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-07-18T15:42:15.891+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: ResultStage 3 (start at <unknown>:0) finished in 0.117 s
[2025-07-18T15:42:15.896+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:42:15.896+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-07-18T15:42:15.896+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 0.129806 s
[2025-07-18T15:42:15.896+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:42:15.896+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Committing epoch 22 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:42:15.937+0000] {subprocess.py:93} INFO - 25/07/18 15:42:15 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:42:16.104+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v24.metadata.json
[2025-07-18T15:42:16.170+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO SnapshotProducer: Committed snapshot 3415559019210791729 (FastAppend)
[2025-07-18T15:42:16.243+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=3415559019210791729, sequenceNumber=23, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.309819416S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=23}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=69}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2940}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=69012}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:42:16.243+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO SparkWrite: Committed in 311 ms
[2025-07-18T15:42:16.246+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 22, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:42:16.258+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/22 using temp file file:/tmp/checkpoints/feedback/commits/.22.4c7d2dd5-2e27-48d5-b906-fff38ed7aab4.tmp
[2025-07-18T15:42:16.288+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.22.4c7d2dd5-2e27-48d5-b906-fff38ed7aab4.tmp to file:/tmp/checkpoints/feedback/commits/22
[2025-07-18T15:42:16.293+0000] {subprocess.py:93} INFO - 25/07/18 15:42:16 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:42:16.295+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:42:16.296+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:42:16.296+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:42:16.296+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:42:15.492Z",
[2025-07-18T15:42:16.297+0000] {subprocess.py:93} INFO -   "batchId" : 22,
[2025-07-18T15:42:16.297+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:42:16.297+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.4859086491739553,
[2025-07-18T15:42:16.297+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.5157232704402515,
[2025-07-18T15:42:16.297+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:42:16.298+0000] {subprocess.py:93} INFO -     "addBatch" : 596,
[2025-07-18T15:42:16.298+0000] {subprocess.py:93} INFO -     "commitOffsets" : 44,
[2025-07-18T15:42:16.298+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:42:16.298+0000] {subprocess.py:93} INFO -     "latestOffset" : 60,
[2025-07-18T15:42:16.299+0000] {subprocess.py:93} INFO -     "queryPlanning" : 34,
[2025-07-18T15:42:16.299+0000] {subprocess.py:93} INFO -     "triggerExecution" : 795,
[2025-07-18T15:42:16.300+0000] {subprocess.py:93} INFO -     "walCommit" : 54
[2025-07-18T15:42:16.300+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:42:16.300+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:42:16.301+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:42:16.301+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:42:16.301+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:42:16.301+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:42:16.302+0000] {subprocess.py:93} INFO -         "0" : 67
[2025-07-18T15:42:16.302+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:16.302+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:16.303+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:42:16.303+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:42:16.304+0000] {subprocess.py:93} INFO -         "0" : 69
[2025-07-18T15:42:16.305+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:16.305+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:16.306+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:42:16.306+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:42:16.306+0000] {subprocess.py:93} INFO -         "0" : 69
[2025-07-18T15:42:16.307+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:42:16.307+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:42:16.307+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.4859086491739553,
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.5157232704402515,
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:42:16.308+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:42:16.309+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:42:16.309+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:42:16.309+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:42:20.792+0000] {subprocess.py:93} INFO - 25/07/18 15:42:20 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:42:20.795+0000] {subprocess.py:93} INFO - 25/07/18 15:42:20 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 77cb57a6bd53:38973 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:42:25.476+0000] {subprocess.py:93} INFO - 25/07/18 15:42:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:25.488+0000] {subprocess.py:93} INFO - 25/07/18 15:42:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:26.295+0000] {subprocess.py:93} INFO - 25/07/18 15:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:35.473+0000] {subprocess.py:93} INFO - 25/07/18 15:42:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:35.498+0000] {subprocess.py:93} INFO - 25/07/18 15:42:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:36.301+0000] {subprocess.py:93} INFO - 25/07/18 15:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:45.477+0000] {subprocess.py:93} INFO - 25/07/18 15:42:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:45.499+0000] {subprocess.py:93} INFO - 25/07/18 15:42:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:46.314+0000] {subprocess.py:93} INFO - 25/07/18 15:42:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:55.485+0000] {subprocess.py:93} INFO - 25/07/18 15:42:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:55.506+0000] {subprocess.py:93} INFO - 25/07/18 15:42:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:42:56.324+0000] {subprocess.py:93} INFO - 25/07/18 15:42:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:05.496+0000] {subprocess.py:93} INFO - 25/07/18 15:43:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:05.505+0000] {subprocess.py:93} INFO - 25/07/18 15:43:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:06.338+0000] {subprocess.py:93} INFO - 25/07/18 15:43:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:15.495+0000] {subprocess.py:93} INFO - 25/07/18 15:43:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:15.510+0000] {subprocess.py:93} INFO - 25/07/18 15:43:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:16.341+0000] {subprocess.py:93} INFO - 25/07/18 15:43:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:25.505+0000] {subprocess.py:93} INFO - 25/07/18 15:43:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:25.516+0000] {subprocess.py:93} INFO - 25/07/18 15:43:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:26.349+0000] {subprocess.py:93} INFO - 25/07/18 15:43:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:35.504+0000] {subprocess.py:93} INFO - 25/07/18 15:43:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:35.517+0000] {subprocess.py:93} INFO - 25/07/18 15:43:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:36.350+0000] {subprocess.py:93} INFO - 25/07/18 15:43:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:45.510+0000] {subprocess.py:93} INFO - 25/07/18 15:43:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:45.524+0000] {subprocess.py:93} INFO - 25/07/18 15:43:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:46.360+0000] {subprocess.py:93} INFO - 25/07/18 15:43:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:55.518+0000] {subprocess.py:93} INFO - 25/07/18 15:43:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:55.529+0000] {subprocess.py:93} INFO - 25/07/18 15:43:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:43:56.362+0000] {subprocess.py:93} INFO - 25/07/18 15:43:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:02.813+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/23 using temp file file:/tmp/checkpoints/reservations/offsets/.23.50236db4-3d45-4c6c-b117-3a230ecdb617.tmp
[2025-07-18T15:44:02.840+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.23.50236db4-3d45-4c6c-b117-3a230ecdb617.tmp to file:/tmp/checkpoints/reservations/offsets/23
[2025-07-18T15:44:02.841+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1752853442803,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:44:02.886+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.887+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.888+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.896+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:02.900+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:02.910+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.911+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.911+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.915+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:02.916+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:02.923+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.924+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.924+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:02.927+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:02.928+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:02.941+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:44:02.944+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:44:02.945+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:44:02.946+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkContext: Created broadcast 8 from start at <unknown>:0
[2025-07-18T15:44:02.946+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:44:02.947+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:44:02.948+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:44:02.949+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
[2025-07-18T15:44:02.954+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:44:02.956+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:44:02.956+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:44:02.957+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:44:02.969+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:44:02.971+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:44:02.974+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:44:02.976+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:44:02.977+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-18T15:44:02.980+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:44:02.981+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-07-18T15:44:02.989+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:44:02.990+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=69 untilOffset=70, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=23 taskId=4 partitionId=0
[2025-07-18T15:44:02.994+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 69 for partition reservations-0
[2025-07-18T15:44:03.000+0000] {subprocess.py:93} INFO - 25/07/18 15:44:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:44:03.003+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.004+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:44:03.004+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=71, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.007+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T15:44:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T15:44:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 9836666 nanos, during time span of 49443958 nanos.
[2025-07-18T15:44:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4784 bytes result sent to driver
[2025-07-18T15:44:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 76 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:44:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-18T15:44:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.101 s
[2025-07-18T15:44:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:44:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-18T15:44:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.106665 s
[2025-07-18T15:44:03.057+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:44:03.057+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committing epoch 23 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:44:03.070+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.170+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v25.metadata.json
[2025-07-18T15:44:03.204+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SnapshotProducer: Committed snapshot 881802391644802609 (FastAppend)
[2025-07-18T15:44:03.231+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=881802391644802609, sequenceNumber=24, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.15969525S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=24}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=70}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2989}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=72651}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:44:03.232+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committed in 160 ms
[2025-07-18T15:44:03.233+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:44:03.238+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/23 using temp file file:/tmp/checkpoints/reservations/commits/.23.e182f37b-94be-40e4-89db-a54d528f0545.tmp
[2025-07-18T15:44:03.255+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.23.e182f37b-94be-40e4-89db-a54d528f0545.tmp to file:/tmp/checkpoints/reservations/commits/23
[2025-07-18T15:44:03.256+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:44:03.256+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:44:03.257+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:44:03.257+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:44:03.259+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:44:02.802Z",
[2025-07-18T15:44:03.259+0000] {subprocess.py:93} INFO -   "batchId" : 23,
[2025-07-18T15:44:03.260+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:44:03.261+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:44:03.261+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.2222222222222223,
[2025-07-18T15:44:03.262+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:44:03.262+0000] {subprocess.py:93} INFO -     "addBatch" : 329,
[2025-07-18T15:44:03.262+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:44:03.262+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:44:03.262+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:44:03.262+0000] {subprocess.py:93} INFO -     "queryPlanning" : 49,
[2025-07-18T15:44:03.263+0000] {subprocess.py:93} INFO -     "triggerExecution" : 450,
[2025-07-18T15:44:03.263+0000] {subprocess.py:93} INFO -     "walCommit" : 46
[2025-07-18T15:44:03.263+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:44:03.263+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:44:03.263+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:44:03.263+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:44:03.263+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:44:03.264+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:44:03.264+0000] {subprocess.py:93} INFO -         "0" : 69
[2025-07-18T15:44:03.264+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:03.264+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:03.264+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:03.265+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:03.266+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:03.266+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:44:03.266+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:44:03.266+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.2222222222222223,
[2025-07-18T15:44:03.266+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:44:03.267+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:44:03.267+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:44:03.267+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:44:03.267+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:44:03.268+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:44:03.268+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:44:03.268+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:44:03.268+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:44:03.268+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:44:03.269+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:44:03.269+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/24 using temp file file:/tmp/checkpoints/reservations/offsets/.24.c2fc0d4a-c96a-40e6-8c6a-343a5acde17f.tmp
[2025-07-18T15:44:03.281+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.24.c2fc0d4a-c96a-40e6-8c6a-343a5acde17f.tmp to file:/tmp/checkpoints/reservations/offsets/24
[2025-07-18T15:44:03.282+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1752853443257,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:44:03.291+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.292+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.292+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.295+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.297+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.306+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.307+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.312+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.312+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.312+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.315+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.315+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.325+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:44:03.331+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:44:03.332+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:44:03.332+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkContext: Created broadcast 10 from start at <unknown>:0
[2025-07-18T15:44:03.332+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:44:03.333+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:44:03.334+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:44:03.334+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:44:03.334+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Final stage: ResultStage 5 (start at <unknown>:0)
[2025-07-18T15:44:03.334+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:44:03.334+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:44:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:44:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:44:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:44:03.342+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:44:03.342+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:44:03.343+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:44:03.343+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:44:03.343+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-07-18T15:44:03.344+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:44:03.344+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-07-18T15:44:03.348+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:44:03.354+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=70 untilOffset=72, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=24 taskId=5 partitionId=0
[2025-07-18T15:44:03.359+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 70 for partition reservations-0
[2025-07-18T15:44:03.360+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:44:03.360+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.360+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:44:03.361+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=72, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.361+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 71 for partition reservations-0
[2025-07-18T15:44:03.361+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:44:03.411+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/25 using temp file file:/tmp/checkpoints/checkins/offsets/.25.57735220-087d-4e2f-b906-f58357b6289d.tmp
[2025-07-18T15:44:03.425+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.25.57735220-087d-4e2f-b906-f58357b6289d.tmp to file:/tmp/checkpoints/checkins/offsets/25
[2025-07-18T15:44:03.427+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1752853443405,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:44:03.435+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.436+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.437+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.437+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.438+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.445+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.446+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.447+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.454+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.461+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.461+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:44:03.475+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.1 MiB)
[2025-07-18T15:44:03.475+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:44:03.476+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkContext: Created broadcast 12 from start at <unknown>:0
[2025-07-18T15:44:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:44:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:44:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:44:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
[2025-07-18T15:44:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:44:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:44:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:44:03.480+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T15:44:03.484+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.0 MiB)
[2025-07-18T15:44:03.485+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:44:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:44:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:44:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-18T15:44:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:44:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-07-18T15:44:03.493+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:44:03.497+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=69 untilOffset=70, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=25 taskId=6 partitionId=0
[2025-07-18T15:44:03.501+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 69 for partition checkins-0
[2025-07-18T15:44:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:44:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:44:03.611+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=71, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T15:44:03.633+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T15:44:03.634+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 109393834 nanos, during time span of 132344042 nanos.
[2025-07-18T15:44:03.634+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4860 bytes result sent to driver
[2025-07-18T15:44:03.636+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 150 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:44:03.637+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-18T15:44:03.637+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.159 s
[2025-07-18T15:44:03.637+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:44:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-07-18T15:44:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.162035 s
[2025-07-18T15:44:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:44:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committing epoch 25 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:44:03.647+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:03.774+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v27.metadata.json
[2025-07-18T15:44:03.839+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SnapshotProducer: Committed snapshot 886174317830675190 (FastAppend)
[2025-07-18T15:44:03.867+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.868+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:44:03.870+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=72, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:03.871+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T15:44:03.898+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=886174317830675190, sequenceNumber=26, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.24816375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=26}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=70}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2869}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=77242}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:44:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committed in 249 ms
[2025-07-18T15:44:03.904+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:44:03.904+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T15:44:03.904+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 509774417 nanos, during time span of 546059209 nanos.
[2025-07-18T15:44:03.905+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4791 bytes result sent to driver
[2025-07-18T15:44:03.907+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 566 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:44:03.909+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-07-18T15:44:03.911+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/25 using temp file file:/tmp/checkpoints/checkins/commits/.25.7b83ee38-d8b6-4ab8-82b7-454747c33c8b.tmp
[2025-07-18T15:44:03.914+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: ResultStage 5 (start at <unknown>:0) finished in 0.579 s
[2025-07-18T15:44:03.917+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:44:03.918+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-07-18T15:44:03.919+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 0.584431 s
[2025-07-18T15:44:03.920+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:44:03.921+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committing epoch 24 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:44:03.938+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:44:03.944+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.25.7b83ee38-d8b6-4ab8-82b7-454747c33c8b.tmp to file:/tmp/checkpoints/checkins/commits/25
[2025-07-18T15:44:03.945+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:44:03.946+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:44:03.946+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:44:03.947+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:44:03.947+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:44:03.404Z",
[2025-07-18T15:44:03.948+0000] {subprocess.py:93} INFO -   "batchId" : 25,
[2025-07-18T15:44:03.948+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:44:03.948+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:44:03.949+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.858736059479554,
[2025-07-18T15:44:03.949+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:44:03.949+0000] {subprocess.py:93} INFO -     "addBatch" : 456,
[2025-07-18T15:44:03.950+0000] {subprocess.py:93} INFO -     "commitOffsets" : 47,
[2025-07-18T15:44:03.950+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:44:03.950+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:44:03.950+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T15:44:03.950+0000] {subprocess.py:93} INFO -     "triggerExecution" : 538,
[2025-07-18T15:44:03.951+0000] {subprocess.py:93} INFO -     "walCommit" : 20
[2025-07-18T15:44:03.951+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:44:03.951+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:44:03.951+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:44:03.951+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:44:03.951+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:44:03.951+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -         "0" : 69
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:03.952+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:03.953+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:44:03.953+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:44:03.953+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:03.954+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:03.955+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:03.955+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:44:03.955+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:44:03.956+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.858736059479554,
[2025-07-18T15:44:03.956+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:44:03.956+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:44:03.957+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:44:03.957+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:44:03.957+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:44:03.957+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:44:03.959+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:44:03.960+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:44:03.960+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:44:03.960+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:44:03.960+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:44:03.960+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/26 using temp file file:/tmp/checkpoints/checkins/offsets/.26.6f388216-464a-4ce1-b5c2-d2f623c43517.tmp
[2025-07-18T15:44:03.993+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.26.6f388216-464a-4ce1-b5c2-d2f623c43517.tmp to file:/tmp/checkpoints/checkins/offsets/26
[2025-07-18T15:44:03.993+0000] {subprocess.py:93} INFO - 25/07/18 15:44:03 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1752853443947,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:44:04.012+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.014+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.014+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.014+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.018+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.028+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.030+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.032+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.033+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.038+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.043+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/23 using temp file file:/tmp/checkpoints/feedback/offsets/.23.b74f1448-1634-431d-ac44-0e970a3def9f.tmp
[2025-07-18T15:44:04.056+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.057+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.058+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.063+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.067+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.084+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:44:04.087+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:44:04.088+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:44:04.089+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkContext: Created broadcast 14 from start at <unknown>:0
[2025-07-18T15:44:04.090+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:44:04.094+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.23.b74f1448-1634-431d-ac44-0e970a3def9f.tmp to file:/tmp/checkpoints/feedback/offsets/23
[2025-07-18T15:44:04.095+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MicroBatchExecution: Committed offsets for batch 23. Metadata OffsetSeqMetadata(0,1752853444028,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:44:04.096+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v26.metadata.json
[2025-07-18T15:44:04.096+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:44:04.096+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:44:04.096+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
[2025-07-18T15:44:04.096+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:44:04.096+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:44:04.098+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:44:04.112+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T15:44:04.119+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.120+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:44:04.120+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.123+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:44:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:44:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:44:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-18T15:44:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:44:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-07-18T15:44:04.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.128+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.141+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:44:04.144+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=70 untilOffset=72, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=26 taskId=7 partitionId=0
[2025-07-18T15:44:04.145+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.148+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.149+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.150+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.166+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.168+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 70 for partition checkins-0
[2025-07-18T15:44:04.169+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:44:04.171+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:04.173+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:44:04.174+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=72, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:04.177+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 71 for partition checkins-0
[2025-07-18T15:44:04.177+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:44:04.177+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SnapshotProducer: Committed snapshot 5742140020700108577 (FastAppend)
[2025-07-18T15:44:04.178+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.178+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.178+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.181+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.182+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.194+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:44:04.199+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:44:04.200+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:44:04.204+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkContext: Created broadcast 16 from start at <unknown>:0
[2025-07-18T15:44:04.207+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 23, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:44:04.212+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:44:04.216+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:44:04.218+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
[2025-07-18T15:44:04.218+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:44:04.219+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:44:04.222+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:44:04.231+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 27.5 KiB, free 433.8 MiB)
[2025-07-18T15:44:04.233+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 433.8 MiB)
[2025-07-18T15:44:04.236+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 77cb57a6bd53:38973 (size: 12.2 KiB, free: 434.1 MiB)
[2025-07-18T15:44:04.242+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:44:04.246+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:44:04.250+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-18T15:44:04.251+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:44:04.252+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5742140020700108577, sequenceNumber=25, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.304106208S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=25}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=72}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2991}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=75642}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:44:04.253+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Committed in 308 ms
[2025-07-18T15:44:04.254+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:44:04.260+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-07-18T15:44:04.261+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/24 using temp file file:/tmp/checkpoints/reservations/commits/.24.8e3aabbf-1def-4c4b-8091-35e5479d8d51.tmp
[2025-07-18T15:44:04.270+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:44:04.270+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=69 untilOffset=70, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=23 taskId=8 partitionId=0
[2025-07-18T15:44:04.291+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 69 for partition feedback-0
[2025-07-18T15:44:04.296+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:44:04.311+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.24.8e3aabbf-1def-4c4b-8091-35e5479d8d51.tmp to file:/tmp/checkpoints/reservations/commits/24
[2025-07-18T15:44:04.312+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:44:04.312+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:44:04.313+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:44:04.313+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:44:04.313+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:44:03.255Z",
[2025-07-18T15:44:04.313+0000] {subprocess.py:93} INFO -   "batchId" : 24,
[2025-07-18T15:44:04.314+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:44:04.315+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.415011037527593,
[2025-07-18T15:44:04.315+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.9029495718363465,
[2025-07-18T15:44:04.315+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:44:04.315+0000] {subprocess.py:93} INFO -     "addBatch" : 941,
[2025-07-18T15:44:04.317+0000] {subprocess.py:93} INFO -     "commitOffsets" : 66,
[2025-07-18T15:44:04.319+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:44:04.319+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:44:04.321+0000] {subprocess.py:93} INFO -     "queryPlanning" : 18,
[2025-07-18T15:44:04.322+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1051,
[2025-07-18T15:44:04.322+0000] {subprocess.py:93} INFO -     "walCommit" : 23
[2025-07-18T15:44:04.324+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:44:04.324+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:44:04.324+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:44:04.326+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:44:04.326+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:44:04.326+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:44:04.326+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:04.326+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:04.326+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:04.326+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:44:04.327+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:44:04.327+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:44:04.327+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:04.327+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:04.327+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:44:04.327+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:44:04.327+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:44:04.328+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:04.328+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:04.328+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:44:04.328+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.415011037527593,
[2025-07-18T15:44:04.328+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.9029495718363465,
[2025-07-18T15:44:04.331+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:44:04.332+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:44:04.332+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:44:04.332+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:44:04.333+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:44:04.333+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:44:04.333+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:44:04.333+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:44:04.333+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:44:04.334+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:44:04.334+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:44:04.417+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:04.418+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:44:04.418+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=72, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:04.421+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T15:44:04.438+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T15:44:04.438+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 2 records), taking 128031416 nanos, during time span of 148025375 nanos.
[2025-07-18T15:44:04.440+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4769 bytes result sent to driver
[2025-07-18T15:44:04.442+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 204 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:44:04.443+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-18T15:44:04.444+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.225 s
[2025-07-18T15:44:04.447+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:44:04.447+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-18T15:44:04.447+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.237926 s
[2025-07-18T15:44:04.449+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:44:04.449+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Committing epoch 23 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:44:04.470+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.599+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v25.metadata.json
[2025-07-18T15:44:04.659+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SnapshotProducer: Committed snapshot 5756360286432560560 (FastAppend)
[2025-07-18T15:44:04.663+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:04.665+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:44:04.667+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=72, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:04.670+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T15:44:04.722+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T15:44:04.724+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 2 records through 2 polls (polled  out 2 records), taking 516659584 nanos, during time span of 574986708 nanos.
[2025-07-18T15:44:04.735+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4862 bytes result sent to driver
[2025-07-18T15:44:04.742+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 626 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:44:04.742+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-18T15:44:04.744+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.644 s
[2025-07-18T15:44:04.747+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:44:04.748+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-18T15:44:04.748+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 0.655652 s
[2025-07-18T15:44:04.749+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:44:04.749+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Committing epoch 26 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:44:04.750+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=5756360286432560560, sequenceNumber=24, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.277818125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=24}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=70}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2762}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=71774}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:44:04.750+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Committed in 278 ms
[2025-07-18T15:44:04.750+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 23, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:44:04.760+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/23 using temp file file:/tmp/checkpoints/feedback/commits/.23.641229d7-bae5-4f79-a656-294e3505af2e.tmp
[2025-07-18T15:44:04.767+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:44:04.789+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.23.641229d7-bae5-4f79-a656-294e3505af2e.tmp to file:/tmp/checkpoints/feedback/commits/23
[2025-07-18T15:44:04.798+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:44:04.798+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:44:04.798+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:44:04.799+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:44:04.799+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:44:04.024Z",
[2025-07-18T15:44:04.799+0000] {subprocess.py:93} INFO -   "batchId" : 23,
[2025-07-18T15:44:04.799+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:44:04.802+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:44:04.804+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.310615989515072,
[2025-07-18T15:44:04.805+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:44:04.805+0000] {subprocess.py:93} INFO -     "addBatch" : 623,
[2025-07-18T15:44:04.806+0000] {subprocess.py:93} INFO -     "commitOffsets" : 40,
[2025-07-18T15:44:04.806+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:44:04.806+0000] {subprocess.py:93} INFO -     "latestOffset" : 4,
[2025-07-18T15:44:04.807+0000] {subprocess.py:93} INFO -     "queryPlanning" : 34,
[2025-07-18T15:44:04.807+0000] {subprocess.py:93} INFO -     "triggerExecution" : 763,
[2025-07-18T15:44:04.808+0000] {subprocess.py:93} INFO -     "walCommit" : 60
[2025-07-18T15:44:04.808+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:44:04.809+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:44:04.810+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:44:04.810+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:44:04.811+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:44:04.815+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:44:04.819+0000] {subprocess.py:93} INFO -         "0" : 69
[2025-07-18T15:44:04.820+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:04.821+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:04.824+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:44:04.825+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:44:04.827+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:04.831+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:04.832+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:04.833+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:44:04.834+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:44:04.836+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:04.837+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:04.839+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:04.840+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:44:04.842+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:44:04.844+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.310615989515072,
[2025-07-18T15:44:04.856+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:44:04.860+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:44:04.862+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:44:04.865+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:44:04.899+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:44:04.916+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:44:04.916+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:44:04.917+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:44:04.918+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:44:04.936+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:44:04.937+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:44:04.946+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/24 using temp file file:/tmp/checkpoints/feedback/offsets/.24.e7c0b806-743b-47ad-aca8-ab3826d1844a.tmp
[2025-07-18T15:44:04.946+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.24.e7c0b806-743b-47ad-aca8-ab3826d1844a.tmp to file:/tmp/checkpoints/feedback/offsets/24
[2025-07-18T15:44:04.947+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO MicroBatchExecution: Committed offsets for batch 24. Metadata OffsetSeqMetadata(0,1752853444798,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:44:04.948+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.948+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.949+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.949+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.960+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.976+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.977+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.977+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:04.978+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:04.989+0000] {subprocess.py:93} INFO - 25/07/18 15:44:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:05.028+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:05.029+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:05.030+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:05.042+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:05.044+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:44:05.052+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v28.metadata.json
[2025-07-18T15:44:05.101+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:44:05.107+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:44:05.108+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:44:05.109+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkContext: Created broadcast 18 from start at <unknown>:0
[2025-07-18T15:44:05.109+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:44:05.110+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:44:05.122+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Got job 9 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:44:05.127+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Final stage: ResultStage 9 (start at <unknown>:0)
[2025-07-18T15:44:05.130+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:44:05.131+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:44:05.132+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:44:05.133+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 27.5 KiB, free 433.7 MiB)
[2025-07-18T15:44:05.137+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.7 MiB)
[2025-07-18T15:44:05.138+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:44:05.140+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:44:05.140+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:44:05.141+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-07-18T15:44:05.142+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:44:05.142+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
[2025-07-18T15:44:05.149+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:44:05.156+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=70 untilOffset=72, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=24 taskId=9 partitionId=0
[2025-07-18T15:44:05.161+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SnapshotProducer: Committed snapshot 2922700726458909183 (FastAppend)
[2025-07-18T15:44:05.161+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 71 for partition feedback-0
[2025-07-18T15:44:05.163+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:44:05.187+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2922700726458909183, sequenceNumber=27, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.418273333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=27}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=72}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2906}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=80148}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:44:05.195+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkWrite: Committed in 418 ms
[2025-07-18T15:44:05.196+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:44:05.223+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/26 using temp file file:/tmp/checkpoints/checkins/commits/.26.155db8be-2ba9-46f8-8741-741317169071.tmp
[2025-07-18T15:44:05.260+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.26.155db8be-2ba9-46f8-8741-741317169071.tmp to file:/tmp/checkpoints/checkins/commits/26
[2025-07-18T15:44:05.268+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:44:05.269+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:44:05.270+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:44:05.270+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:44:05.271+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:44:03.944Z",
[2025-07-18T15:44:05.271+0000] {subprocess.py:93} INFO -   "batchId" : 26,
[2025-07-18T15:44:05.272+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:44:05.272+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.7037037037037033,
[2025-07-18T15:44:05.275+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.5209125475285172,
[2025-07-18T15:44:05.276+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:44:05.276+0000] {subprocess.py:93} INFO -     "addBatch" : 1166,
[2025-07-18T15:44:05.276+0000] {subprocess.py:93} INFO -     "commitOffsets" : 76,
[2025-07-18T15:44:05.277+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:44:05.277+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:44:05.278+0000] {subprocess.py:93} INFO -     "queryPlanning" : 24,
[2025-07-18T15:44:05.278+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1315,
[2025-07-18T15:44:05.279+0000] {subprocess.py:93} INFO -     "walCommit" : 45
[2025-07-18T15:44:05.279+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:44:05.279+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:44:05.280+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:44:05.281+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:44:05.282+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:44:05.282+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:44:05.284+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:05.285+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:05.286+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:05.286+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:44:05.287+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:44:05.288+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:44:05.289+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:05.292+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:05.292+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.7037037037037033,
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.5209125475285172,
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:44:05.293+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:44:05.294+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:44:05.294+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:44:05.294+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:44:05.294+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:44:05.294+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:44:05.294+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:44:05.294+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:44:05.667+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:05.670+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:44:05.673+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=72, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:44:05.676+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T15:44:05.739+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
[2025-07-18T15:44:05.740+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 2 records through 1 polls (polled  out 1 records), taking 511993501 nanos, during time span of 586185501 nanos.
[2025-07-18T15:44:05.746+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 4808 bytes result sent to driver
[2025-07-18T15:44:05.750+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 614 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:44:05.750+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-07-18T15:44:05.754+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: ResultStage 9 (start at <unknown>:0) finished in 0.631 s
[2025-07-18T15:44:05.757+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:44:05.757+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-07-18T15:44:05.758+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO DAGScheduler: Job 9 finished: start at <unknown>:0, took 0.639289 s
[2025-07-18T15:44:05.758+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:44:05.758+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkWrite: Committing epoch 24 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:44:05.774+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:44:05.901+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v26.metadata.json
[2025-07-18T15:44:05.948+0000] {subprocess.py:93} INFO - 25/07/18 15:44:05 INFO SnapshotProducer: Committed snapshot 7124577217372584793 (FastAppend)
[2025-07-18T15:44:06.013+0000] {subprocess.py:93} INFO - 25/07/18 15:44:06 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7124577217372584793, sequenceNumber=25, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.236247375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=25}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=72}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2905}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=74679}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:44:06.014+0000] {subprocess.py:93} INFO - 25/07/18 15:44:06 INFO SparkWrite: Committed in 236 ms
[2025-07-18T15:44:06.015+0000] {subprocess.py:93} INFO - 25/07/18 15:44:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 24, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:44:06.018+0000] {subprocess.py:93} INFO - 25/07/18 15:44:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/24 using temp file file:/tmp/checkpoints/feedback/commits/.24.59d1cbb0-0280-4147-98b1-ca5cdebb1f18.tmp
[2025-07-18T15:44:06.037+0000] {subprocess.py:93} INFO - 25/07/18 15:44:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.24.59d1cbb0-0280-4147-98b1-ca5cdebb1f18.tmp to file:/tmp/checkpoints/feedback/commits/24
[2025-07-18T15:44:06.038+0000] {subprocess.py:93} INFO - 25/07/18 15:44:06 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:44:06.039+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:44:06.039+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:44:06.039+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:44:06.039+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:44:04.791Z",
[2025-07-18T15:44:06.040+0000] {subprocess.py:93} INFO -   "batchId" : 24,
[2025-07-18T15:44:06.040+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:44:06.041+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.607561929595828,
[2025-07-18T15:44:06.041+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.6064257028112447,
[2025-07-18T15:44:06.041+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:44:06.042+0000] {subprocess.py:93} INFO -     "addBatch" : 1055,
[2025-07-18T15:44:06.042+0000] {subprocess.py:93} INFO -     "commitOffsets" : 27,
[2025-07-18T15:44:06.042+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:44:06.043+0000] {subprocess.py:93} INFO -     "latestOffset" : 7,
[2025-07-18T15:44:06.043+0000] {subprocess.py:93} INFO -     "queryPlanning" : 50,
[2025-07-18T15:44:06.044+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1245,
[2025-07-18T15:44:06.044+0000] {subprocess.py:93} INFO -     "walCommit" : 104
[2025-07-18T15:44:06.045+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:44:06.045+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:44:06.045+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -         "0" : 70
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:44:06.046+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:44:06.047+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:44:06.048+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.607561929595828,
[2025-07-18T15:44:06.048+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.6064257028112447,
[2025-07-18T15:44:06.048+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:44:06.048+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:44:06.048+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:44:06.049+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:44:12.309+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:44:12.311+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:44:12.312+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:44:12.314+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:44:12.315+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:44:12.318+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:44:12.321+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:44:12.322+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 77cb57a6bd53:38973 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:44:12.324+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:44:12.325+0000] {subprocess.py:93} INFO - 25/07/18 15:44:12 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:44:14.314+0000] {subprocess.py:93} INFO - 25/07/18 15:44:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:15.267+0000] {subprocess.py:93} INFO - 25/07/18 15:44:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:16.044+0000] {subprocess.py:93} INFO - 25/07/18 15:44:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:24.321+0000] {subprocess.py:93} INFO - 25/07/18 15:44:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:25.276+0000] {subprocess.py:93} INFO - 25/07/18 15:44:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:26.052+0000] {subprocess.py:93} INFO - 25/07/18 15:44:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:34.331+0000] {subprocess.py:93} INFO - 25/07/18 15:44:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:35.277+0000] {subprocess.py:93} INFO - 25/07/18 15:44:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:36.061+0000] {subprocess.py:93} INFO - 25/07/18 15:44:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:44.338+0000] {subprocess.py:93} INFO - 25/07/18 15:44:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:45.285+0000] {subprocess.py:93} INFO - 25/07/18 15:44:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:46.073+0000] {subprocess.py:93} INFO - 25/07/18 15:44:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:54.336+0000] {subprocess.py:93} INFO - 25/07/18 15:44:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:55.294+0000] {subprocess.py:93} INFO - 25/07/18 15:44:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:44:56.076+0000] {subprocess.py:93} INFO - 25/07/18 15:44:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:04.346+0000] {subprocess.py:93} INFO - 25/07/18 15:45:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:05.306+0000] {subprocess.py:93} INFO - 25/07/18 15:45:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:06.076+0000] {subprocess.py:93} INFO - 25/07/18 15:45:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:14.356+0000] {subprocess.py:93} INFO - 25/07/18 15:45:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:15.317+0000] {subprocess.py:93} INFO - 25/07/18 15:45:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:16.085+0000] {subprocess.py:93} INFO - 25/07/18 15:45:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:24.371+0000] {subprocess.py:93} INFO - 25/07/18 15:45:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:25.330+0000] {subprocess.py:93} INFO - 25/07/18 15:45:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:26.095+0000] {subprocess.py:93} INFO - 25/07/18 15:45:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:34.380+0000] {subprocess.py:93} INFO - 25/07/18 15:45:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:35.333+0000] {subprocess.py:93} INFO - 25/07/18 15:45:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:36.101+0000] {subprocess.py:93} INFO - 25/07/18 15:45:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:44.383+0000] {subprocess.py:93} INFO - 25/07/18 15:45:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:45.342+0000] {subprocess.py:93} INFO - 25/07/18 15:45:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:46.100+0000] {subprocess.py:93} INFO - 25/07/18 15:45:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:54.387+0000] {subprocess.py:93} INFO - 25/07/18 15:45:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:55.340+0000] {subprocess.py:93} INFO - 25/07/18 15:45:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:45:56.106+0000] {subprocess.py:93} INFO - 25/07/18 15:45:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:01.842+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/25 using temp file file:/tmp/checkpoints/reservations/offsets/.25.3d91f5ce-8e53-42d8-b42c-0a9018531f17.tmp
[2025-07-18T15:46:01.864+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.25.3d91f5ce-8e53-42d8-b42c-0a9018531f17.tmp to file:/tmp/checkpoints/reservations/offsets/25
[2025-07-18T15:46:01.865+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1752853561827,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:01.901+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.901+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.901+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.905+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:01.909+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:01.923+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.925+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.926+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.927+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:01.927+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:01.932+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.933+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.934+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:01.934+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:01.937+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:01.954+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:46:01.958+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:46:01.958+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:46:01.960+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkContext: Created broadcast 20 from start at <unknown>:0
[2025-07-18T15:46:01.960+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:01.960+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:01.961+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO DAGScheduler: Got job 10 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:01.961+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO DAGScheduler: Final stage: ResultStage 10 (start at <unknown>:0)
[2025-07-18T15:46:01.962+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:01.962+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:01.962+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:01.965+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:46:01.967+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:46:01.967+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:46:01.968+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:01.968+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[43] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:01.969+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-07-18T15:46:01.971+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:46:01.971+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2025-07-18T15:46:01.983+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:01.985+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=72 untilOffset=73, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=25 taskId=10 partitionId=0
[2025-07-18T15:46:01.989+0000] {subprocess.py:93} INFO - 25/07/18 15:46:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 72 for partition reservations-0
[2025-07-18T15:46:02.008+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:46:02.022+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.024+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:46:02.024+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=74, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.027+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T15:46:02.069+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
[2025-07-18T15:46:02.069+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 33358667 nanos, during time span of 78695792 nanos.
[2025-07-18T15:46:02.069+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 4682 bytes result sent to driver
[2025-07-18T15:46:02.070+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 100 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:02.071+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool
[2025-07-18T15:46:02.072+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: ResultStage 10 (start at <unknown>:0) finished in 0.109 s
[2025-07-18T15:46:02.072+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:02.072+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-07-18T15:46:02.072+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 10 finished: start at <unknown>:0, took 0.111522 s
[2025-07-18T15:46:02.073+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:02.073+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing epoch 25 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:46:02.086+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.183+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v27.metadata.json
[2025-07-18T15:46:02.212+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SnapshotProducer: Committed snapshot 5882146160502805123 (FastAppend)
[2025-07-18T15:46:02.256+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5882146160502805123, sequenceNumber=26, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.163174S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=26}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=73}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2933}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=78575}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:02.259+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committed in 164 ms
[2025-07-18T15:46:02.259+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:46:02.264+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/25 using temp file file:/tmp/checkpoints/reservations/commits/.25.650b8e50-5633-4e91-ad68-770935bc9aca.tmp
[2025-07-18T15:46:02.294+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.25.650b8e50-5633-4e91-ad68-770935bc9aca.tmp to file:/tmp/checkpoints/reservations/commits/25
[2025-07-18T15:46:02.295+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:02.295+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:46:02.295+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:46:02.295+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:02.295+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:01.825Z",
[2025-07-18T15:46:02.295+0000] {subprocess.py:93} INFO -   "batchId" : 25,
[2025-07-18T15:46:02.295+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:46:02.296+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:46:02.296+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.141327623126338,
[2025-07-18T15:46:02.296+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:02.296+0000] {subprocess.py:93} INFO -     "addBatch" : 338,
[2025-07-18T15:46:02.296+0000] {subprocess.py:93} INFO -     "commitOffsets" : 42,
[2025-07-18T15:46:02.296+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:02.296+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:46:02.297+0000] {subprocess.py:93} INFO -     "queryPlanning" : 48,
[2025-07-18T15:46:02.297+0000] {subprocess.py:93} INFO -     "triggerExecution" : 467,
[2025-07-18T15:46:02.297+0000] {subprocess.py:93} INFO -     "walCommit" : 34
[2025-07-18T15:46:02.297+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:02.297+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:02.298+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:02.298+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:46:02.298+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:02.299+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:46:02.299+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:46:02.300+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:02.300+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:02.300+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:02.301+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:46:02.301+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:02.301+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:02.301+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:02.301+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:02.302+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:46:02.302+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:02.302+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:02.302+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:02.303+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:46:02.304+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:46:02.304+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.141327623126338,
[2025-07-18T15:46:02.304+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:02.304+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:02.305+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:02.305+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:02.305+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:02.305+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:02.306+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:02.306+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:46:02.307+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:46:02.307+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:02.307+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:02.307+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/26 using temp file file:/tmp/checkpoints/reservations/offsets/.26.1a75ba2f-8633-4ae9-ae89-f2b7da9e069f.tmp
[2025-07-18T15:46:02.327+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.26.1a75ba2f-8633-4ae9-ae89-f2b7da9e069f.tmp to file:/tmp/checkpoints/reservations/offsets/26
[2025-07-18T15:46:02.329+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1752853562295,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:02.335+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.335+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.336+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.337+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.342+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.349+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.350+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.350+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.370+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.371+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.375+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.375+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.376+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.378+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.379+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.387+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:46:02.391+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:46:02.391+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:02.392+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Created broadcast 22 from start at <unknown>:0
[2025-07-18T15:46:02.392+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:02.393+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:02.393+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Got job 11 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:02.394+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Final stage: ResultStage 11 (start at <unknown>:0)
[2025-07-18T15:46:02.394+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:02.394+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:02.394+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:02.399+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T15:46:02.400+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T15:46:02.400+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:02.401+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:02.401+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[47] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:02.401+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-07-18T15:46:02.402+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:46:02.402+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2025-07-18T15:46:02.407+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:02.408+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=73 untilOffset=75, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=26 taskId=11 partitionId=0
[2025-07-18T15:46:02.412+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 73 for partition reservations-0
[2025-07-18T15:46:02.413+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:46:02.415+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.415+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:46:02.416+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=75, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 74 for partition reservations-0
[2025-07-18T15:46:02.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:46:02.437+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/27 using temp file file:/tmp/checkpoints/checkins/offsets/.27.8f9716db-34ae-42ca-af83-00e5f95f8cf3.tmp
[2025-07-18T15:46:02.451+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.27.8f9716db-34ae-42ca-af83-00e5f95f8cf3.tmp to file:/tmp/checkpoints/checkins/offsets/27
[2025-07-18T15:46:02.452+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1752853562431,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:02.459+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.459+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.459+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.462+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.463+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.468+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.469+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.469+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.469+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.470+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.475+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.475+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.476+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.476+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.477+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.485+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:46:02.486+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T15:46:02.492+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:46:02.493+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Created broadcast 24 from start at <unknown>:0
[2025-07-18T15:46:02.493+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:02.494+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:02.494+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Got job 12 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:02.494+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Final stage: ResultStage 12 (start at <unknown>:0)
[2025-07-18T15:46:02.494+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:02.495+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:02.497+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:02.499+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T15:46:02.499+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:46:02.500+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:46:02.502+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:02.502+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[51] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:02.502+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-07-18T15:46:02.505+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:46:02.507+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
[2025-07-18T15:46:02.508+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:02.511+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=72 untilOffset=73, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=27 taskId=12 partitionId=0
[2025-07-18T15:46:02.514+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 72 for partition checkins-0
[2025-07-18T15:46:02.516+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:46:02.623+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.624+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:46:02.624+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=74, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.626+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T15:46:02.641+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Committed partition 0 (task 12, attempt 0, stage 12.0)
[2025-07-18T15:46:02.642+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 109984833 nanos, during time span of 127516833 nanos.
[2025-07-18T15:46:02.643+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 4824 bytes result sent to driver
[2025-07-18T15:46:02.644+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 149 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:02.645+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-07-18T15:46:02.646+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: ResultStage 12 (start at <unknown>:0) finished in 0.155 s
[2025-07-18T15:46:02.646+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-07-18T15:46:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 12 finished: start at <unknown>:0, took 0.157027 s
[2025-07-18T15:46:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing epoch 27 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:46:02.653+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.710+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v29.metadata.json
[2025-07-18T15:46:02.736+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SnapshotProducer: Committed snapshot 991946143319569961 (FastAppend)
[2025-07-18T15:46:02.761+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=991946143319569961, sequenceNumber=28, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.104242292S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=28}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=73}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2934}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=83082}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:02.761+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committed in 105 ms
[2025-07-18T15:46:02.762+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:46:02.764+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/27 using temp file file:/tmp/checkpoints/checkins/commits/.27.05ff77cf-75b7-49f2-b8ac-01dd79e619cc.tmp
[2025-07-18T15:46:02.778+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.27.05ff77cf-75b7-49f2-b8ac-01dd79e619cc.tmp to file:/tmp/checkpoints/checkins/commits/27
[2025-07-18T15:46:02.778+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:02.778+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:46:02.778+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:46:02.778+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:02.429Z",
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -   "batchId" : 27,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.8818443804034586,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -     "addBatch" : 293,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -     "commitOffsets" : 19,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:02.779+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -     "triggerExecution" : 347,
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -     "walCommit" : 19
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:02.780+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:02.781+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:02.782+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:02.782+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:02.782+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:02.782+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:02.782+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:46:02.783+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:46:02.783+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.8818443804034586,
[2025-07-18T15:46:02.783+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:02.783+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:02.783+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:02.783+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:02.783+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:02.784+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:02.784+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:02.784+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:46:02.784+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:46:02.784+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:02.784+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:02.784+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/28 using temp file file:/tmp/checkpoints/checkins/offsets/.28.646c410b-2474-47b0-a8e9-55dc7d4068d2.tmp
[2025-07-18T15:46:02.793+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.28.646c410b-2474-47b0-a8e9-55dc7d4068d2.tmp to file:/tmp/checkpoints/checkins/offsets/28
[2025-07-18T15:46:02.794+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1752853562778,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:02.798+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.798+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.798+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.799+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.801+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.803+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.803+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.804+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.805+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.805+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.810+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.812+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.813+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.814+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.814+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:02.823+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:46:02.826+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:46:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Created broadcast 26 from start at <unknown>:0
[2025-07-18T15:46:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:02.829+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Got job 13 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:02.830+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Final stage: ResultStage 13 (start at <unknown>:0)
[2025-07-18T15:46:02.831+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:02.831+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:02.832+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:02.833+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T15:46:02.839+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T15:46:02.841+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:46:02.842+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:02.842+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[55] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:02.842+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-07-18T15:46:02.843+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:46:02.844+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2025-07-18T15:46:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:02.850+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=73 untilOffset=74, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=28 taskId=13 partitionId=0
[2025-07-18T15:46:02.854+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 73 for partition checkins-0
[2025-07-18T15:46:02.854+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:46:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:46:02.857+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=75, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.859+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T15:46:02.878+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Committed partition 0 (task 13, attempt 0, stage 13.0)
[2025-07-18T15:46:02.878+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 3120250 nanos, during time span of 23655583 nanos.
[2025-07-18T15:46:02.879+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 4799 bytes result sent to driver
[2025-07-18T15:46:02.880+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 37 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:02.880+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-07-18T15:46:02.881+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: ResultStage 13 (start at <unknown>:0) finished in 0.052 s
[2025-07-18T15:46:02.882+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:02.882+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-07-18T15:46:02.882+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 13 finished: start at <unknown>:0, took 0.053465 s
[2025-07-18T15:46:02.882+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:02.882+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing epoch 28 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:46:02.892+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:02.919+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.920+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:46:02.920+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=75, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:02.920+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T15:46:02.931+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DataWritingSparkTask: Committed partition 0 (task 11, attempt 0, stage 11.0)
[2025-07-18T15:46:02.931+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 507194084 nanos, during time span of 519870708 nanos.
[2025-07-18T15:46:02.932+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 4698 bytes result sent to driver
[2025-07-18T15:46:02.933+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 532 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:02.934+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-07-18T15:46:02.934+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: ResultStage 11 (start at <unknown>:0) finished in 0.540 s
[2025-07-18T15:46:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-07-18T15:46:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO DAGScheduler: Job 11 finished: start at <unknown>:0, took 0.541718 s
[2025-07-18T15:46:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:02.936+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing epoch 26 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:46:02.944+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:46:02.962+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v30.metadata.json
[2025-07-18T15:46:02.983+0000] {subprocess.py:93} INFO - 25/07/18 15:46:02 INFO SnapshotProducer: Committed snapshot 1345662139458451269 (FastAppend)
[2025-07-18T15:46:03.003+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=1345662139458451269, sequenceNumber=29, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.108427875S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=29}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=74}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2834}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=85916}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:03.003+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committed in 108 ms
[2025-07-18T15:46:03.004+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:46:03.006+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/28 using temp file file:/tmp/checkpoints/checkins/commits/.28.e647ec6a-a544-48c5-941b-de5ac760da68.tmp
[2025-07-18T15:46:03.011+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v28.metadata.json
[2025-07-18T15:46:03.025+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.28.e647ec6a-a544-48c5-941b-de5ac760da68.tmp to file:/tmp/checkpoints/checkins/commits/28
[2025-07-18T15:46:03.025+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:03.026+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:46:03.026+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:46:03.026+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:03.026+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:02.777Z",
[2025-07-18T15:46:03.026+0000] {subprocess.py:93} INFO -   "batchId" : 28,
[2025-07-18T15:46:03.027+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:46:03.027+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.873563218390805,
[2025-07-18T15:46:03.027+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.065040650406504,
[2025-07-18T15:46:03.027+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:03.027+0000] {subprocess.py:93} INFO -     "addBatch" : 200,
[2025-07-18T15:46:03.027+0000] {subprocess.py:93} INFO -     "commitOffsets" : 23,
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -     "triggerExecution" : 246,
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -     "walCommit" : 15
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:03.028+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:46:03.029+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:03.029+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:03.029+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:03.029+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.029+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.029+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:03.029+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:03.030+0000] {subprocess.py:93} INFO -         "0" : 74
[2025-07-18T15:46:03.030+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.030+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.030+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:03.030+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:03.031+0000] {subprocess.py:93} INFO -         "0" : 74
[2025-07-18T15:46:03.031+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.032+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.032+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:46:03.032+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.873563218390805,
[2025-07-18T15:46:03.033+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.065040650406504,
[2025-07-18T15:46:03.033+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:03.033+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:03.034+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:03.034+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:03.034+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:03.034+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:03.035+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:03.035+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:46:03.035+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:46:03.035+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:03.035+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/29 using temp file file:/tmp/checkpoints/checkins/offsets/.29.e357625f-e7a1-4188-85de-21bfdf4d0635.tmp
[2025-07-18T15:46:03.044+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/25 using temp file file:/tmp/checkpoints/feedback/offsets/.25.5d6c06f6-c42c-4c42-af86-7eb0f93ee526.tmp
[2025-07-18T15:46:03.044+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SnapshotProducer: Committed snapshot 5947821193175617413 (FastAppend)
[2025-07-18T15:46:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.29.e357625f-e7a1-4188-85de-21bfdf4d0635.tmp to file:/tmp/checkpoints/checkins/offsets/29
[2025-07-18T15:46:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1752853563026,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:03.072+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.074+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.075+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5947821193175617413, sequenceNumber=27, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.125147625S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=27}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=75}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3038}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=81613}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:03.075+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.076+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committed in 126 ms
[2025-07-18T15:46:03.077+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:46:03.077+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.079+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.080+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.082+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.25.5d6c06f6-c42c-4c42-af86-7eb0f93ee526.tmp to file:/tmp/checkpoints/feedback/offsets/25
[2025-07-18T15:46:03.090+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Committed offsets for batch 25. Metadata OffsetSeqMetadata(0,1752853563036,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:03.095+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/26 using temp file file:/tmp/checkpoints/reservations/commits/.26.7b197d6f-3053-4cdf-9b97-94ad1e830a2e.tmp
[2025-07-18T15:46:03.096+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.100+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.102+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.103+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.104+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.104+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.105+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:46:03.105+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.105+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.105+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.106+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.106+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.106+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.107+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.107+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:46:03.107+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.108+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.109+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.110+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.111+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.111+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.112+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.112+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.113+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.113+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:46:03.116+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.117+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.1 MiB)
[2025-07-18T15:46:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:46:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 28 from start at <unknown>:0
[2025-07-18T15:46:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:03.119+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:03.119+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.120+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Got job 14 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:03.120+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Final stage: ResultStage 14 (start at <unknown>:0)
[2025-07-18T15:46:03.120+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:03.120+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:03.121+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:03.123+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 28.0 KiB, free 434.1 MiB)
[2025-07-18T15:46:03.126+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.26.7b197d6f-3053-4cdf-9b97-94ad1e830a2e.tmp to file:/tmp/checkpoints/reservations/commits/26
[2025-07-18T15:46:03.126+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T15:46:03.127+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.127+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:03.127+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[59] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:03.127+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
[2025-07-18T15:46:03.128+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:46:03.128+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:03.129+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:46:03.129+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:46:03.130+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:03.132+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:02.294Z",
[2025-07-18T15:46:03.132+0000] {subprocess.py:93} INFO -   "batchId" : 26,
[2025-07-18T15:46:03.133+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:46:03.133+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.264392324093817,
[2025-07-18T15:46:03.134+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.4125452352231607,
[2025-07-18T15:46:03.135+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:03.135+0000] {subprocess.py:93} INFO -     "addBatch" : 723,
[2025-07-18T15:46:03.135+0000] {subprocess.py:93} INFO -     "commitOffsets" : 55,
[2025-07-18T15:46:03.135+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:03.136+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:46:03.136+0000] {subprocess.py:93} INFO -     "queryPlanning" : 18,
[2025-07-18T15:46:03.136+0000] {subprocess.py:93} INFO -     "triggerExecution" : 829,
[2025-07-18T15:46:03.136+0000] {subprocess.py:93} INFO -     "walCommit" : 31
[2025-07-18T15:46:03.136+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:03.136+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:03.137+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:03.137+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:46:03.137+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:03.137+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:46:03.137+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:03.137+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.138+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.138+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:03.138+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:46:03.138+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:46:03.138+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.139+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.139+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:03.139+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:46:03.139+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:46:03.139+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.139+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.139+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:46:03.140+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.264392324093817,
[2025-07-18T15:46:03.140+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.4125452352231607,
[2025-07-18T15:46:03.140+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:03.140+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:03.141+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:03.141+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:03.141+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:03.141+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:46:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 30 from start at <unknown>:0
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=74 untilOffset=75, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=29 taskId=14 partitionId=0
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Got job 15 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Final stage: ResultStage 15 (start at <unknown>:0)
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 74 for partition checkins-0
[2025-07-18T15:46:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:03.144+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:46:03.144+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 27.5 KiB, free 434.0 MiB)
[2025-07-18T15:46:03.144+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.0 MiB)
[2025-07-18T15:46:03.145+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.145+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:03.145+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[63] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:03.146+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
[2025-07-18T15:46:03.146+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:46:03.146+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
[2025-07-18T15:46:03.154+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:03.155+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=72 untilOffset=73, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=25 taskId=15 partitionId=0
[2025-07-18T15:46:03.157+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 72 for partition feedback-0
[2025-07-18T15:46:03.158+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:46:03.229+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:03.230+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:46:03.230+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=74, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:03.231+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T15:46:03.244+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DataWritingSparkTask: Committed partition 0 (task 15, attempt 0, stage 15.0)
[2025-07-18T15:46:03.244+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 73005000 nanos, during time span of 87252292 nanos.
[2025-07-18T15:46:03.245+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 4767 bytes result sent to driver
[2025-07-18T15:46:03.246+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 102 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-07-18T15:46:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: ResultStage 15 (start at <unknown>:0) finished in 0.111 s
[2025-07-18T15:46:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:03.248+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished
[2025-07-18T15:46:03.248+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Job 15 finished: start at <unknown>:0, took 0.115599 s
[2025-07-18T15:46:03.248+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:03.248+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committing epoch 25 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:46:03.258+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.321+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v27.metadata.json
[2025-07-18T15:46:03.341+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SnapshotProducer: Committed snapshot 480342573759006763 (FastAppend)
[2025-07-18T15:46:03.361+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=480342573759006763, sequenceNumber=26, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.100294208S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=26}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=73}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=77628}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:03.362+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committed in 101 ms
[2025-07-18T15:46:03.362+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 25, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:46:03.363+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/25 using temp file file:/tmp/checkpoints/feedback/commits/.25.2ff7b306-a3a2-42e1-beb8-e5775c6583b6.tmp
[2025-07-18T15:46:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.25.2ff7b306-a3a2-42e1-beb8-e5775c6583b6.tmp to file:/tmp/checkpoints/feedback/commits/25
[2025-07-18T15:46:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:03.373+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:03.034Z",
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "batchId" : 25,
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.9585798816568047,
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:03.374+0000] {subprocess.py:93} INFO -     "addBatch" : 269,
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -     "commitOffsets" : 15,
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -     "queryPlanning" : 11,
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -     "triggerExecution" : 338,
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -     "walCommit" : 39
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:03.375+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -         "0" : 72
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.376+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.9585798816568047,
[2025-07-18T15:46:03.377+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:03.378+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:03.379+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/26 using temp file file:/tmp/checkpoints/feedback/offsets/.26.777db816-2a4b-4b48-b758-2085b7a5bf05.tmp
[2025-07-18T15:46:03.389+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.26.777db816-2a4b-4b48-b758-2085b7a5bf05.tmp to file:/tmp/checkpoints/feedback/offsets/26
[2025-07-18T15:46:03.389+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Committed offsets for batch 26. Metadata OffsetSeqMetadata(0,1752853563373,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:03.393+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.393+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.393+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.394+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.395+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.399+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.399+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.400+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.400+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.401+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.407+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.407+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.408+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.408+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.408+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.415+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:46:03.416+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:46:03.417+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.417+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 32 from start at <unknown>:0
[2025-07-18T15:46:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Got job 16 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Final stage: ResultStage 16 (start at <unknown>:0)
[2025-07-18T15:46:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:03.420+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 27.5 KiB, free 433.9 MiB)
[2025-07-18T15:46:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.9 MiB)
[2025-07-18T15:46:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:46:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[67] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-07-18T15:46:03.422+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:46:03.422+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2025-07-18T15:46:03.427+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:03.427+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=73 untilOffset=74, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=26 taskId=16 partitionId=0
[2025-07-18T15:46:03.430+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 73 for partition feedback-0
[2025-07-18T15:46:03.431+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:46:03.431+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:03.431+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:46:03.432+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=75, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:03.433+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T15:46:03.444+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DataWritingSparkTask: Committed partition 0 (task 16, attempt 0, stage 16.0)
[2025-07-18T15:46:03.444+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 2357792 nanos, during time span of 14221291 nanos.
[2025-07-18T15:46:03.445+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 4767 bytes result sent to driver
[2025-07-18T15:46:03.447+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 24 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-07-18T15:46:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: ResultStage 16 (start at <unknown>:0) finished in 0.028 s
[2025-07-18T15:46:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-07-18T15:46:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Job 16 finished: start at <unknown>:0, took 0.029396 s
[2025-07-18T15:46:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committing epoch 26 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:46:03.454+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v28.metadata.json
[2025-07-18T15:46:03.526+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SnapshotProducer: Committed snapshot 1086051877102946345 (FastAppend)
[2025-07-18T15:46:03.542+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=1086051877102946345, sequenceNumber=27, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.083517125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=27}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=74}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2835}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=80463}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:03.542+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committed in 84 ms
[2025-07-18T15:46:03.543+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 26, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:46:03.545+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/26 using temp file file:/tmp/checkpoints/feedback/commits/.26.dde56fce-ab13-4e65-8b11-34f12e4e66ab.tmp
[2025-07-18T15:46:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.26.dde56fce-ab13-4e65-8b11-34f12e4e66ab.tmp to file:/tmp/checkpoints/feedback/commits/26
[2025-07-18T15:46:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:03.373Z",
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "batchId" : 26,
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.949852507374631,
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 5.46448087431694,
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:03.558+0000] {subprocess.py:93} INFO -     "addBatch" : 143,
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "commitOffsets" : 18,
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "latestOffset" : 0,
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "triggerExecution" : 183,
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:46:03.559+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -         "0" : 73
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -         "0" : 74
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:03.560+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -         "0" : 74
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.949852507374631,
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 5.46448087431694,
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:03.561+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:03.562+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/27 using temp file file:/tmp/checkpoints/feedback/offsets/.27.6a80f8bd-bad5-4088-9a9b-1c98751a6b21.tmp
[2025-07-18T15:46:03.581+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.27.6a80f8bd-bad5-4088-9a9b-1c98751a6b21.tmp to file:/tmp/checkpoints/feedback/offsets/27
[2025-07-18T15:46:03.583+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1752853563558,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:46:03.597+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.597+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.598+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.599+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.616+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.618+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.619+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.622+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.623+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.636+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.644+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:03.653+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.658+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:46:03.677+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:03.684+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:46:03.690+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=75, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:03.691+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T15:46:03.708+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:46:03.709+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:46:03.709+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:46:03.711+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 34 from start at <unknown>:0
[2025-07-18T15:46:03.716+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:46:03.721+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:46:03.725+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Got job 17 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:46:03.725+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Final stage: ResultStage 17 (start at <unknown>:0)
[2025-07-18T15:46:03.726+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:46:03.727+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:46:03.735+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:46:03.735+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 27.5 KiB, free 433.8 MiB)
[2025-07-18T15:46:03.738+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.8 MiB)
[2025-07-18T15:46:03.738+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:46:03.739+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:46:03.740+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[71] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:46:03.743+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
[2025-07-18T15:46:03.744+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:46:03.745+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2025-07-18T15:46:03.747+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:46:03.748+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DataWritingSparkTask: Committed partition 0 (task 14, attempt 0, stage 14.0)
[2025-07-18T15:46:03.751+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 517006542 nanos, during time span of 599197834 nanos.
[2025-07-18T15:46:03.752+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 4858 bytes result sent to driver
[2025-07-18T15:46:03.753+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 612 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:03.756+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-07-18T15:46:03.756+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=74 untilOffset=75, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=27 taskId=17 partitionId=0
[2025-07-18T15:46:03.757+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: ResultStage 14 (start at <unknown>:0) finished in 0.622 s
[2025-07-18T15:46:03.757+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:03.757+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
[2025-07-18T15:46:03.757+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 74 for partition feedback-0
[2025-07-18T15:46:03.757+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:46:03.759+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO DAGScheduler: Job 14 finished: start at <unknown>:0, took 0.631687 s
[2025-07-18T15:46:03.760+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:03.761+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committing epoch 29 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:46:03.772+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:46:03.892+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v31.metadata.json
[2025-07-18T15:46:03.937+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SnapshotProducer: Committed snapshot 4750395457928461186 (FastAppend)
[2025-07-18T15:46:03.969+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4750395457928461186, sequenceNumber=30, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.19701175S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=30}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=75}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2911}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=88827}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:03.969+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO SparkWrite: Committed in 197 ms
[2025-07-18T15:46:03.969+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:46:03.976+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/29 using temp file file:/tmp/checkpoints/checkins/commits/.29.63896920-d190-4448-beda-e5eb2c887d9b.tmp
[2025-07-18T15:46:03.994+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.29.63896920-d190-4448-beda-e5eb2c887d9b.tmp to file:/tmp/checkpoints/checkins/commits/29
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO - 25/07/18 15:46:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:03.024Z",
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "batchId" : 29,
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.048582995951417,
[2025-07-18T15:46:03.995+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.0319917440660475,
[2025-07-18T15:46:03.997+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:03.997+0000] {subprocess.py:93} INFO -     "addBatch" : 892,
[2025-07-18T15:46:03.997+0000] {subprocess.py:93} INFO -     "commitOffsets" : 25,
[2025-07-18T15:46:03.997+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:03.997+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:46:03.997+0000] {subprocess.py:93} INFO -     "queryPlanning" : 15,
[2025-07-18T15:46:03.997+0000] {subprocess.py:93} INFO -     "triggerExecution" : 969,
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -     "walCommit" : 33
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -         "0" : 74
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:03.998+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.048582995951417,
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.0319917440660475,
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:03.999+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:04.000+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:04.000+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:04.000+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:46:04.000+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:46:04.000+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:04.000+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:04.252+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:04.253+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:46:04.253+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=75, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:46:04.254+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T15:46:04.271+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO DataWritingSparkTask: Committed partition 0 (task 17, attempt 0, stage 17.0)
[2025-07-18T15:46:04.272+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 504403583 nanos, during time span of 523756000 nanos.
[2025-07-18T15:46:04.276+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 4769 bytes result sent to driver
[2025-07-18T15:46:04.277+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 559 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:46:04.279+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-07-18T15:46:04.279+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO DAGScheduler: ResultStage 17 (start at <unknown>:0) finished in 0.574 s
[2025-07-18T15:46:04.279+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:46:04.279+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-07-18T15:46:04.280+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO DAGScheduler: Job 17 finished: start at <unknown>:0, took 0.575997 s
[2025-07-18T15:46:04.280+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:46:04.280+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO SparkWrite: Committing epoch 27 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:46:04.289+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:46:04.363+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v29.metadata.json
[2025-07-18T15:46:04.392+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO SnapshotProducer: Committed snapshot 38862978661661286 (FastAppend)
[2025-07-18T15:46:04.405+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=38862978661661286, sequenceNumber=28, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.11510725S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=28}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=75}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2771}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=83234}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:46:04.405+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO SparkWrite: Committed in 115 ms
[2025-07-18T15:46:04.405+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:46:04.409+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/27 using temp file file:/tmp/checkpoints/feedback/commits/.27.ac8ef0b3-56c4-4a76-b46e-5ebf9378ae63.tmp
[2025-07-18T15:46:04.423+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.27.ac8ef0b3-56c4-4a76-b46e-5ebf9378ae63.tmp to file:/tmp/checkpoints/feedback/commits/27
[2025-07-18T15:46:04.423+0000] {subprocess.py:93} INFO - 25/07/18 15:46:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:46:04.423+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:46:03.557Z",
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "batchId" : 27,
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 5.434782608695652,
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.1560693641618498,
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -     "addBatch" : 800,
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -     "commitOffsets" : 19,
[2025-07-18T15:46:04.424+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -     "queryPlanning" : 22,
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -     "triggerExecution" : 865,
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -     "walCommit" : 22
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -         "0" : 74
[2025-07-18T15:46:04.425+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:04.426+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:04.426+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:46:04.426+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:04.426+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:46:04.428+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:04.428+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:04.428+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:46:04.428+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:46:04.428+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:46:04.428+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:46:04.428+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 5.434782608695652,
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.1560693641618498,
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:46:04.429+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:46:04.430+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:46:13.125+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:13.925+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:13.929+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:13.932+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:46:13.933+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:46:13.937+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:46:13.939+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:46:13.942+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:46:13.943+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:46:13.945+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:46:13.995+0000] {subprocess.py:93} INFO - 25/07/18 15:46:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:14.424+0000] {subprocess.py:93} INFO - 25/07/18 15:46:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:23.125+0000] {subprocess.py:93} INFO - 25/07/18 15:46:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:23.997+0000] {subprocess.py:93} INFO - 25/07/18 15:46:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:24.424+0000] {subprocess.py:93} INFO - 25/07/18 15:46:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:33.130+0000] {subprocess.py:93} INFO - 25/07/18 15:46:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:34.006+0000] {subprocess.py:93} INFO - 25/07/18 15:46:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:34.433+0000] {subprocess.py:93} INFO - 25/07/18 15:46:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:43.133+0000] {subprocess.py:93} INFO - 25/07/18 15:46:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:44.011+0000] {subprocess.py:93} INFO - 25/07/18 15:46:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:44.437+0000] {subprocess.py:93} INFO - 25/07/18 15:46:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:53.133+0000] {subprocess.py:93} INFO - 25/07/18 15:46:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:54.020+0000] {subprocess.py:93} INFO - 25/07/18 15:46:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:46:54.436+0000] {subprocess.py:93} INFO - 25/07/18 15:46:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:47:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:04.026+0000] {subprocess.py:93} INFO - 25/07/18 15:47:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:04.437+0000] {subprocess.py:93} INFO - 25/07/18 15:47:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:13.145+0000] {subprocess.py:93} INFO - 25/07/18 15:47:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:14.037+0000] {subprocess.py:93} INFO - 25/07/18 15:47:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:14.440+0000] {subprocess.py:93} INFO - 25/07/18 15:47:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:15.529+0000] {subprocess.py:93} INFO - 25/07/18 15:47:15 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2025-07-18T15:47:15.532+0000] {subprocess.py:93} INFO - 25/07/18 15:47:15 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
[2025-07-18T15:47:15.532+0000] {subprocess.py:93} INFO - 25/07/18 15:47:15 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
[2025-07-18T15:47:23.146+0000] {subprocess.py:93} INFO - 25/07/18 15:47:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:24.042+0000] {subprocess.py:93} INFO - 25/07/18 15:47:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:24.451+0000] {subprocess.py:93} INFO - 25/07/18 15:47:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:33.153+0000] {subprocess.py:93} INFO - 25/07/18 15:47:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:34.046+0000] {subprocess.py:93} INFO - 25/07/18 15:47:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:34.457+0000] {subprocess.py:93} INFO - 25/07/18 15:47:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:43.161+0000] {subprocess.py:93} INFO - 25/07/18 15:47:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:44.056+0000] {subprocess.py:93} INFO - 25/07/18 15:47:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:44.463+0000] {subprocess.py:93} INFO - 25/07/18 15:47:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:53.171+0000] {subprocess.py:93} INFO - 25/07/18 15:47:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:54.066+0000] {subprocess.py:93} INFO - 25/07/18 15:47:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:47:54.462+0000] {subprocess.py:93} INFO - 25/07/18 15:47:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:01.416+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/27 using temp file file:/tmp/checkpoints/reservations/offsets/.27.f9791ebb-acd6-4485-abfd-c6274f926d24.tmp
[2025-07-18T15:48:01.438+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.27.f9791ebb-acd6-4485-abfd-c6274f926d24.tmp to file:/tmp/checkpoints/reservations/offsets/27
[2025-07-18T15:48:01.439+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MicroBatchExecution: Committed offsets for batch 27. Metadata OffsetSeqMetadata(0,1752853681404,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:01.455+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.455+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.456+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.461+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.469+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.478+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.478+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.478+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.479+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.480+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.487+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.488+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.488+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.488+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.489+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.501+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:48:01.507+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:48:01.508+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:48:01.509+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkContext: Created broadcast 36 from start at <unknown>:0
[2025-07-18T15:48:01.513+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:01.514+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:01.514+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Got job 18 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:01.514+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Final stage: ResultStage 18 (start at <unknown>:0)
[2025-07-18T15:48:01.514+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:01.514+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:01.514+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:01.515+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:48:01.518+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:48:01.518+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:48:01.519+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:01.519+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[75] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:01.519+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-07-18T15:48:01.520+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:48:01.521+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
[2025-07-18T15:48:01.529+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:01.530+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=75 untilOffset=76, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=27 taskId=18 partitionId=0
[2025-07-18T15:48:01.534+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 75 for partition reservations-0
[2025-07-18T15:48:01.539+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:48:01.602+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:01.603+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:48:01.603+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=77, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:01.605+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T15:48:01.630+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DataWritingSparkTask: Committed partition 0 (task 18, attempt 0, stage 18.0)
[2025-07-18T15:48:01.630+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 68723708 nanos, during time span of 94912250 nanos.
[2025-07-18T15:48:01.631+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 4693 bytes result sent to driver
[2025-07-18T15:48:01.633+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 112 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:01.634+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-07-18T15:48:01.634+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: ResultStage 18 (start at <unknown>:0) finished in 0.121 s
[2025-07-18T15:48:01.635+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:01.635+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-07-18T15:48:01.635+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Job 18 finished: start at <unknown>:0, took 0.123524 s
[2025-07-18T15:48:01.635+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:01.636+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Committing epoch 27 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:48:01.647+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.721+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v29.metadata.json
[2025-07-18T15:48:01.750+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SnapshotProducer: Committed snapshot 3527740043904743527 (FastAppend)
[2025-07-18T15:48:01.784+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=3527740043904743527, sequenceNumber=28, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.135832375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=28}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=76}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2941}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=84554}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:01.785+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Committed in 136 ms
[2025-07-18T15:48:01.786+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 27, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:48:01.793+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/27 using temp file file:/tmp/checkpoints/reservations/commits/.27.bf01394f-5df0-4ba5-96fb-26bfaf75a317.tmp
[2025-07-18T15:48:01.834+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.27.bf01394f-5df0-4ba5-96fb-26bfaf75a317.tmp to file:/tmp/checkpoints/reservations/commits/27
[2025-07-18T15:48:01.835+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:01.835+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:48:01.836+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:48:01.836+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:01.836+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:01.403Z",
[2025-07-18T15:48:01.836+0000] {subprocess.py:93} INFO -   "batchId" : 27,
[2025-07-18T15:48:01.837+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:48:01.837+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:48:01.838+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.3255813953488373,
[2025-07-18T15:48:01.838+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:01.838+0000] {subprocess.py:93} INFO -     "addBatch" : 309,
[2025-07-18T15:48:01.838+0000] {subprocess.py:93} INFO -     "commitOffsets" : 51,
[2025-07-18T15:48:01.839+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:48:01.839+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:48:01.840+0000] {subprocess.py:93} INFO -     "queryPlanning" : 36,
[2025-07-18T15:48:01.840+0000] {subprocess.py:93} INFO -     "triggerExecution" : 430,
[2025-07-18T15:48:01.841+0000] {subprocess.py:93} INFO -     "walCommit" : 33
[2025-07-18T15:48:01.841+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:01.841+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:01.841+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:01.842+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:48:01.842+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:01.842+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:48:01.842+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:48:01.843+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:01.843+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:01.843+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:01.843+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:48:01.843+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:01.843+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:48:01.844+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.3255813953488373,
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:01.845+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:01.846+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:48:01.846+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:48:01.846+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:01.846+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:01.846+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/28 using temp file file:/tmp/checkpoints/reservations/offsets/.28.c49f16e9-8455-4947-8a90-25f459e8ed95.tmp
[2025-07-18T15:48:01.856+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.28.c49f16e9-8455-4947-8a90-25f459e8ed95.tmp to file:/tmp/checkpoints/reservations/offsets/28
[2025-07-18T15:48:01.856+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1752853681836,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:01.861+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.861+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.861+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.863+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.864+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.867+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.867+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.867+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.868+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.868+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.872+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.872+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.872+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:01.873+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.874+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:01.881+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:48:01.884+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:48:01.885+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:01.885+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkContext: Created broadcast 38 from start at <unknown>:0
[2025-07-18T15:48:01.885+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:01.885+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:01.886+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Got job 19 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:01.886+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Final stage: ResultStage 19 (start at <unknown>:0)
[2025-07-18T15:48:01.886+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:01.887+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:01.887+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:01.887+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T15:48:01.890+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T15:48:01.890+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:01.890+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:01.891+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[79] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:01.891+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0
[2025-07-18T15:48:01.891+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:48:01.892+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
[2025-07-18T15:48:01.896+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:01.897+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=76 untilOffset=78, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=28 taskId=19 partitionId=0
[2025-07-18T15:48:01.900+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 76 for partition reservations-0
[2025-07-18T15:48:01.900+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:48:01.901+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:01.901+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:48:01.902+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=78, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:01.903+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 77 for partition reservations-0
[2025-07-18T15:48:01.903+0000] {subprocess.py:93} INFO - 25/07/18 15:48:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:48:02.028+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/30 using temp file file:/tmp/checkpoints/checkins/offsets/.30.e2342f54-3778-4cff-83dd-3b01143d37ae.tmp
[2025-07-18T15:48:02.043+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.30.e2342f54-3778-4cff-83dd-3b01143d37ae.tmp to file:/tmp/checkpoints/checkins/offsets/30
[2025-07-18T15:48:02.044+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1752853682021,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:02.049+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.049+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.049+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.050+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.053+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.059+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.059+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.060+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.060+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.060+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.064+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.065+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.065+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.066+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.066+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.073+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:48:02.075+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T15:48:02.076+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.076+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 40 from start at <unknown>:0
[2025-07-18T15:48:02.077+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:02.077+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:02.077+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Got job 20 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:02.077+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Final stage: ResultStage 20 (start at <unknown>:0)
[2025-07-18T15:48:02.077+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:02.077+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:02.078+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[83] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:02.078+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T15:48:02.080+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:48:02.080+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.080+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:02.081+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[83] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:02.081+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0
[2025-07-18T15:48:02.081+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:48:02.082+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
[2025-07-18T15:48:02.086+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:02.086+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=75 untilOffset=76, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=30 taskId=20 partitionId=0
[2025-07-18T15:48:02.089+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 75 for partition checkins-0
[2025-07-18T15:48:02.090+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:48:02.213+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.213+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:48:02.214+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=77, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.215+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T15:48:02.235+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Committed partition 0 (task 20, attempt 0, stage 20.0)
[2025-07-18T15:48:02.236+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 124729375 nanos, during time span of 145230667 nanos.
[2025-07-18T15:48:02.238+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 4861 bytes result sent to driver
[2025-07-18T15:48:02.242+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 158 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:02.247+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-07-18T15:48:02.247+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: ResultStage 20 (start at <unknown>:0) finished in 0.163 s
[2025-07-18T15:48:02.247+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:02.248+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished
[2025-07-18T15:48:02.248+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 20 finished: start at <unknown>:0, took 0.164692 s
[2025-07-18T15:48:02.248+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:02.248+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing epoch 30 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:48:02.255+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.324+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v32.metadata.json
[2025-07-18T15:48:02.344+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SnapshotProducer: Committed snapshot 7161012892132478444 (FastAppend)
[2025-07-18T15:48:02.356+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=7161012892132478444, sequenceNumber=31, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.101217625S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=31}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=76}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2890}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=91717}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:02.357+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committed in 101 ms
[2025-07-18T15:48:02.357+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:48:02.361+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/30 using temp file file:/tmp/checkpoints/checkins/commits/.30.6ee83b0d-931d-4362-8535-4dbc9c289f9f.tmp
[2025-07-18T15:48:02.373+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.30.6ee83b0d-931d-4362-8535-4dbc9c289f9f.tmp to file:/tmp/checkpoints/checkins/commits/30
[2025-07-18T15:48:02.374+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:02.374+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:48:02.374+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:48:02.374+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:02.374+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:02.020Z",
[2025-07-18T15:48:02.374+0000] {subprocess.py:93} INFO -   "batchId" : 30,
[2025-07-18T15:48:02.374+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:48:02.375+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 62.5,
[2025-07-18T15:48:02.375+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.8409090909090913,
[2025-07-18T15:48:02.375+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:02.375+0000] {subprocess.py:93} INFO -     "addBatch" : 303,
[2025-07-18T15:48:02.375+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T15:48:02.375+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:48:02.375+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:48:02.376+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:48:02.376+0000] {subprocess.py:93} INFO -     "triggerExecution" : 352,
[2025-07-18T15:48:02.376+0000] {subprocess.py:93} INFO -     "walCommit" : 21
[2025-07-18T15:48:02.376+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:02.376+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:02.376+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:02.377+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:48:02.377+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:02.377+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:02.377+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:48:02.377+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.377+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.377+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:02.378+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:02.378+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:02.378+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.378+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.378+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:02.378+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:02.379+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:02.379+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.379+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.379+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:48:02.379+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 62.5,
[2025-07-18T15:48:02.380+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.8409090909090913,
[2025-07-18T15:48:02.380+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:02.380+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:02.380+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:02.380+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:02.380+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:02.381+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:02.381+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:02.381+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:48:02.381+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:48:02.381+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:02.381+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:02.382+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/31 using temp file file:/tmp/checkpoints/checkins/offsets/.31.2a8fbc4c-adfe-4925-9804-0b8f43852e6f.tmp
[2025-07-18T15:48:02.392+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.31.2a8fbc4c-adfe-4925-9804-0b8f43852e6f.tmp to file:/tmp/checkpoints/checkins/offsets/31
[2025-07-18T15:48:02.393+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1752853682374,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:02.396+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.397+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.397+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.398+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.399+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.402+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.403+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.403+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.404+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.405+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.406+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.406+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:48:02.407+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=78, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.407+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T15:48:02.411+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.413+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.413+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.414+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.414+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.422+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:48:02.422+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Committed partition 0 (task 19, attempt 0, stage 19.0)
[2025-07-18T15:48:02.422+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 505197291 nanos, during time span of 521156709 nanos.
[2025-07-18T15:48:02.423+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 4698 bytes result sent to driver
[2025-07-18T15:48:02.424+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 531 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:02.424+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool
[2025-07-18T15:48:02.424+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:48:02.425+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: ResultStage 19 (start at <unknown>:0) finished in 0.537 s
[2025-07-18T15:48:02.425+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.426+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 42 from start at <unknown>:0
[2025-07-18T15:48:02.426+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:02.426+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:02.427+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished
[2025-07-18T15:48:02.428+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 19 finished: start at <unknown>:0, took 0.539088 s
[2025-07-18T15:48:02.428+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:02.428+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:02.429+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing epoch 28 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:48:02.430+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Got job 21 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:02.431+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Final stage: ResultStage 21 (start at <unknown>:0)
[2025-07-18T15:48:02.431+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:02.431+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[87] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:48:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[87] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0
[2025-07-18T15:48:02.433+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:48:02.433+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
[2025-07-18T15:48:02.437+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:48:02.438+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:02.439+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=76 untilOffset=77, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=31 taskId=21 partitionId=0
[2025-07-18T15:48:02.442+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 76 for partition checkins-0
[2025-07-18T15:48:02.443+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:48:02.444+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.445+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:48:02.445+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=78, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.445+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 21, attempt 0, stage 21.0)
[2025-07-18T15:48:02.457+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Committed partition 0 (task 21, attempt 0, stage 21.0)
[2025-07-18T15:48:02.457+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 2112375 nanos, during time span of 15139625 nanos.
[2025-07-18T15:48:02.458+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Finished task 0.0 in stage 21.0 (TID 21). 4815 bytes result sent to driver
[2025-07-18T15:48:02.458+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 28 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:02.459+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2025-07-18T15:48:02.459+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: ResultStage 21 (start at <unknown>:0) finished in 0.033 s
[2025-07-18T15:48:02.460+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:02.460+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2025-07-18T15:48:02.460+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 21 finished: start at <unknown>:0, took 0.034392 s
[2025-07-18T15:48:02.460+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:02.460+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing epoch 31 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:48:02.466+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.523+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v30.metadata.json
[2025-07-18T15:48:02.535+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v33.metadata.json
[2025-07-18T15:48:02.548+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SnapshotProducer: Committed snapshot 1556146168896485316 (FastAppend)
[2025-07-18T15:48:02.557+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SnapshotProducer: Committed snapshot 1903449848898222143 (FastAppend)
[2025-07-18T15:48:02.566+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=1556146168896485316, sequenceNumber=29, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.127859792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=29}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=78}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3044}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=87598}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committed in 128 ms
[2025-07-18T15:48:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:48:02.571+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/28 using temp file file:/tmp/checkpoints/reservations/commits/.28.e3be72e8-9ec7-477e-ac6a-3dcca03a21c0.tmp
[2025-07-18T15:48:02.573+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=1903449848898222143, sequenceNumber=32, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.106023125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=32}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=77}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2918}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=94635}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:02.573+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committed in 106 ms
[2025-07-18T15:48:02.574+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:48:02.578+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/31 using temp file file:/tmp/checkpoints/checkins/commits/.31.a15e9c9f-b0e0-4da7-83ad-f90c573380bf.tmp
[2025-07-18T15:48:02.588+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.28.e3be72e8-9ec7-477e-ac6a-3dcca03a21c0.tmp to file:/tmp/checkpoints/reservations/commits/28
[2025-07-18T15:48:02.589+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:02.589+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:48:02.589+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:48:02.589+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:01.834Z",
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -   "batchId" : 28,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.640371229698376,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.6595744680851063,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -     "addBatch" : 701,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:48:02.590+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:48:02.591+0000] {subprocess.py:93} INFO -     "triggerExecution" : 752,
[2025-07-18T15:48:02.591+0000] {subprocess.py:93} INFO -     "walCommit" : 19
[2025-07-18T15:48:02.591+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:02.591+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:02.592+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:02.592+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:48:02.592+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:02.592+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:48:02.592+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:02.592+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.592+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.593+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:02.593+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:48:02.593+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.594+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.640371229698376,
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.6595744680851063,
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:48:02.595+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.31.a15e9c9f-b0e0-4da7-83ad-f90c573380bf.tmp to file:/tmp/checkpoints/checkins/commits/31
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:02.373Z",
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO -   "batchId" : 31,
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:48:02.596+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.8328611898017,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.566210045662101,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -     "addBatch" : 172,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -     "commitOffsets" : 20,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -     "triggerExecution" : 219,
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -     "walCommit" : 17
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:02.597+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:02.598+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:02.599+0000] {subprocess.py:93} INFO -         "0" : 77
[2025-07-18T15:48:02.599+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.599+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.599+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:02.599+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:02.599+0000] {subprocess.py:93} INFO -         "0" : 77
[2025-07-18T15:48:02.599+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.8328611898017,
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.566210045662101,
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:02.600+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:02.601+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/32 using temp file file:/tmp/checkpoints/checkins/offsets/.32.009622aa-bba2-4405-b197-b474521fa045.tmp
[2025-07-18T15:48:02.609+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.32.009622aa-bba2-4405-b197-b474521fa045.tmp to file:/tmp/checkpoints/checkins/offsets/32
[2025-07-18T15:48:02.609+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1752853682595,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:02.614+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.615+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.615+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.616+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.616+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.619+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.619+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.620+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.620+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.621+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.624+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.625+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.625+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:02.625+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.627+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.632+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/28 using temp file file:/tmp/checkpoints/feedback/offsets/.28.ea6dbf1a-8715-445b-9889-dc87174a3670.tmp
[2025-07-18T15:48:02.635+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.638+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.639+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.1 MiB)
[2025-07-18T15:48:02.640+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 44 from start at <unknown>:0
[2025-07-18T15:48:02.642+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:02.643+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:02.643+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Got job 22 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:02.644+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Final stage: ResultStage 22 (start at <unknown>:0)
[2025-07-18T15:48:02.645+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:02.645+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:02.645+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[91] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:02.646+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 28.0 KiB, free 433.7 MiB)
[2025-07-18T15:48:02.646+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.7 MiB)
[2025-07-18T15:48:02.646+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:48:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[91] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:02.647+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-07-18T15:48:02.652+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:48:02.652+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
[2025-07-18T15:48:02.652+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.28.ea6dbf1a-8715-445b-9889-dc87174a3670.tmp to file:/tmp/checkpoints/feedback/offsets/28
[2025-07-18T15:48:02.652+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Committed offsets for batch 28. Metadata OffsetSeqMetadata(0,1752853682626,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:02.655+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:02.656+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.656+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=77 untilOffset=78, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=32 taskId=22 partitionId=0
[2025-07-18T15:48:02.657+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.657+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.658+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.659+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.664+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 77 for partition checkins-0
[2025-07-18T15:48:02.665+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:48:02.665+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.665+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.666+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.666+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.667+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.670+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.670+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.670+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.671+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.674+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:02.681+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:48:02.684+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.685+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.687+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.688+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.688+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:48:02.688+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 46 from start at <unknown>:0
[2025-07-18T15:48:02.688+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:02.688+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:02.688+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Got job 23 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:02.690+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Final stage: ResultStage 23 (start at <unknown>:0)
[2025-07-18T15:48:02.691+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:02.692+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:02.693+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[95] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:02.693+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 27.5 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.694+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.8 MiB)
[2025-07-18T15:48:02.694+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:48:02.694+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:02.694+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[95] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:02.694+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0
[2025-07-18T15:48:02.695+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:48:02.695+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
[2025-07-18T15:48:02.696+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:48:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:02.699+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=75 untilOffset=76, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=28 taskId=23 partitionId=0
[2025-07-18T15:48:02.700+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 75 for partition feedback-0
[2025-07-18T15:48:02.701+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.702+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:48:02.704+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.707+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.709+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:48:02.818+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.819+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:48:02.819+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=77, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:02.821+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 23, attempt 0, stage 23.0)
[2025-07-18T15:48:02.836+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DataWritingSparkTask: Committed partition 0 (task 23, attempt 0, stage 23.0)
[2025-07-18T15:48:02.837+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 119238167 nanos, during time span of 136231958 nanos.
[2025-07-18T15:48:02.837+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 4773 bytes result sent to driver
[2025-07-18T15:48:02.837+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 145 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:02.837+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool
[2025-07-18T15:48:02.838+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: ResultStage 23 (start at <unknown>:0) finished in 0.150 s
[2025-07-18T15:48:02.838+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:02.838+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished
[2025-07-18T15:48:02.839+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO DAGScheduler: Job 23 finished: start at <unknown>:0, took 0.151027 s
[2025-07-18T15:48:02.839+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:02.839+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing epoch 28 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:48:02.852+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:02.918+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v30.metadata.json
[2025-07-18T15:48:02.949+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SnapshotProducer: Committed snapshot 980958718970121265 (FastAppend)
[2025-07-18T15:48:02.970+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=980958718970121265, sequenceNumber=29, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.122256542S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=29}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=76}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2971}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=86205}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:02.973+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO SparkWrite: Committed in 123 ms
[2025-07-18T15:48:02.973+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 28, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:48:02.980+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/28 using temp file file:/tmp/checkpoints/feedback/commits/.28.ee3bbcae-aaf7-4bbe-9a86-0ecac6a00ca5.tmp
[2025-07-18T15:48:02.995+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.28.ee3bbcae-aaf7-4bbe-9a86-0ecac6a00ca5.tmp to file:/tmp/checkpoints/feedback/commits/28
[2025-07-18T15:48:02.997+0000] {subprocess.py:93} INFO - 25/07/18 15:48:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:02.998+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:48:02.998+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:48:02.998+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:02.998+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:02.625Z",
[2025-07-18T15:48:02.999+0000] {subprocess.py:93} INFO -   "batchId" : 28,
[2025-07-18T15:48:02.999+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:48:02.999+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:48:02.999+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.717391304347826,
[2025-07-18T15:48:03.000+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:03.000+0000] {subprocess.py:93} INFO -     "addBatch" : 310,
[2025-07-18T15:48:03.000+0000] {subprocess.py:93} INFO -     "commitOffsets" : 24,
[2025-07-18T15:48:03.001+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:48:03.002+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:48:03.002+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:48:03.002+0000] {subprocess.py:93} INFO -     "triggerExecution" : 368,
[2025-07-18T15:48:03.002+0000] {subprocess.py:93} INFO -     "walCommit" : 25
[2025-07-18T15:48:03.003+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:03.003+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:03.003+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:03.003+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:48:03.004+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:03.004+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:03.004+0000] {subprocess.py:93} INFO -         "0" : 75
[2025-07-18T15:48:03.004+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.005+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.005+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:03.005+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:03.005+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:03.006+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.006+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.006+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:03.007+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:03.007+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:03.007+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.007+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.008+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:48:03.008+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:48:03.008+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.717391304347826,
[2025-07-18T15:48:03.008+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:03.009+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:03.009+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:03.009+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:03.010+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:03.010+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:03.010+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:03.010+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:48:03.011+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:48:03.011+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:03.011+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:03.012+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/29 using temp file file:/tmp/checkpoints/feedback/offsets/.29.a91dca59-424a-4e1f-8f14-3b2099bf76e9.tmp
[2025-07-18T15:48:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.29.a91dca59-424a-4e1f-8f14-3b2099bf76e9.tmp to file:/tmp/checkpoints/feedback/offsets/29
[2025-07-18T15:48:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1752853682996,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:03.026+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.029+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.029+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.029+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.030+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.032+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.032+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.034+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.035+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.037+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.037+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.042+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:48:03.044+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:48:03.045+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:03.046+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkContext: Created broadcast 48 from start at <unknown>:0
[2025-07-18T15:48:03.047+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Got job 24 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Final stage: ResultStage 24 (start at <unknown>:0)
[2025-07-18T15:48:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[99] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 27.5 KiB, free 433.9 MiB)
[2025-07-18T15:48:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.9 MiB)
[2025-07-18T15:48:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:48:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[99] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2025-07-18T15:48:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:48:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
[2025-07-18T15:48:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:03.057+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=76 untilOffset=77, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=29 taskId=24 partitionId=0
[2025-07-18T15:48:03.061+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 76 for partition feedback-0
[2025-07-18T15:48:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:48:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:48:03.064+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=78, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:03.067+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 24, attempt 0, stage 24.0)
[2025-07-18T15:48:03.103+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DataWritingSparkTask: Committed partition 0 (task 24, attempt 0, stage 24.0)
[2025-07-18T15:48:03.104+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 5026584 nanos, during time span of 35366916 nanos.
[2025-07-18T15:48:03.108+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO Executor: Finished task 0.0 in stage 24.0 (TID 24). 4818 bytes result sent to driver
[2025-07-18T15:48:03.113+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 61 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool
[2025-07-18T15:48:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: ResultStage 24 (start at <unknown>:0) finished in 0.066 s
[2025-07-18T15:48:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2025-07-18T15:48:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Job 24 finished: start at <unknown>:0, took 0.068076 s
[2025-07-18T15:48:03.114+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:03.115+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Committing epoch 29 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:48:03.120+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.165+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:03.166+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:48:03.166+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=78, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:03.169+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 22, attempt 0, stage 22.0)
[2025-07-18T15:48:03.183+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DataWritingSparkTask: Committed partition 0 (task 22, attempt 0, stage 22.0)
[2025-07-18T15:48:03.184+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 505241833 nanos, during time span of 523623917 nanos.
[2025-07-18T15:48:03.185+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 4860 bytes result sent to driver
[2025-07-18T15:48:03.187+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 538 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:03.188+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool
[2025-07-18T15:48:03.188+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: ResultStage 22 (start at <unknown>:0) finished in 0.545 s
[2025-07-18T15:48:03.188+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:03.188+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-07-18T15:48:03.189+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Job 22 finished: start at <unknown>:0, took 0.547658 s
[2025-07-18T15:48:03.190+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:03.190+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Committing epoch 32 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:48:03.199+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:48:03.211+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v31.metadata.json
[2025-07-18T15:48:03.275+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SnapshotProducer: Committed snapshot 2778188849153880971 (FastAppend)
[2025-07-18T15:48:03.310+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=2778188849153880971, sequenceNumber=30, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.189048709S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=30}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=77}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2856}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=89061}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Committed in 191 ms
[2025-07-18T15:48:03.317+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:48:03.317+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v34.metadata.json
[2025-07-18T15:48:03.321+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/29 using temp file file:/tmp/checkpoints/feedback/commits/.29.25db9a8f-fff5-4799-bf8b-531bccf9e03e.tmp
[2025-07-18T15:48:03.358+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.29.25db9a8f-fff5-4799-bf8b-531bccf9e03e.tmp to file:/tmp/checkpoints/feedback/commits/29
[2025-07-18T15:48:03.359+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:03.359+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:48:03.359+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:48:03.360+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:03.360+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:02.995Z",
[2025-07-18T15:48:03.360+0000] {subprocess.py:93} INFO -   "batchId" : 29,
[2025-07-18T15:48:03.361+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:48:03.361+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.7027027027027026,
[2025-07-18T15:48:03.361+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.7624309392265194,
[2025-07-18T15:48:03.362+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:03.362+0000] {subprocess.py:93} INFO -     "addBatch" : 285,
[2025-07-18T15:48:03.362+0000] {subprocess.py:93} INFO -     "commitOffsets" : 47,
[2025-07-18T15:48:03.363+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:48:03.363+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:48:03.364+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:48:03.364+0000] {subprocess.py:93} INFO -     "triggerExecution" : 362,
[2025-07-18T15:48:03.364+0000] {subprocess.py:93} INFO -     "walCommit" : 21
[2025-07-18T15:48:03.364+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:03.365+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:03.365+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:03.366+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:48:03.366+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:03.366+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:03.367+0000] {subprocess.py:93} INFO -         "0" : 76
[2025-07-18T15:48:03.368+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.368+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.369+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:03.369+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:03.369+0000] {subprocess.py:93} INFO -         "0" : 77
[2025-07-18T15:48:03.370+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.370+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.370+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:03.371+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:03.372+0000] {subprocess.py:93} INFO -         "0" : 77
[2025-07-18T15:48:03.373+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.373+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.373+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:48:03.373+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.7027027027027026,
[2025-07-18T15:48:03.374+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.7624309392265194,
[2025-07-18T15:48:03.374+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:03.375+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:03.375+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:03.375+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:03.376+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:03.376+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:03.376+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:03.376+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:48:03.377+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:48:03.377+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:03.381+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:03.381+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SnapshotProducer: Committed snapshot 8002302705355784907 (FastAppend)
[2025-07-18T15:48:03.382+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/30 using temp file file:/tmp/checkpoints/feedback/offsets/.30.67a7ca88-4cae-4dd6-be09-22961673da5a.tmp
[2025-07-18T15:48:03.390+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=8002302705355784907, sequenceNumber=33, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.189359667S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=33}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=78}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2910}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=97545}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:03.391+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Committed in 190 ms
[2025-07-18T15:48:03.392+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:48:03.403+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/32 using temp file file:/tmp/checkpoints/checkins/commits/.32.d4ddc8ad-75e8-48ce-811e-ca25e77ace73.tmp
[2025-07-18T15:48:03.407+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.30.67a7ca88-4cae-4dd6-be09-22961673da5a.tmp to file:/tmp/checkpoints/feedback/offsets/30
[2025-07-18T15:48:03.409+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1752853683360,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:48:03.417+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.419+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.421+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.422+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.423+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.434+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.435+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.437+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.439+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.440+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.445+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.447+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.449+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.32.d4ddc8ad-75e8-48ce-811e-ca25e77ace73.tmp to file:/tmp/checkpoints/checkins/commits/32
[2025-07-18T15:48:03.450+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:03.450+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:03.451+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:48:03.453+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:48:03.454+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:03.454+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:02.593Z",
[2025-07-18T15:48:03.455+0000] {subprocess.py:93} INFO -   "batchId" : 32,
[2025-07-18T15:48:03.455+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:48:03.456+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.545454545454546,
[2025-07-18T15:48:03.456+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.17096018735363,
[2025-07-18T15:48:03.456+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:03.457+0000] {subprocess.py:93} INFO -     "addBatch" : 772,
[2025-07-18T15:48:03.457+0000] {subprocess.py:93} INFO -     "commitOffsets" : 57,
[2025-07-18T15:48:03.457+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:48:03.457+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:48:03.457+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:48:03.459+0000] {subprocess.py:93} INFO -     "triggerExecution" : 854,
[2025-07-18T15:48:03.460+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T15:48:03.461+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:03.461+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:03.462+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:03.463+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:48:03.463+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:03.464+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:03.464+0000] {subprocess.py:93} INFO -         "0" : 77
[2025-07-18T15:48:03.468+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.469+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.472+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:03.476+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:03.477+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:48:03.477+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.479+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.480+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:03.481+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:48:03.482+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:48:03.483+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:03.487+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:03.488+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:48:03.492+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.545454545454546,
[2025-07-18T15:48:03.493+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.17096018735363,
[2025-07-18T15:48:03.494+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:03.495+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:03.495+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:03.495+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:03.497+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:03.498+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:03.499+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:03.499+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:48:03.500+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:48:03.501+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:03.501+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:03.501+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.501+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:48:03.502+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:48:03.502+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:48:03.502+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:03.503+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkContext: Created broadcast 50 from start at <unknown>:0
[2025-07-18T15:48:03.503+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:48:03.503+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:48:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Got job 25 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:48:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Final stage: ResultStage 25 (start at <unknown>:0)
[2025-07-18T15:48:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:48:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:48:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[103] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:48:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 27.5 KiB, free 433.8 MiB)
[2025-07-18T15:48:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.8 MiB)
[2025-07-18T15:48:03.505+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:48:03.505+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:48:03.505+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[103] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:48:03.506+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0
[2025-07-18T15:48:03.506+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:48:03.506+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
[2025-07-18T15:48:03.506+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:48:03.506+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=77 untilOffset=78, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=30 taskId=25 partitionId=0
[2025-07-18T15:48:03.507+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 77 for partition feedback-0
[2025-07-18T15:48:03.507+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:48:03.998+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:03.998+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:48:03.998+0000] {subprocess.py:93} INFO - 25/07/18 15:48:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=78, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:48:04.000+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 25, attempt 0, stage 25.0)
[2025-07-18T15:48:04.012+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO DataWritingSparkTask: Committed partition 0 (task 25, attempt 0, stage 25.0)
[2025-07-18T15:48:04.013+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 502682459 nanos, during time span of 516603708 nanos.
[2025-07-18T15:48:04.013+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 4759 bytes result sent to driver
[2025-07-18T15:48:04.015+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 534 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:48:04.015+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2025-07-18T15:48:04.015+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO DAGScheduler: ResultStage 25 (start at <unknown>:0) finished in 0.542 s
[2025-07-18T15:48:04.015+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:48:04.015+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished
[2025-07-18T15:48:04.015+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO DAGScheduler: Job 25 finished: start at <unknown>:0, took 0.543265 s
[2025-07-18T15:48:04.015+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:48:04.016+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO SparkWrite: Committing epoch 30 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:48:04.023+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:48:04.093+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v32.metadata.json
[2025-07-18T15:48:04.113+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO SnapshotProducer: Committed snapshot 7795293595005840387 (FastAppend)
[2025-07-18T15:48:04.130+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7795293595005840387, sequenceNumber=31, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.105814209S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=31}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=78}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2794}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=91855}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:48:04.131+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO SparkWrite: Committed in 106 ms
[2025-07-18T15:48:04.131+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:48:04.135+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/30 using temp file file:/tmp/checkpoints/feedback/commits/.30.131366f6-8573-42d5-92be-ed8ee2b4bcb3.tmp
[2025-07-18T15:48:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.30.131366f6-8573-42d5-92be-ed8ee2b4bcb3.tmp to file:/tmp/checkpoints/feedback/commits/30
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO - 25/07/18 15:48:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:48:03.357Z",
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "batchId" : 30,
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.7624309392265194,
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.2674271229404308,
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:48:04.148+0000] {subprocess.py:93} INFO -     "addBatch" : 709,
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "commitOffsets" : 18,
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "queryPlanning" : 14,
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "triggerExecution" : 789,
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "walCommit" : 45
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:48:04.149+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -         "0" : 77
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:48:04.150+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.7624309392265194,
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.2674271229404308,
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:48:04.151+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:48:04.152+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:48:12.599+0000] {subprocess.py:93} INFO - 25/07/18 15:48:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:13.451+0000] {subprocess.py:93} INFO - 25/07/18 15:48:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:14.150+0000] {subprocess.py:93} INFO - 25/07/18 15:48:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:15.193+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:48:15.195+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:48:15.197+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:48:15.199+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:48:15.201+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:48:15.204+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:48:15.205+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:48:15.207+0000] {subprocess.py:93} INFO - 25/07/18 15:48:15 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:48:22.605+0000] {subprocess.py:93} INFO - 25/07/18 15:48:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:23.453+0000] {subprocess.py:93} INFO - 25/07/18 15:48:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:24.157+0000] {subprocess.py:93} INFO - 25/07/18 15:48:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:32.614+0000] {subprocess.py:93} INFO - 25/07/18 15:48:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:33.465+0000] {subprocess.py:93} INFO - 25/07/18 15:48:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:34.162+0000] {subprocess.py:93} INFO - 25/07/18 15:48:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:42.615+0000] {subprocess.py:93} INFO - 25/07/18 15:48:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:43.467+0000] {subprocess.py:93} INFO - 25/07/18 15:48:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:44.169+0000] {subprocess.py:93} INFO - 25/07/18 15:48:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:52.619+0000] {subprocess.py:93} INFO - 25/07/18 15:48:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:53.477+0000] {subprocess.py:93} INFO - 25/07/18 15:48:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:48:54.173+0000] {subprocess.py:93} INFO - 25/07/18 15:48:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:02.622+0000] {subprocess.py:93} INFO - 25/07/18 15:49:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:49:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:04.179+0000] {subprocess.py:93} INFO - 25/07/18 15:49:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:12.624+0000] {subprocess.py:93} INFO - 25/07/18 15:49:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:13.481+0000] {subprocess.py:93} INFO - 25/07/18 15:49:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:14.181+0000] {subprocess.py:93} INFO - 25/07/18 15:49:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:22.626+0000] {subprocess.py:93} INFO - 25/07/18 15:49:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:23.486+0000] {subprocess.py:93} INFO - 25/07/18 15:49:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:24.187+0000] {subprocess.py:93} INFO - 25/07/18 15:49:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:32.631+0000] {subprocess.py:93} INFO - 25/07/18 15:49:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:33.493+0000] {subprocess.py:93} INFO - 25/07/18 15:49:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:34.195+0000] {subprocess.py:93} INFO - 25/07/18 15:49:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:42.633+0000] {subprocess.py:93} INFO - 25/07/18 15:49:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:43.497+0000] {subprocess.py:93} INFO - 25/07/18 15:49:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:44.211+0000] {subprocess.py:93} INFO - 25/07/18 15:49:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:52.639+0000] {subprocess.py:93} INFO - 25/07/18 15:49:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:53.504+0000] {subprocess.py:93} INFO - 25/07/18 15:49:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:49:54.221+0000] {subprocess.py:93} INFO - 25/07/18 15:49:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:01.816+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/29 using temp file file:/tmp/checkpoints/reservations/offsets/.29.4e6a40c4-ec52-425c-bf8d-94a5c5cf242b.tmp
[2025-07-18T15:50:01.843+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.29.4e6a40c4-ec52-425c-bf8d-94a5c5cf242b.tmp to file:/tmp/checkpoints/reservations/offsets/29
[2025-07-18T15:50:01.844+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO MicroBatchExecution: Committed offsets for batch 29. Metadata OffsetSeqMetadata(0,1752853801798,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:50:01.860+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.860+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.860+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.864+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:01.866+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:01.871+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.872+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.872+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.873+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:01.874+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:01.894+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.895+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.896+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:01.896+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:01.897+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:01.917+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:50:01.918+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:50:01.918+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:50:01.920+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkContext: Created broadcast 52 from start at <unknown>:0
[2025-07-18T15:50:01.921+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:50:01.921+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:50:01.925+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO DAGScheduler: Got job 26 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:50:01.926+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO DAGScheduler: Final stage: ResultStage 26 (start at <unknown>:0)
[2025-07-18T15:50:01.926+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:50:01.926+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:50:01.926+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:50:01.926+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:50:01.929+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:50:01.930+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:50:01.931+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:50:01.931+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:50:01.931+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-07-18T15:50:01.932+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:50:01.935+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
[2025-07-18T15:50:01.943+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:50:01.945+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=78 untilOffset=79, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=29 taskId=26 partitionId=0
[2025-07-18T15:50:01.951+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 78 for partition reservations-0
[2025-07-18T15:50:01.956+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:50:01.999+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:01.999+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:50:02.000+0000] {subprocess.py:93} INFO - 25/07/18 15:50:01 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=80, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:02.001+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 26, attempt 0, stage 26.0)
[2025-07-18T15:50:02.044+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DataWritingSparkTask: Committed partition 0 (task 26, attempt 0, stage 26.0)
[2025-07-18T15:50:02.045+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 47732750 nanos, during time span of 94207334 nanos.
[2025-07-18T15:50:02.049+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 4701 bytes result sent to driver
[2025-07-18T15:50:02.050+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 115 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:50:02.051+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-07-18T15:50:02.051+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: ResultStage 26 (start at <unknown>:0) finished in 0.125 s
[2025-07-18T15:50:02.051+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:50:02.052+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2025-07-18T15:50:02.052+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Job 26 finished: start at <unknown>:0, took 0.128919 s
[2025-07-18T15:50:02.052+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:50:02.052+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committing epoch 29 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:50:02.065+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.148+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v31.metadata.json
[2025-07-18T15:50:02.181+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SnapshotProducer: Committed snapshot 6852887713776537449 (FastAppend)
[2025-07-18T15:50:02.207+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6852887713776537449, sequenceNumber=30, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.141894958S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=30}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=79}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2969}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=90567}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:50:02.208+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committed in 142 ms
[2025-07-18T15:50:02.209+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 29, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:50:02.214+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/29 using temp file file:/tmp/checkpoints/reservations/commits/.29.efa735e9-3de6-4490-aab3-9966332125b4.tmp
[2025-07-18T15:50:02.249+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.29.efa735e9-3de6-4490-aab3-9966332125b4.tmp to file:/tmp/checkpoints/reservations/commits/29
[2025-07-18T15:50:02.251+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:50:02.253+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:50:02.253+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:50:02.254+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:50:02.255+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:50:01.797Z",
[2025-07-18T15:50:02.255+0000] {subprocess.py:93} INFO -   "batchId" : 29,
[2025-07-18T15:50:02.255+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:50:02.255+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:50:02.255+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.2222222222222223,
[2025-07-18T15:50:02.256+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:50:02.256+0000] {subprocess.py:93} INFO -     "addBatch" : 338,
[2025-07-18T15:50:02.256+0000] {subprocess.py:93} INFO -     "commitOffsets" : 42,
[2025-07-18T15:50:02.256+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:50:02.256+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:50:02.258+0000] {subprocess.py:93} INFO -     "queryPlanning" : 28,
[2025-07-18T15:50:02.258+0000] {subprocess.py:93} INFO -     "triggerExecution" : 450,
[2025-07-18T15:50:02.258+0000] {subprocess.py:93} INFO -     "walCommit" : 39
[2025-07-18T15:50:02.258+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:50:02.259+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:50:02.260+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:50:02.261+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:50:02.261+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:50:02.262+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:50:02.263+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:50:02.263+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:02.264+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:02.264+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:50:02.264+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:50:02.264+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:02.265+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:02.265+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:02.265+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:50:02.265+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:50:02.266+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:02.266+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:02.266+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:02.266+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:50:02.267+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:50:02.267+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.2222222222222223,
[2025-07-18T15:50:02.267+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:50:02.267+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:50:02.268+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:50:02.268+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:50:02.268+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:50:02.268+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:50:02.269+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:50:02.269+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:50:02.269+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:50:02.269+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:50:02.269+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:50:02.270+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/30 using temp file file:/tmp/checkpoints/reservations/offsets/.30.7eb5e297-6432-4ce7-a842-6898e81521fe.tmp
[2025-07-18T15:50:02.281+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.30.7eb5e297-6432-4ce7-a842-6898e81521fe.tmp to file:/tmp/checkpoints/reservations/offsets/30
[2025-07-18T15:50:02.281+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MicroBatchExecution: Committed offsets for batch 30. Metadata OffsetSeqMetadata(0,1752853802253,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:50:02.285+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.286+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.286+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.287+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.287+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.295+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.295+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.295+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.296+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.296+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.297+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.298+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.298+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.300+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.301+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.311+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:50:02.313+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:50:02.314+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:50:02.314+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Created broadcast 54 from start at <unknown>:0
[2025-07-18T15:50:02.314+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:50:02.315+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:50:02.315+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Got job 27 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:50:02.315+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Final stage: ResultStage 27 (start at <unknown>:0)
[2025-07-18T15:50:02.315+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:50:02.315+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:50:02.315+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[111] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:50:02.315+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T15:50:02.319+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T15:50:02.321+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:50:02.322+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:50:02.322+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[111] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:50:02.326+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0
[2025-07-18T15:50:02.327+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:50:02.328+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
[2025-07-18T15:50:02.328+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:50:02.328+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=79 untilOffset=81, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=30 taskId=27 partitionId=0
[2025-07-18T15:50:02.332+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 79 for partition reservations-0
[2025-07-18T15:50:02.333+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:50:02.334+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:02.335+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:50:02.336+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=81, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:02.336+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 80 for partition reservations-0
[2025-07-18T15:50:02.338+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:50:02.408+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/33 using temp file file:/tmp/checkpoints/checkins/offsets/.33.12143bf9-5ad4-4a87-80cb-3b20eee94dbd.tmp
[2025-07-18T15:50:02.427+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.33.12143bf9-5ad4-4a87-80cb-3b20eee94dbd.tmp to file:/tmp/checkpoints/checkins/offsets/33
[2025-07-18T15:50:02.431+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1752853802400,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:50:02.431+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.431+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.432+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.434+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.435+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.441+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.442+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.442+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.442+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.444+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.448+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.448+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.449+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.451+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.452+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.463+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:50:02.466+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T15:50:02.467+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:50:02.467+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Created broadcast 56 from start at <unknown>:0
[2025-07-18T15:50:02.467+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:50:02.468+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:50:02.469+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Got job 28 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:50:02.469+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Final stage: ResultStage 28 (start at <unknown>:0)
[2025-07-18T15:50:02.469+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:50:02.472+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:50:02.473+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[115] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:50:02.473+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T15:50:02.474+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:50:02.474+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:50:02.475+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:50:02.475+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[115] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:50:02.476+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2025-07-18T15:50:02.476+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:50:02.477+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
[2025-07-18T15:50:02.482+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:50:02.483+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=78 untilOffset=79, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=33 taskId=28 partitionId=0
[2025-07-18T15:50:02.484+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 78 for partition checkins-0
[2025-07-18T15:50:02.487+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:50:02.604+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:02.605+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:50:02.605+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=80, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:02.606+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 28, attempt 0, stage 28.0)
[2025-07-18T15:50:02.620+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DataWritingSparkTask: Committed partition 0 (task 28, attempt 0, stage 28.0)
[2025-07-18T15:50:02.621+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 119623667 nanos, during time span of 135353709 nanos.
[2025-07-18T15:50:02.622+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 4822 bytes result sent to driver
[2025-07-18T15:50:02.624+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 147 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:50:02.625+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2025-07-18T15:50:02.626+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: ResultStage 28 (start at <unknown>:0) finished in 0.155 s
[2025-07-18T15:50:02.626+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:50:02.626+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2025-07-18T15:50:02.627+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Job 28 finished: start at <unknown>:0, took 0.156504 s
[2025-07-18T15:50:02.627+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:50:02.627+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committing epoch 33 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:50:02.644+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.767+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v35.metadata.json
[2025-07-18T15:50:02.813+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SnapshotProducer: Committed snapshot 7341346366513943882 (FastAppend)
[2025-07-18T15:50:02.836+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=7341346366513943882, sequenceNumber=34, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.190309167S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=34}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=79}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2904}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=100449}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:50:02.837+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committed in 192 ms
[2025-07-18T15:50:02.837+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:50:02.842+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:02.843+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:50:02.843+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/33 using temp file file:/tmp/checkpoints/checkins/commits/.33.956be940-237e-4465-9a0e-84aeb4622e1c.tmp
[2025-07-18T15:50:02.843+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=81, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:02.846+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 27, attempt 0, stage 27.0)
[2025-07-18T15:50:02.866+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DataWritingSparkTask: Committed partition 0 (task 27, attempt 0, stage 27.0)
[2025-07-18T15:50:02.867+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 510800042 nanos, during time span of 534078959 nanos.
[2025-07-18T15:50:02.868+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 4701 bytes result sent to driver
[2025-07-18T15:50:02.869+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 546 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:50:02.870+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2025-07-18T15:50:02.870+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: ResultStage 27 (start at <unknown>:0) finished in 0.555 s
[2025-07-18T15:50:02.870+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.33.956be940-237e-4465-9a0e-84aeb4622e1c.tmp to file:/tmp/checkpoints/checkins/commits/33
[2025-07-18T15:50:02.870+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:50:02.871+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2025-07-18T15:50:02.871+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Job 27 finished: start at <unknown>:0, took 0.556461 s
[2025-07-18T15:50:02.872+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:50:02.873+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:50:02.876+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:50:02.876+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:50:02.876+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:50:02.398Z",
[2025-07-18T15:50:02.876+0000] {subprocess.py:93} INFO -   "batchId" : 33,
[2025-07-18T15:50:02.877+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:50:02.877+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:50:02.877+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.1321961620469083,
[2025-07-18T15:50:02.877+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:50:02.877+0000] {subprocess.py:93} INFO -     "addBatch" : 399,
[2025-07-18T15:50:02.877+0000] {subprocess.py:93} INFO -     "commitOffsets" : 32,
[2025-07-18T15:50:02.877+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:50:02.878+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:50:02.878+0000] {subprocess.py:93} INFO -     "queryPlanning" : 13,
[2025-07-18T15:50:02.878+0000] {subprocess.py:93} INFO -     "triggerExecution" : 469,
[2025-07-18T15:50:02.879+0000] {subprocess.py:93} INFO -     "walCommit" : 23
[2025-07-18T15:50:02.880+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:50:02.880+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:50:02.881+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:50:02.881+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:50:02.881+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:50:02.881+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:50:02.884+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:50:02.884+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:50:02.885+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.1321961620469083,
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:50:02.886+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:50:02.887+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:50:02.887+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:50:02.887+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:50:02.887+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:50:02.887+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:50:02.887+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:50:02.888+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:50:02.889+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:50:02.890+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committing epoch 30 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:50:02.890+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/34 using temp file file:/tmp/checkpoints/checkins/offsets/.34.806634dd-acd4-4886-8106-2a79ccc95a72.tmp
[2025-07-18T15:50:02.890+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:50:02.916+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.34.806634dd-acd4-4886-8106-2a79ccc95a72.tmp to file:/tmp/checkpoints/checkins/offsets/34
[2025-07-18T15:50:02.921+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1752853802869,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:50:02.924+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.925+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.925+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.928+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.930+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.942+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.942+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.943+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.943+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.944+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.951+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.953+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.954+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:02.956+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.958+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:02.970+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:50:02.978+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:50:02.979+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:50:02.980+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Created broadcast 58 from start at <unknown>:0
[2025-07-18T15:50:02.980+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:50:02.980+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:50:02.981+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Got job 29 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:50:02.981+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Final stage: ResultStage 29 (start at <unknown>:0)
[2025-07-18T15:50:02.981+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:50:02.981+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:50:02.982+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[119] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:50:02.983+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T15:50:02.991+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T15:50:02.992+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:50:02.992+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:50:02.993+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[119] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:50:02.994+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0
[2025-07-18T15:50:02.994+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:50:02.995+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
[2025-07-18T15:50:02.997+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v32.metadata.json
[2025-07-18T15:50:02.999+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:50:03.000+0000] {subprocess.py:93} INFO - 25/07/18 15:50:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=79 untilOffset=81, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=34 taskId=29 partitionId=0
[2025-07-18T15:50:03.006+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 79 for partition checkins-0
[2025-07-18T15:50:03.007+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:50:03.008+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.009+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:50:03.010+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=81, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.013+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 80 for partition checkins-0
[2025-07-18T15:50:03.014+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:50:03.030+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/31 using temp file file:/tmp/checkpoints/feedback/offsets/.31.4a3af091-2760-4725-9178-ea7731bfd9eb.tmp
[2025-07-18T15:50:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SnapshotProducer: Committed snapshot 5000143426115058646 (FastAppend)
[2025-07-18T15:50:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.31.4a3af091-2760-4725-9178-ea7731bfd9eb.tmp to file:/tmp/checkpoints/feedback/offsets/31
[2025-07-18T15:50:03.044+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1752853803022,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:50:03.048+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5000143426115058646, sequenceNumber=31, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.163715333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=31}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=81}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3007}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=93574}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:50:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committed in 164 ms
[2025-07-18T15:50:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 30, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:50:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/30 using temp file file:/tmp/checkpoints/reservations/commits/.30.c211f491-d1a1-48c8-a8a1-2b5cc57e87f5.tmp
[2025-07-18T15:50:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.061+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.071+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:50:03.074+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.30.c211f491-d1a1-48c8-a8a1-2b5cc57e87f5.tmp to file:/tmp/checkpoints/reservations/commits/30
[2025-07-18T15:50:03.075+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:50:03.075+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:50:03.076+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:50:03.076+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:50:03.076+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:50:02.250Z",
[2025-07-18T15:50:03.076+0000] {subprocess.py:93} INFO -   "batchId" : 30,
[2025-07-18T15:50:03.077+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:50:03.077+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.415011037527593,
[2025-07-18T15:50:03.077+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.444987775061125,
[2025-07-18T15:50:03.077+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:50:03.077+0000] {subprocess.py:93} INFO -     "addBatch" : 759,
[2025-07-18T15:50:03.078+0000] {subprocess.py:93} INFO -     "commitOffsets" : 21,
[2025-07-18T15:50:03.078+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:50:03.078+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:50:03.078+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:50:03.078+0000] {subprocess.py:93} INFO -     "triggerExecution" : 818,
[2025-07-18T15:50:03.078+0000] {subprocess.py:93} INFO -     "walCommit" : 26
[2025-07-18T15:50:03.078+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:50:03.079+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:50:03.079+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:50:03.079+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:50:03.079+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:50:03.079+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:50:03.079+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:03.079+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:50:03.080+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:50:03.081+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:50:03.081+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.081+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.081+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:50:03.081+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.415011037527593,
[2025-07-18T15:50:03.081+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.444987775061125,
[2025-07-18T15:50:03.081+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:50:03.082+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:50:03.082+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:50:03.082+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:50:03.082+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:50:03.082+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:50:03.082+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:50:03.083+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:50:03.083+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:50:03.083+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:50:03.083+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:50:03.083+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:50:03.083+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.083+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Created broadcast 60 from start at <unknown>:0
[2025-07-18T15:50:03.084+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:50:03.084+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:50:03.084+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Got job 30 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:50:03.084+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Final stage: ResultStage 30 (start at <unknown>:0)
[2025-07-18T15:50:03.084+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:50:03.084+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:50:03.085+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[123] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:50:03.085+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 27.5 KiB, free 433.7 MiB)
[2025-07-18T15:50:03.085+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.7 MiB)
[2025-07-18T15:50:03.086+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.086+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:50:03.086+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[123] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:50:03.086+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0
[2025-07-18T15:50:03.087+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:50:03.091+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
[2025-07-18T15:50:03.092+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:50:03.094+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=78 untilOffset=79, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=31 taskId=30 partitionId=0
[2025-07-18T15:50:03.096+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 78 for partition feedback-0
[2025-07-18T15:50:03.099+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:50:03.211+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.212+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:50:03.212+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=80, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.213+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 30, attempt 0, stage 30.0)
[2025-07-18T15:50:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DataWritingSparkTask: Committed partition 0 (task 30, attempt 0, stage 30.0)
[2025-07-18T15:50:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 115757291 nanos, during time span of 128536958 nanos.
[2025-07-18T15:50:03.226+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO Executor: Finished task 0.0 in stage 30.0 (TID 30). 4758 bytes result sent to driver
[2025-07-18T15:50:03.227+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 141 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:50:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool
[2025-07-18T15:50:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: ResultStage 30 (start at <unknown>:0) finished in 0.149 s
[2025-07-18T15:50:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:50:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished
[2025-07-18T15:50:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Job 30 finished: start at <unknown>:0, took 0.150647 s
[2025-07-18T15:50:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:50:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committing epoch 31 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:50:03.234+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.285+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v33.metadata.json
[2025-07-18T15:50:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SnapshotProducer: Committed snapshot 1355578815946525886 (FastAppend)
[2025-07-18T15:50:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=1355578815946525886, sequenceNumber=32, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.08202575S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=32}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=79}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2761}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=94616}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:50:03.316+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committed in 82 ms
[2025-07-18T15:50:03.317+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:50:03.319+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/31 using temp file file:/tmp/checkpoints/feedback/commits/.31.7722028c-9928-465e-bcef-5055d4b1061a.tmp
[2025-07-18T15:50:03.329+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.31.7722028c-9928-465e-bcef-5055d4b1061a.tmp to file:/tmp/checkpoints/feedback/commits/31
[2025-07-18T15:50:03.329+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:50:03.019Z",
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "batchId" : 31,
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 62.5,
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 3.236245954692557,
[2025-07-18T15:50:03.330+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -     "addBatch" : 265,
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -     "commitOffsets" : 12,
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -     "triggerExecution" : 309,
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -     "walCommit" : 21
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:50:03.331+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -         "0" : 78
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.332+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 62.5,
[2025-07-18T15:50:03.333+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 3.236245954692557,
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:50:03.334+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:50:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/32 using temp file file:/tmp/checkpoints/feedback/offsets/.32.ff3dc8fc-48f3-4ee1-9698-b00c173b6cd2.tmp
[2025-07-18T15:50:03.343+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.32.ff3dc8fc-48f3-4ee1-9698-b00c173b6cd2.tmp to file:/tmp/checkpoints/feedback/offsets/32
[2025-07-18T15:50:03.344+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1752853803330,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:50:03.347+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.348+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.348+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.348+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.349+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.352+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.352+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.352+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.353+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.354+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.358+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.358+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.358+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.359+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.360+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.367+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
[2025-07-18T15:50:03.369+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.7 MiB)
[2025-07-18T15:50:03.369+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.369+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Created broadcast 62 from start at <unknown>:0
[2025-07-18T15:50:03.369+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:50:03.369+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:50:03.370+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Got job 31 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:50:03.370+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Final stage: ResultStage 31 (start at <unknown>:0)
[2025-07-18T15:50:03.370+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:50:03.370+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:50:03.371+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[127] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:50:03.371+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 27.5 KiB, free 433.6 MiB)
[2025-07-18T15:50:03.372+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.6 MiB)
[2025-07-18T15:50:03.372+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.372+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:50:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[127] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:50:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0
[2025-07-18T15:50:03.373+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:50:03.378+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
[2025-07-18T15:50:03.379+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:50:03.379+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=79 untilOffset=80, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=32 taskId=31 partitionId=0
[2025-07-18T15:50:03.381+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 79 for partition feedback-0
[2025-07-18T15:50:03.381+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:50:03.412+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.413+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:50:03.414+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=81, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.414+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 31, attempt 0, stage 31.0)
[2025-07-18T15:50:03.423+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DataWritingSparkTask: Committed partition 0 (task 31, attempt 0, stage 31.0)
[2025-07-18T15:50:03.424+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 31040458 nanos, during time span of 42709334 nanos.
[2025-07-18T15:50:03.424+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO Executor: Finished task 0.0 in stage 31.0 (TID 31). 4773 bytes result sent to driver
[2025-07-18T15:50:03.425+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 52 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:50:03.426+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool
[2025-07-18T15:50:03.426+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: ResultStage 31 (start at <unknown>:0) finished in 0.055 s
[2025-07-18T15:50:03.426+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:50:03.426+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished
[2025-07-18T15:50:03.427+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Job 31 finished: start at <unknown>:0, took 0.055868 s
[2025-07-18T15:50:03.427+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:50:03.427+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committing epoch 32 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:50:03.431+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v34.metadata.json
[2025-07-18T15:50:03.499+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SnapshotProducer: Committed snapshot 1916671255531080055 (FastAppend)
[2025-07-18T15:50:03.511+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=1916671255531080055, sequenceNumber=33, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.079523833S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=33}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=80}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2956}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=97572}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:50:03.512+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committed in 80 ms
[2025-07-18T15:50:03.512+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:50:03.516+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/32 using temp file file:/tmp/checkpoints/feedback/commits/.32.023a829f-6117-4760-bc2c-2d26fa8a4dfa.tmp
[2025-07-18T15:50:03.517+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.519+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:50:03.520+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=81, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:03.520+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 29, attempt 0, stage 29.0)
[2025-07-18T15:50:03.527+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DataWritingSparkTask: Committed partition 0 (task 29, attempt 0, stage 29.0)
[2025-07-18T15:50:03.529+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 2 records through 2 polls (polled  out 2 records), taking 507462626 nanos, during time span of 522429000 nanos.
[2025-07-18T15:50:03.529+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 4825 bytes result sent to driver
[2025-07-18T15:50:03.530+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.32.023a829f-6117-4760-bc2c-2d26fa8a4dfa.tmp to file:/tmp/checkpoints/feedback/commits/32
[2025-07-18T15:50:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 538 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:50:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool
[2025-07-18T15:50:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:50:03.532+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:50:03.533+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:50:03.533+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:50:03.533+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:50:03.329Z",
[2025-07-18T15:50:03.534+0000] {subprocess.py:93} INFO -   "batchId" : 32,
[2025-07-18T15:50:03.534+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:50:03.534+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.2258064516129035,
[2025-07-18T15:50:03.534+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 5.025125628140703,
[2025-07-18T15:50:03.534+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:50:03.535+0000] {subprocess.py:93} INFO -     "addBatch" : 161,
[2025-07-18T15:50:03.535+0000] {subprocess.py:93} INFO -     "commitOffsets" : 18,
[2025-07-18T15:50:03.535+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:50:03.535+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:50:03.536+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:50:03.536+0000] {subprocess.py:93} INFO -     "triggerExecution" : 199,
[2025-07-18T15:50:03.536+0000] {subprocess.py:93} INFO -     "walCommit" : 13
[2025-07-18T15:50:03.536+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:50:03.537+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:50:03.537+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:50:03.537+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:50:03.538+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:50:03.538+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:03.538+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:03.539+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.539+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.539+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:50:03.539+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:03.539+0000] {subprocess.py:93} INFO -         "0" : 80
[2025-07-18T15:50:03.540+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.540+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.540+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:50:03.540+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:03.540+0000] {subprocess.py:93} INFO -         "0" : 80
[2025-07-18T15:50:03.541+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.541+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.542+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:50:03.542+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.2258064516129035,
[2025-07-18T15:50:03.543+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 5.025125628140703,
[2025-07-18T15:50:03.543+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:50:03.543+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:50:03.544+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:50:03.544+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:50:03.544+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:50:03.544+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:50:03.544+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:50:03.545+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:50:03.546+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:50:03.546+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:50:03.547+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:50:03.547+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: ResultStage 29 (start at <unknown>:0) finished in 0.550 s
[2025-07-18T15:50:03.547+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:50:03.548+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2025-07-18T15:50:03.548+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Job 29 finished: start at <unknown>:0, took 0.555047 s
[2025-07-18T15:50:03.548+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:50:03.549+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committing epoch 34 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:50:03.549+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/33 using temp file file:/tmp/checkpoints/feedback/offsets/.33.7b61160e-74ad-4642-8330-cdc7a25adf68.tmp
[2025-07-18T15:50:03.550+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.550+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.550+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:50:03.550+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.552+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.1 MiB)
[2025-07-18T15:50:03.554+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:50:03.559+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_63_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:50:03.563+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.33.7b61160e-74ad-4642-8330-cdc7a25adf68.tmp to file:/tmp/checkpoints/feedback/offsets/33
[2025-07-18T15:50:03.564+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1752853803531,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:50:03.565+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:50:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:50:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.570+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.570+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.572+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:50:03.575+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.578+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_62_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:50:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.583+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:50:03.589+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.589+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.589+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:03.592+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.592+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:50:03.600+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:50:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:50:03.604+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:50:03.604+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Created broadcast 64 from start at <unknown>:0
[2025-07-18T15:50:03.604+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:50:03.605+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:50:03.606+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Got job 32 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:50:03.606+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Final stage: ResultStage 32 (start at <unknown>:0)
[2025-07-18T15:50:03.606+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:50:03.607+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:50:03.608+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[131] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:50:03.608+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T15:50:03.608+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.1 MiB)
[2025-07-18T15:50:03.608+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 77cb57a6bd53:38973 (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T15:50:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:50:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[131] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:50:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2025-07-18T15:50:03.616+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:50:03.617+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
[2025-07-18T15:50:03.619+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:50:03.620+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=80 untilOffset=81, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=33 taskId=32 partitionId=0
[2025-07-18T15:50:03.626+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 80 for partition feedback-0
[2025-07-18T15:50:03.626+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v36.metadata.json
[2025-07-18T15:50:03.627+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:50:03.671+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SnapshotProducer: Committed snapshot 4110823694584189946 (FastAppend)
[2025-07-18T15:50:03.694+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4110823694584189946, sequenceNumber=35, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.145117208S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=35}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=81}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2918}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=103367}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:50:03.696+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO SparkWrite: Committed in 146 ms
[2025-07-18T15:50:03.697+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:50:03.700+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/34 using temp file file:/tmp/checkpoints/checkins/commits/.34.0bb13f96-218b-4b66-bc9f-69c8adced13e.tmp
[2025-07-18T15:50:03.745+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.34.0bb13f96-218b-4b66-bc9f-69c8adced13e.tmp to file:/tmp/checkpoints/checkins/commits/34
[2025-07-18T15:50:03.745+0000] {subprocess.py:93} INFO - 25/07/18 15:50:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:50:03.746+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:50:03.746+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:50:03.747+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:50:03.747+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:50:02.868Z",
[2025-07-18T15:50:03.747+0000] {subprocess.py:93} INFO -   "batchId" : 34,
[2025-07-18T15:50:03.747+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:50:03.747+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.25531914893617,
[2025-07-18T15:50:03.748+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.2857142857142856,
[2025-07-18T15:50:03.748+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:50:03.748+0000] {subprocess.py:93} INFO -     "addBatch" : 759,
[2025-07-18T15:50:03.748+0000] {subprocess.py:93} INFO -     "commitOffsets" : 51,
[2025-07-18T15:50:03.748+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:50:03.749+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:50:03.749+0000] {subprocess.py:93} INFO -     "queryPlanning" : 18,
[2025-07-18T15:50:03.749+0000] {subprocess.py:93} INFO -     "triggerExecution" : 875,
[2025-07-18T15:50:03.749+0000] {subprocess.py:93} INFO -     "walCommit" : 44
[2025-07-18T15:50:03.749+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:50:03.750+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:50:03.750+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:50:03.750+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:50:03.751+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:50:03.753+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:50:03.753+0000] {subprocess.py:93} INFO -         "0" : 79
[2025-07-18T15:50:03.755+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.758+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.758+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:50:03.758+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:50:03.759+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:50:03.759+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:50:03.760+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.25531914893617,
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.2857142857142856,
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:50:03.761+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:50:03.762+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:50:03.762+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:50:03.762+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:50:04.132+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:04.133+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:50:04.133+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=81, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:50:04.133+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 32, attempt 0, stage 32.0)
[2025-07-18T15:50:04.143+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO DataWritingSparkTask: Committed partition 0 (task 32, attempt 0, stage 32.0)
[2025-07-18T15:50:04.143+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 504004584 nanos, during time span of 517494542 nanos.
[2025-07-18T15:50:04.144+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO Executor: Finished task 0.0 in stage 32.0 (TID 32). 4775 bytes result sent to driver
[2025-07-18T15:50:04.145+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 532 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:50:04.145+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool
[2025-07-18T15:50:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO DAGScheduler: ResultStage 32 (start at <unknown>:0) finished in 0.541 s
[2025-07-18T15:50:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:50:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
[2025-07-18T15:50:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO DAGScheduler: Job 32 finished: start at <unknown>:0, took 0.543648 s
[2025-07-18T15:50:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:50:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO SparkWrite: Committing epoch 33 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:50:04.155+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:50:04.217+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v35.metadata.json
[2025-07-18T15:50:04.238+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO SnapshotProducer: Committed snapshot 1740298073517955432 (FastAppend)
[2025-07-18T15:50:04.251+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=1740298073517955432, sequenceNumber=34, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.096505917S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=34}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=81}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2748}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=100320}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:50:04.252+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO SparkWrite: Committed in 96 ms
[2025-07-18T15:50:04.252+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:50:04.256+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/33 using temp file file:/tmp/checkpoints/feedback/commits/.33.44f97e93-7bca-45b8-8e99-4e4fc54fff78.tmp
[2025-07-18T15:50:04.267+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.33.44f97e93-7bca-45b8-8e99-4e4fc54fff78.tmp to file:/tmp/checkpoints/feedback/commits/33
[2025-07-18T15:50:04.267+0000] {subprocess.py:93} INFO - 25/07/18 15:50:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:50:04.267+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:50:04.267+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:50:04.267+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:50:04.268+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:50:03.529Z",
[2025-07-18T15:50:04.268+0000] {subprocess.py:93} INFO -   "batchId" : 33,
[2025-07-18T15:50:04.268+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 5.0,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.3568521031207599,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -     "addBatch" : 675,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -     "queryPlanning" : 13,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -     "triggerExecution" : 737,
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -     "walCommit" : 30
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:50:04.269+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -         "0" : 80
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:50:04.270+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:50:04.271+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:50:04.271+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:50:04.271+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:50:04.272+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:50:04.272+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 5.0,
[2025-07-18T15:50:04.272+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.3568521031207599,
[2025-07-18T15:50:04.272+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:50:04.272+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:50:04.272+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:50:04.272+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:50:04.273+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:50:04.273+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:50:04.273+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:50:04.273+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:50:04.273+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:50:04.273+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:50:04.273+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:50:13.081+0000] {subprocess.py:93} INFO - 25/07/18 15:50:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:13.757+0000] {subprocess.py:93} INFO - 25/07/18 15:50:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:14.267+0000] {subprocess.py:93} INFO - 25/07/18 15:50:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:20.083+0000] {subprocess.py:93} INFO - 25/07/18 15:50:20 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:50:20.086+0000] {subprocess.py:93} INFO - 25/07/18 15:50:20 INFO BlockManagerInfo: Removed broadcast_65_piece0 on 77cb57a6bd53:38973 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:50:20.088+0000] {subprocess.py:93} INFO - 25/07/18 15:50:20 INFO BlockManagerInfo: Removed broadcast_64_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:50:23.089+0000] {subprocess.py:93} INFO - 25/07/18 15:50:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:23.765+0000] {subprocess.py:93} INFO - 25/07/18 15:50:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:24.272+0000] {subprocess.py:93} INFO - 25/07/18 15:50:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:33.092+0000] {subprocess.py:93} INFO - 25/07/18 15:50:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:33.766+0000] {subprocess.py:93} INFO - 25/07/18 15:50:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:34.279+0000] {subprocess.py:93} INFO - 25/07/18 15:50:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:43.095+0000] {subprocess.py:93} INFO - 25/07/18 15:50:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:43.778+0000] {subprocess.py:93} INFO - 25/07/18 15:50:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:44.286+0000] {subprocess.py:93} INFO - 25/07/18 15:50:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:53.105+0000] {subprocess.py:93} INFO - 25/07/18 15:50:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:53.791+0000] {subprocess.py:93} INFO - 25/07/18 15:50:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:50:54.295+0000] {subprocess.py:93} INFO - 25/07/18 15:50:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:03.104+0000] {subprocess.py:93} INFO - 25/07/18 15:51:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:03.793+0000] {subprocess.py:93} INFO - 25/07/18 15:51:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:04.303+0000] {subprocess.py:93} INFO - 25/07/18 15:51:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:13.112+0000] {subprocess.py:93} INFO - 25/07/18 15:51:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:13.794+0000] {subprocess.py:93} INFO - 25/07/18 15:51:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:14.310+0000] {subprocess.py:93} INFO - 25/07/18 15:51:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:23.123+0000] {subprocess.py:93} INFO - 25/07/18 15:51:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:23.806+0000] {subprocess.py:93} INFO - 25/07/18 15:51:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:24.322+0000] {subprocess.py:93} INFO - 25/07/18 15:51:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:33.133+0000] {subprocess.py:93} INFO - 25/07/18 15:51:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:33.811+0000] {subprocess.py:93} INFO - 25/07/18 15:51:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:34.331+0000] {subprocess.py:93} INFO - 25/07/18 15:51:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:43.146+0000] {subprocess.py:93} INFO - 25/07/18 15:51:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:43.816+0000] {subprocess.py:93} INFO - 25/07/18 15:51:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:44.333+0000] {subprocess.py:93} INFO - 25/07/18 15:51:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:53.143+0000] {subprocess.py:93} INFO - 25/07/18 15:51:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:53.828+0000] {subprocess.py:93} INFO - 25/07/18 15:51:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:51:54.339+0000] {subprocess.py:93} INFO - 25/07/18 15:51:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:02.275+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/31 using temp file file:/tmp/checkpoints/reservations/offsets/.31.62700867-0a01-484c-bf1e-4cc37c689534.tmp
[2025-07-18T15:52:02.297+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.31.62700867-0a01-484c-bf1e-4cc37c689534.tmp to file:/tmp/checkpoints/reservations/offsets/31
[2025-07-18T15:52:02.297+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MicroBatchExecution: Committed offsets for batch 31. Metadata OffsetSeqMetadata(0,1752853922260,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:02.343+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.344+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.344+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.349+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.351+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.359+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.359+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.359+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.361+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.361+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.367+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.367+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.367+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.368+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.369+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.383+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:52:02.386+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:52:02.387+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:52:02.387+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Created broadcast 66 from start at <unknown>:0
[2025-07-18T15:52:02.387+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:02.391+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:02.392+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Got job 33 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:02.393+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Final stage: ResultStage 33 (start at <unknown>:0)
[2025-07-18T15:52:02.393+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:02.393+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:02.394+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[135] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:02.394+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:52:02.395+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:52:02.396+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:52:02.396+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:02.396+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[135] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:02.398+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0
[2025-07-18T15:52:02.398+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:52:02.400+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
[2025-07-18T15:52:02.412+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:02.414+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=81 untilOffset=82, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=31 taskId=33 partitionId=0
[2025-07-18T15:52:02.418+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 81 for partition reservations-0
[2025-07-18T15:52:02.419+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Node -1 disconnected.
[2025-07-18T15:52:02.427+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:52:02.462+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:02.463+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:52:02.463+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=83, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:02.471+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 33, attempt 0, stage 33.0)
[2025-07-18T15:52:02.512+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DataWritingSparkTask: Committed partition 0 (task 33, attempt 0, stage 33.0)
[2025-07-18T15:52:02.513+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 44524375 nanos, during time span of 91779667 nanos.
[2025-07-18T15:52:02.513+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO Executor: Finished task 0.0 in stage 33.0 (TID 33). 4732 bytes result sent to driver
[2025-07-18T15:52:02.515+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 114 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:02.515+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2025-07-18T15:52:02.516+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: ResultStage 33 (start at <unknown>:0) finished in 0.122 s
[2025-07-18T15:52:02.516+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:02.516+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished
[2025-07-18T15:52:02.516+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Job 33 finished: start at <unknown>:0, took 0.125781 s
[2025-07-18T15:52:02.517+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:02.517+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Committing epoch 31 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:52:02.530+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.638+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v33.metadata.json
[2025-07-18T15:52:02.672+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SnapshotProducer: Committed snapshot 5501878487636265656 (FastAppend)
[2025-07-18T15:52:02.703+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5501878487636265656, sequenceNumber=32, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.172666625S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=32}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=82}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2927}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=96501}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:02.704+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Committed in 174 ms
[2025-07-18T15:52:02.705+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 31, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:52:02.714+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/31 using temp file file:/tmp/checkpoints/reservations/commits/.31.99b0c8c2-3b93-49d7-bb0d-9a2bea1aab9b.tmp
[2025-07-18T15:52:02.753+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.31.99b0c8c2-3b93-49d7-bb0d-9a2bea1aab9b.tmp to file:/tmp/checkpoints/reservations/commits/31
[2025-07-18T15:52:02.759+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:02.760+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:52:02.760+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:52:02.761+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:02.761+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:02.259Z",
[2025-07-18T15:52:02.761+0000] {subprocess.py:93} INFO -   "batchId" : 31,
[2025-07-18T15:52:02.761+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:52:02.761+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:52:02.762+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.028397565922921,
[2025-07-18T15:52:02.762+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:02.762+0000] {subprocess.py:93} INFO -     "addBatch" : 348,
[2025-07-18T15:52:02.762+0000] {subprocess.py:93} INFO -     "commitOffsets" : 49,
[2025-07-18T15:52:02.762+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:52:02.762+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:52:02.763+0000] {subprocess.py:93} INFO -     "queryPlanning" : 55,
[2025-07-18T15:52:02.763+0000] {subprocess.py:93} INFO -     "triggerExecution" : 493,
[2025-07-18T15:52:02.763+0000] {subprocess.py:93} INFO -     "walCommit" : 36
[2025-07-18T15:52:02.763+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:02.763+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:02.765+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:02.765+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:52:02.766+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:02.766+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:52:02.766+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:52:02.767+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:02.768+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:02.768+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:02.769+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:52:02.770+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:02.770+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:02.771+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:02.772+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:02.773+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:52:02.774+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:02.774+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:02.776+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:02.776+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:52:02.777+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:52:02.777+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.028397565922921,
[2025-07-18T15:52:02.778+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:02.778+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:02.780+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:02.780+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:02.781+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:02.781+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:02.781+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:02.782+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:52:02.782+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:52:02.783+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:02.783+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:02.783+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/32 using temp file file:/tmp/checkpoints/reservations/offsets/.32.7495888d-bf5b-4a4b-8755-011309f922c0.tmp
[2025-07-18T15:52:02.801+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.32.7495888d-bf5b-4a4b-8755-011309f922c0.tmp to file:/tmp/checkpoints/reservations/offsets/32
[2025-07-18T15:52:02.801+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MicroBatchExecution: Committed offsets for batch 32. Metadata OffsetSeqMetadata(0,1752853922760,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:02.811+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.811+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.811+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.812+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.812+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.817+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.818+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.819+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.820+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.821+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.830+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.831+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.831+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:02.833+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.834+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.842+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:52:02.843+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.1 MiB)
[2025-07-18T15:52:02.846+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:52:02.847+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Created broadcast 68 from start at <unknown>:0
[2025-07-18T15:52:02.847+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:02.848+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:02.848+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Got job 34 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:02.848+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Final stage: ResultStage 34 (start at <unknown>:0)
[2025-07-18T15:52:02.848+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[139] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:02.849+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T15:52:02.852+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T15:52:02.853+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:52:02.854+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:02.854+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[139] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:02.855+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0
[2025-07-18T15:52:02.855+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:52:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO Executor: Running task 0.0 in stage 34.0 (TID 34)
[2025-07-18T15:52:02.863+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:02.864+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=82 untilOffset=84, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=32 taskId=34 partitionId=0
[2025-07-18T15:52:02.869+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 82 for partition reservations-0
[2025-07-18T15:52:02.870+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:52:02.871+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:02.873+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:52:02.874+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=84, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:02.876+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 83 for partition reservations-0
[2025-07-18T15:52:02.877+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:52:02.878+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/35 using temp file file:/tmp/checkpoints/checkins/offsets/.35.190d0d8a-e7d7-487b-9488-464a722b90a5.tmp
[2025-07-18T15:52:02.909+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.35.190d0d8a-e7d7-487b-9488-464a722b90a5.tmp to file:/tmp/checkpoints/checkins/offsets/35
[2025-07-18T15:52:02.910+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1752853922861,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:02.910+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.910+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.910+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.910+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.911+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.914+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.915+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.915+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.915+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.916+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.920+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.921+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.921+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:02.922+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.922+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:02.930+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:52:02.932+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:52:02.932+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:52:02.932+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Created broadcast 70 from start at <unknown>:0
[2025-07-18T15:52:02.932+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:02.933+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:02.934+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Got job 35 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:02.934+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Final stage: ResultStage 35 (start at <unknown>:0)
[2025-07-18T15:52:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[143] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:02.935+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T15:52:02.937+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:52:02.937+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:52:02.938+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:02.938+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[143] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:02.938+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2025-07-18T15:52:02.938+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:52:02.942+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO Executor: Running task 0.0 in stage 35.0 (TID 35)
[2025-07-18T15:52:02.943+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:02.944+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=81 untilOffset=82, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=35 taskId=35 partitionId=0
[2025-07-18T15:52:02.947+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 81 for partition checkins-0
[2025-07-18T15:52:02.947+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Node -1 disconnected.
[2025-07-18T15:52:02.949+0000] {subprocess.py:93} INFO - 25/07/18 15:52:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:52:03.061+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:52:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=83, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 35, attempt 0, stage 35.0)
[2025-07-18T15:52:03.076+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Committed partition 0 (task 35, attempt 0, stage 35.0)
[2025-07-18T15:52:03.076+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 113910208 nanos, during time span of 128582916 nanos.
[2025-07-18T15:52:03.077+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Finished task 0.0 in stage 35.0 (TID 35). 4815 bytes result sent to driver
[2025-07-18T15:52:03.078+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 140 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:03.078+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool
[2025-07-18T15:52:03.078+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: ResultStage 35 (start at <unknown>:0) finished in 0.144 s
[2025-07-18T15:52:03.078+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:03.079+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2025-07-18T15:52:03.079+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 35 finished: start at <unknown>:0, took 0.145567 s
[2025-07-18T15:52:03.079+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:03.079+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing epoch 35 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:52:03.085+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.137+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v37.metadata.json
[2025-07-18T15:52:03.153+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SnapshotProducer: Committed snapshot 2307702954264926835 (FastAppend)
[2025-07-18T15:52:03.167+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2307702954264926835, sequenceNumber=36, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.082033916S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=36}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=82}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2911}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=106278}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:03.168+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committed in 82 ms
[2025-07-18T15:52:03.168+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:52:03.172+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/35 using temp file file:/tmp/checkpoints/checkins/commits/.35.6d9efc1e-c321-450e-bbd1-f88df80d24e0.tmp
[2025-07-18T15:52:03.183+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.35.6d9efc1e-c321-450e-bbd1-f88df80d24e0.tmp to file:/tmp/checkpoints/checkins/commits/35
[2025-07-18T15:52:03.184+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:03.184+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:52:03.184+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:52:03.184+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:03.184+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:02.858Z",
[2025-07-18T15:52:03.184+0000] {subprocess.py:93} INFO -   "batchId" : 35,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 3.0769230769230766,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -     "addBatch" : 255,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -     "commitOffsets" : 17,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:52:03.185+0000] {subprocess.py:93} INFO -     "triggerExecution" : 325,
[2025-07-18T15:52:03.186+0000] {subprocess.py:93} INFO -     "walCommit" : 44
[2025-07-18T15:52:03.186+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:03.186+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:03.186+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:03.186+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:52:03.186+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:03.186+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:03.187+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:52:03.187+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.187+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.187+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:03.187+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:03.187+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:03.187+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 71.42857142857143,
[2025-07-18T15:52:03.188+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 3.0769230769230766,
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:52:03.189+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:52:03.190+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:03.190+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:03.190+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/36 using temp file file:/tmp/checkpoints/checkins/offsets/.36.85d12f46-7253-46ed-b1fb-c45c8e4317ef.tmp
[2025-07-18T15:52:03.200+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.36.85d12f46-7253-46ed-b1fb-c45c8e4317ef.tmp to file:/tmp/checkpoints/checkins/offsets/36
[2025-07-18T15:52:03.200+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1752853923184,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:03.204+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.205+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.205+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.206+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.206+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.210+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.210+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.210+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.211+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.211+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.220+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.221+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.222+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.226+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.237+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:52:03.240+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 433.9 MiB)
[2025-07-18T15:52:03.240+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:52:03.241+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 72 from start at <unknown>:0
[2025-07-18T15:52:03.241+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:03.242+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:03.242+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Got job 36 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:03.242+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Final stage: ResultStage 36 (start at <unknown>:0)
[2025-07-18T15:52:03.242+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:03.242+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:03.243+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[147] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:03.243+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T15:52:03.246+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T15:52:03.246+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[147] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0
[2025-07-18T15:52:03.247+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:52:03.248+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Running task 0.0 in stage 36.0 (TID 36)
[2025-07-18T15:52:03.253+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:03.254+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=82 untilOffset=83, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=36 taskId=36 partitionId=0
[2025-07-18T15:52:03.257+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 82 for partition checkins-0
[2025-07-18T15:52:03.257+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:52:03.261+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.261+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:52:03.263+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=84, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.264+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 36, attempt 0, stage 36.0)
[2025-07-18T15:52:03.278+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Committed partition 0 (task 36, attempt 0, stage 36.0)
[2025-07-18T15:52:03.279+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 4822167 nanos, during time span of 21711084 nanos.
[2025-07-18T15:52:03.279+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Finished task 0.0 in stage 36.0 (TID 36). 4826 bytes result sent to driver
[2025-07-18T15:52:03.280+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 32 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:03.282+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool
[2025-07-18T15:52:03.282+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: ResultStage 36 (start at <unknown>:0) finished in 0.039 s
[2025-07-18T15:52:03.282+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:03.282+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 36: Stage finished
[2025-07-18T15:52:03.283+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 36 finished: start at <unknown>:0, took 0.041532 s
[2025-07-18T15:52:03.284+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:03.284+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing epoch 36 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:52:03.293+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.366+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v38.metadata.json
[2025-07-18T15:52:03.377+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.377+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:52:03.377+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=84, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.377+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 34, attempt 0, stage 34.0)
[2025-07-18T15:52:03.388+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Committed partition 0 (task 34, attempt 0, stage 34.0)
[2025-07-18T15:52:03.389+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 509865751 nanos, during time span of 521559375 nanos.
[2025-07-18T15:52:03.390+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SnapshotProducer: Committed snapshot 7963487299278792118 (FastAppend)
[2025-07-18T15:52:03.390+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Finished task 0.0 in stage 34.0 (TID 34). 4695 bytes result sent to driver
[2025-07-18T15:52:03.390+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 538 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:03.390+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2025-07-18T15:52:03.391+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: ResultStage 34 (start at <unknown>:0) finished in 0.544 s
[2025-07-18T15:52:03.391+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:03.391+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2025-07-18T15:52:03.391+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 34 finished: start at <unknown>:0, took 0.545848 s
[2025-07-18T15:52:03.391+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:03.391+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing epoch 32 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:52:03.396+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:52:03.403+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=7963487299278792118, sequenceNumber=37, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.109524375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=37}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=83}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2911}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=109189}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:03.404+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committed in 110 ms
[2025-07-18T15:52:03.404+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:52:03.408+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/36 using temp file file:/tmp/checkpoints/checkins/commits/.36.c9a94a77-5844-443c-b3c0-31005702c2c4.tmp
[2025-07-18T15:52:03.423+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.36.c9a94a77-5844-443c-b3c0-31005702c2c4.tmp to file:/tmp/checkpoints/checkins/commits/36
[2025-07-18T15:52:03.423+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:03.423+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:52:03.423+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:52:03.423+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:03.183Z",
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -   "batchId" : 36,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.0769230769230766,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.184100418410042,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -     "addBatch" : 195,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -     "commitOffsets" : 20,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:52:03.424+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -     "triggerExecution" : 239,
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -     "walCommit" : 15
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.425+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -         "0" : 83
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:03.426+0000] {subprocess.py:93} INFO -         "0" : 83
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.0769230769230766,
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.184100418410042,
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:03.427+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:03.428+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:03.428+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:03.428+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:03.429+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:03.429+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:52:03.429+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:52:03.429+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:03.429+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:03.429+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/37 using temp file file:/tmp/checkpoints/checkins/offsets/.37.8979d4f4-f748-4639-8f32-ecfca724b852.tmp
[2025-07-18T15:52:03.444+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.37.8979d4f4-f748-4639-8f32-ecfca724b852.tmp to file:/tmp/checkpoints/checkins/offsets/37
[2025-07-18T15:52:03.447+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1752853923423,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.449+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.449+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.449+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.450+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.454+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.461+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.461+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.462+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.464+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.465+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v34.metadata.json
[2025-07-18T15:52:03.472+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:52:03.476+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:52:03.476+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 74 from start at <unknown>:0
[2025-07-18T15:52:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Got job 37 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Final stage: ResultStage 37 (start at <unknown>:0)
[2025-07-18T15:52:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[151] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:03.480+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 28.0 KiB, free 433.7 MiB)
[2025-07-18T15:52:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/34 using temp file file:/tmp/checkpoints/feedback/offsets/.34.86419ec1-608f-4af1-ae43-f5add4581f5b.tmp
[2025-07-18T15:52:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.7 MiB)
[2025-07-18T15:52:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:03.483+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[151] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:03.484+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0
[2025-07-18T15:52:03.485+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:52:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Running task 0.0 in stage 37.0 (TID 37)
[2025-07-18T15:52:03.490+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:03.491+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=83 untilOffset=84, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=37 taskId=37 partitionId=0
[2025-07-18T15:52:03.494+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 83 for partition checkins-0
[2025-07-18T15:52:03.494+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SnapshotProducer: Committed snapshot 780135062913765796 (FastAppend)
[2025-07-18T15:52:03.494+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:52:03.499+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.34.86419ec1-608f-4af1-ae43-f5add4581f5b.tmp to file:/tmp/checkpoints/feedback/offsets/34
[2025-07-18T15:52:03.500+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1752853923470,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.504+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.505+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.506+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.511+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.512+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.512+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=780135062913765796, sequenceNumber=33, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.114665959S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=33}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=84}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3031}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=99532}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:03.513+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committed in 115 ms
[2025-07-18T15:52:03.513+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 32, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:52:03.516+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.516+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.517+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.517+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/32 using temp file file:/tmp/checkpoints/reservations/commits/.32.0beed387-07a7-4e7c-a973-5d213835a8f5.tmp
[2025-07-18T15:52:03.518+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.518+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.525+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
[2025-07-18T15:52:03.527+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.7 MiB)
[2025-07-18T15:52:03.529+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.530+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 76 from start at <unknown>:0
[2025-07-18T15:52:03.530+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Got job 38 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Final stage: ResultStage 38 (start at <unknown>:0)
[2025-07-18T15:52:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:03.533+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[155] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:03.534+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 27.5 KiB, free 433.6 MiB)
[2025-07-18T15:52:03.534+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.32.0beed387-07a7-4e7c-a973-5d213835a8f5.tmp to file:/tmp/checkpoints/reservations/commits/32
[2025-07-18T15:52:03.534+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:03.535+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:52:03.535+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:52:03.535+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:03.535+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:02.757Z",
[2025-07-18T15:52:03.535+0000] {subprocess.py:93} INFO -   "batchId" : 32,
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.016064257028113,
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.5806451612903225,
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -     "addBatch" : 697,
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:52:03.536+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -     "queryPlanning" : 12,
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -     "triggerExecution" : 775,
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -     "walCommit" : 40
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:52:03.537+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.538+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.539+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:03.539+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:52:03.539+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:52:03.539+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.539+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.539+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:52:03.539+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.016064257028113,
[2025-07-18T15:52:03.540+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.5806451612903225,
[2025-07-18T15:52:03.540+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:03.541+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:03.541+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:03.541+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:03.541+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:03.541+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:03.542+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:03.542+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:52:03.543+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:52:03.543+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:03.543+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:03.543+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.6 MiB)
[2025-07-18T15:52:03.543+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.543+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:03.544+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[155] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:03.544+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0
[2025-07-18T15:52:03.544+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:52:03.544+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Running task 0.0 in stage 38.0 (TID 38)
[2025-07-18T15:52:03.544+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:03.544+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=81 untilOffset=82, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=34 taskId=38 partitionId=0
[2025-07-18T15:52:03.545+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 81 for partition feedback-0
[2025-07-18T15:52:03.545+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO NetworkClient: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Node -1 disconnected.
[2025-07-18T15:52:03.545+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:52:03.665+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.666+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:52:03.666+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=83, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.667+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 38, attempt 0, stage 38.0)
[2025-07-18T15:52:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Committed partition 0 (task 38, attempt 0, stage 38.0)
[2025-07-18T15:52:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 123563417 nanos, during time span of 136174333 nanos.
[2025-07-18T15:52:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Finished task 0.0 in stage 38.0 (TID 38). 4765 bytes result sent to driver
[2025-07-18T15:52:03.680+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 146 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:03.681+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool
[2025-07-18T15:52:03.682+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: ResultStage 38 (start at <unknown>:0) finished in 0.150 s
[2025-07-18T15:52:03.682+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:03.682+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished
[2025-07-18T15:52:03.683+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 38 finished: start at <unknown>:0, took 0.151156 s
[2025-07-18T15:52:03.683+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:03.683+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing epoch 34 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:52:03.687+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.735+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v36.metadata.json
[2025-07-18T15:52:03.758+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SnapshotProducer: Committed snapshot 2434880749278698282 (FastAppend)
[2025-07-18T15:52:03.772+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=2434880749278698282, sequenceNumber=35, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.084100375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=35}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=82}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=103269}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:03.773+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committed in 84 ms
[2025-07-18T15:52:03.773+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:52:03.777+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/34 using temp file file:/tmp/checkpoints/feedback/commits/.34.884af372-0f87-41cb-a82d-ebedab12bb21.tmp
[2025-07-18T15:52:03.788+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.34.884af372-0f87-41cb-a82d-ebedab12bb21.tmp to file:/tmp/checkpoints/feedback/commits/34
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:03.468Z",
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO -   "batchId" : 34,
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:52:03.789+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:52:03.790+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 3.134796238244514,
[2025-07-18T15:52:03.790+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:03.803+0000] {subprocess.py:93} INFO -     "addBatch" : 266,
[2025-07-18T15:52:03.805+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T15:52:03.805+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:52:03.805+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:52:03.809+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:52:03.809+0000] {subprocess.py:93} INFO -     "triggerExecution" : 319,
[2025-07-18T15:52:03.809+0000] {subprocess.py:93} INFO -     "walCommit" : 29
[2025-07-18T15:52:03.809+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:03.809+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:03.810+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:03.810+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:52:03.814+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:03.814+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:03.814+0000] {subprocess.py:93} INFO -         "0" : 81
[2025-07-18T15:52:03.814+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.814+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:03.815+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:03.816+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:03.816+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:03.816+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:52:03.816+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:52:03.816+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 3.134796238244514,
[2025-07-18T15:52:03.816+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:03.817+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:03.819+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:03.821+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:03.822+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/35 using temp file file:/tmp/checkpoints/feedback/offsets/.35.7a40677f-c3c7-4fd3-b9bf-4131c4eb1012.tmp
[2025-07-18T15:52:03.830+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.35.7a40677f-c3c7-4fd3-b9bf-4131c4eb1012.tmp to file:/tmp/checkpoints/feedback/offsets/35
[2025-07-18T15:52:03.830+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1752853923789,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:03.834+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.835+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.835+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.835+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.836+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.839+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.839+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.839+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.840+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.841+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.844+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.844+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.844+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.845+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.845+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:03.851+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
[2025-07-18T15:52:03.852+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.6 MiB)
[2025-07-18T15:52:03.852+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.0 MiB)
[2025-07-18T15:52:03.852+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 78 from start at <unknown>:0
[2025-07-18T15:52:03.853+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:03.853+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:03.854+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Got job 39 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:03.854+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Final stage: ResultStage 39 (start at <unknown>:0)
[2025-07-18T15:52:03.854+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:03.854+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:03.854+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[159] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:03.855+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 27.5 KiB, free 433.5 MiB)
[2025-07-18T15:52:03.864+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 433.5 MiB)
[2025-07-18T15:52:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 77cb57a6bd53:38973 (size: 12.2 KiB, free: 434.0 MiB)
[2025-07-18T15:52:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[159] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0
[2025-07-18T15:52:03.865+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:52:03.868+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Running task 0.0 in stage 39.0 (TID 39)
[2025-07-18T15:52:03.869+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_67_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.0 MiB)
[2025-07-18T15:52:03.869+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.870+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:03.871+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=82 untilOffset=83, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=35 taskId=39 partitionId=0
[2025-07-18T15:52:03.872+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_73_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.882+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_68_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.884+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 82 for partition feedback-0
[2025-07-18T15:52:03.884+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:52:03.884+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.885+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:52:03.886+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=84, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.887+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_66_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:52:03.887+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 39, attempt 0, stage 39.0)
[2025-07-18T15:52:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_70_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:52:03.892+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_72_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:52:03.894+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_77_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:52:03.896+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_71_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:52:03.898+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO BlockManagerInfo: Removed broadcast_69_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:52:03.898+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Committed partition 0 (task 39, attempt 0, stage 39.0)
[2025-07-18T15:52:03.898+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 2903958 nanos, during time span of 15694041 nanos.
[2025-07-18T15:52:03.899+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO Executor: Finished task 0.0 in stage 39.0 (TID 39). 4773 bytes result sent to driver
[2025-07-18T15:52:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 36 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool
[2025-07-18T15:52:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: ResultStage 39 (start at <unknown>:0) finished in 0.045 s
[2025-07-18T15:52:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished
[2025-07-18T15:52:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DAGScheduler: Job 39 finished: start at <unknown>:0, took 0.046787 s
[2025-07-18T15:52:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:03.901+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing epoch 35 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:52:03.906+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:03.956+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v37.metadata.json
[2025-07-18T15:52:03.972+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SnapshotProducer: Committed snapshot 3148977805160002096 (FastAppend)
[2025-07-18T15:52:03.984+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=3148977805160002096, sequenceNumber=36, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.077679833S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=36}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=83}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2813}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=106082}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:03.985+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SparkWrite: Committed in 78 ms
[2025-07-18T15:52:03.985+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:52:03.988+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/35 using temp file file:/tmp/checkpoints/feedback/commits/.35.200ca084-d4ff-4f9f-9b9d-f47dbbce16fb.tmp
[2025-07-18T15:52:03.996+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.997+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:52:03.998+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=84, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:03.998+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 37, attempt 0, stage 37.0)
[2025-07-18T15:52:04.000+0000] {subprocess.py:93} INFO - 25/07/18 15:52:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.35.200ca084-d4ff-4f9f-9b9d-f47dbbce16fb.tmp to file:/tmp/checkpoints/feedback/commits/35
[2025-07-18T15:52:04.001+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:04.002+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:52:04.002+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:52:04.003+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:04.003+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:03.788Z",
[2025-07-18T15:52:04.003+0000] {subprocess.py:93} INFO -   "batchId" : 35,
[2025-07-18T15:52:04.003+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:52:04.003+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.125,
[2025-07-18T15:52:04.004+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.739336492890995,
[2025-07-18T15:52:04.004+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:04.004+0000] {subprocess.py:93} INFO -     "addBatch" : 147,
[2025-07-18T15:52:04.004+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T15:52:04.004+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:52:04.004+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:52:04.004+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:52:04.005+0000] {subprocess.py:93} INFO -     "triggerExecution" : 211,
[2025-07-18T15:52:04.005+0000] {subprocess.py:93} INFO -     "walCommit" : 41
[2025-07-18T15:52:04.005+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:04.005+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:04.005+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:04.005+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:52:04.006+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:04.006+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:04.006+0000] {subprocess.py:93} INFO -         "0" : 82
[2025-07-18T15:52:04.006+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.006+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.006+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -         "0" : 83
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -         "0" : 83
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:52:04.007+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.125,
[2025-07-18T15:52:04.008+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.739336492890995,
[2025-07-18T15:52:04.008+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:04.008+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:04.008+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:04.008+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:04.009+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:04.009+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:04.009+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:04.009+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:52:04.009+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:52:04.010+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:04.010+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:04.010+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/36 using temp file file:/tmp/checkpoints/feedback/offsets/.36.c5bd01c9-a144-43bd-8c28-e982cd447e57.tmp
[2025-07-18T15:52:04.010+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DataWritingSparkTask: Committed partition 0 (task 37, attempt 0, stage 37.0)
[2025-07-18T15:52:04.010+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 502998042 nanos, during time span of 515963250 nanos.
[2025-07-18T15:52:04.011+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO Executor: Finished task 0.0 in stage 37.0 (TID 37). 4863 bytes result sent to driver
[2025-07-18T15:52:04.011+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 527 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:04.011+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool
[2025-07-18T15:52:04.011+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: ResultStage 37 (start at <unknown>:0) finished in 0.534 s
[2025-07-18T15:52:04.013+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:04.013+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished
[2025-07-18T15:52:04.013+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Job 37 finished: start at <unknown>:0, took 0.535282 s
[2025-07-18T15:52:04.014+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:04.014+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Committing epoch 37 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:52:04.023+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:52:04.023+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.36.c5bd01c9-a144-43bd-8c28-e982cd447e57.tmp to file:/tmp/checkpoints/feedback/offsets/36
[2025-07-18T15:52:04.023+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1752853924001,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:52:04.025+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.025+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.025+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.027+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:04.028+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:04.032+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.033+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.033+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.034+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:04.038+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:04.040+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.041+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.041+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.041+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:04.042+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:52:04.048+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:52:04.049+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:52:04.049+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:52:04.049+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkContext: Created broadcast 80 from start at <unknown>:0
[2025-07-18T15:52:04.050+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:52:04.050+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:52:04.050+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Got job 40 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:52:04.050+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Final stage: ResultStage 40 (start at <unknown>:0)
[2025-07-18T15:52:04.050+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:52:04.050+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:52:04.051+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[163] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:52:04.053+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 27.5 KiB, free 433.9 MiB)
[2025-07-18T15:52:04.053+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.9 MiB)
[2025-07-18T15:52:04.053+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:52:04.054+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:52:04.054+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[163] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:52:04.054+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0
[2025-07-18T15:52:04.055+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:52:04.056+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO Executor: Running task 0.0 in stage 40.0 (TID 40)
[2025-07-18T15:52:04.059+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:52:04.059+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=83 untilOffset=84, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=36 taskId=40 partitionId=0
[2025-07-18T15:52:04.064+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 83 for partition feedback-0
[2025-07-18T15:52:04.065+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:52:04.082+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v39.metadata.json
[2025-07-18T15:52:04.105+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SnapshotProducer: Committed snapshot 1856727649722573341 (FastAppend)
[2025-07-18T15:52:04.122+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=1856727649722573341, sequenceNumber=38, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.100538917S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=38}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=84}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2903}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=112092}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:04.122+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Committed in 101 ms
[2025-07-18T15:52:04.123+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:52:04.125+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/37 using temp file file:/tmp/checkpoints/checkins/commits/.37.b02437bb-3a7b-4f66-8080-4c9609b1c109.tmp
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.37.b02437bb-3a7b-4f66-8080-4c9609b1c109.tmp to file:/tmp/checkpoints/checkins/commits/37
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:03.422Z",
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO -   "batchId" : 37,
[2025-07-18T15:52:04.139+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.184100418410042,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.402524544179523,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "addBatch" : 670,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "commitOffsets" : 15,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "triggerExecution" : 713,
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "walCommit" : 20
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -         "0" : 83
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.140+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.184100418410042,
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.402524544179523,
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:04.141+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:04.142+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:04.142+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:52:04.142+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:52:04.142+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:04.142+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:04.569+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:04.570+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:52:04.570+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=84, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:52:04.574+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 40, attempt 0, stage 40.0)
[2025-07-18T15:52:04.595+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DataWritingSparkTask: Committed partition 0 (task 40, attempt 0, stage 40.0)
[2025-07-18T15:52:04.596+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 505991792 nanos, during time span of 530705250 nanos.
[2025-07-18T15:52:04.596+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO Executor: Finished task 0.0 in stage 40.0 (TID 40). 4767 bytes result sent to driver
[2025-07-18T15:52:04.600+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 543 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:52:04.600+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2025-07-18T15:52:04.601+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: ResultStage 40 (start at <unknown>:0) finished in 0.547 s
[2025-07-18T15:52:04.601+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:52:04.601+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
[2025-07-18T15:52:04.601+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO DAGScheduler: Job 40 finished: start at <unknown>:0, took 0.548644 s
[2025-07-18T15:52:04.601+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:52:04.601+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Committing epoch 36 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:52:04.610+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:52:04.731+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v38.metadata.json
[2025-07-18T15:52:04.788+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SnapshotProducer: Committed snapshot 7166834259855529298 (FastAppend)
[2025-07-18T15:52:04.829+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7166834259855529298, sequenceNumber=37, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.215588542S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=37}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=84}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2877}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=108959}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:52:04.835+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO SparkWrite: Committed in 217 ms
[2025-07-18T15:52:04.838+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:52:04.847+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/36 using temp file file:/tmp/checkpoints/feedback/commits/.36.2a017c5d-f5ce-4c35-bbf4-65d55e3d896a.tmp
[2025-07-18T15:52:04.887+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.36.2a017c5d-f5ce-4c35-bbf4-65d55e3d896a.tmp to file:/tmp/checkpoints/feedback/commits/36
[2025-07-18T15:52:04.890+0000] {subprocess.py:93} INFO - 25/07/18 15:52:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:52:04.891+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:52:04.895+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:52:04.897+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:52:04.900+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:52:04.000Z",
[2025-07-18T15:52:04.905+0000] {subprocess.py:93} INFO -   "batchId" : 36,
[2025-07-18T15:52:04.906+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:52:04.907+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.716981132075472,
[2025-07-18T15:52:04.908+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.1273957158962795,
[2025-07-18T15:52:04.910+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:52:04.912+0000] {subprocess.py:93} INFO -     "addBatch" : 798,
[2025-07-18T15:52:04.913+0000] {subprocess.py:93} INFO -     "commitOffsets" : 60,
[2025-07-18T15:52:04.913+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:52:04.913+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:52:04.914+0000] {subprocess.py:93} INFO -     "queryPlanning" : 9,
[2025-07-18T15:52:04.915+0000] {subprocess.py:93} INFO -     "triggerExecution" : 887,
[2025-07-18T15:52:04.916+0000] {subprocess.py:93} INFO -     "walCommit" : 18
[2025-07-18T15:52:04.917+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:52:04.917+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:52:04.918+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:52:04.918+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:52:04.918+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:52:04.918+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:04.918+0000] {subprocess.py:93} INFO -         "0" : 83
[2025-07-18T15:52:04.918+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.919+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.919+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:52:04.923+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:04.924+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:52:04.924+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.924+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.924+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:52:04.924+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:52:04.926+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:52:04.926+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:52:04.928+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:52:04.928+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:52:04.928+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.716981132075472,
[2025-07-18T15:52:04.928+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.1273957158962795,
[2025-07-18T15:52:04.929+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:52:04.929+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:52:04.930+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:52:04.930+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:52:04.931+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:52:04.931+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:52:04.932+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:52:04.932+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:52:04.932+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:52:04.933+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:52:04.933+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:52:13.545+0000] {subprocess.py:93} INFO - 25/07/18 15:52:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:14.141+0000] {subprocess.py:93} INFO - 25/07/18 15:52:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:14.893+0000] {subprocess.py:93} INFO - 25/07/18 15:52:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:18.223+0000] {subprocess.py:93} INFO - 25/07/18 15:52:18 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:52:18.226+0000] {subprocess.py:93} INFO - 25/07/18 15:52:18 INFO BlockManagerInfo: Removed broadcast_75_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:52:18.229+0000] {subprocess.py:93} INFO - 25/07/18 15:52:18 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:52:18.230+0000] {subprocess.py:93} INFO - 25/07/18 15:52:18 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:52:18.232+0000] {subprocess.py:93} INFO - 25/07/18 15:52:18 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 77cb57a6bd53:38973 in memory (size: 12.2 KiB, free: 434.3 MiB)
[2025-07-18T15:52:18.233+0000] {subprocess.py:93} INFO - 25/07/18 15:52:18 INFO BlockManagerInfo: Removed broadcast_74_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:52:23.549+0000] {subprocess.py:93} INFO - 25/07/18 15:52:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:24.150+0000] {subprocess.py:93} INFO - 25/07/18 15:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:24.890+0000] {subprocess.py:93} INFO - 25/07/18 15:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:33.560+0000] {subprocess.py:93} INFO - 25/07/18 15:52:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:34.152+0000] {subprocess.py:93} INFO - 25/07/18 15:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:34.890+0000] {subprocess.py:93} INFO - 25/07/18 15:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:43.573+0000] {subprocess.py:93} INFO - 25/07/18 15:52:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:44.154+0000] {subprocess.py:93} INFO - 25/07/18 15:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:44.898+0000] {subprocess.py:93} INFO - 25/07/18 15:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:53.583+0000] {subprocess.py:93} INFO - 25/07/18 15:52:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:54.160+0000] {subprocess.py:93} INFO - 25/07/18 15:52:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:52:54.899+0000] {subprocess.py:93} INFO - 25/07/18 15:52:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:03.587+0000] {subprocess.py:93} INFO - 25/07/18 15:53:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:04.163+0000] {subprocess.py:93} INFO - 25/07/18 15:53:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:04.909+0000] {subprocess.py:93} INFO - 25/07/18 15:53:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:13.593+0000] {subprocess.py:93} INFO - 25/07/18 15:53:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:14.163+0000] {subprocess.py:93} INFO - 25/07/18 15:53:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:14.922+0000] {subprocess.py:93} INFO - 25/07/18 15:53:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:23.603+0000] {subprocess.py:93} INFO - 25/07/18 15:53:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:24.173+0000] {subprocess.py:93} INFO - 25/07/18 15:53:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:24.925+0000] {subprocess.py:93} INFO - 25/07/18 15:53:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:33.607+0000] {subprocess.py:93} INFO - 25/07/18 15:53:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:34.182+0000] {subprocess.py:93} INFO - 25/07/18 15:53:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:34.941+0000] {subprocess.py:93} INFO - 25/07/18 15:53:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:43.606+0000] {subprocess.py:93} INFO - 25/07/18 15:53:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:44.187+0000] {subprocess.py:93} INFO - 25/07/18 15:53:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:44.942+0000] {subprocess.py:93} INFO - 25/07/18 15:53:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:53.608+0000] {subprocess.py:93} INFO - 25/07/18 15:53:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:54.192+0000] {subprocess.py:93} INFO - 25/07/18 15:53:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:53:54.945+0000] {subprocess.py:93} INFO - 25/07/18 15:53:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:02.596+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/33 using temp file file:/tmp/checkpoints/reservations/offsets/.33.9d851221-65a7-4e1e-8334-01938cede85b.tmp
[2025-07-18T15:54:02.618+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.33.9d851221-65a7-4e1e-8334-01938cede85b.tmp to file:/tmp/checkpoints/reservations/offsets/33
[2025-07-18T15:54:02.619+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO MicroBatchExecution: Committed offsets for batch 33. Metadata OffsetSeqMetadata(0,1752854042583,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:02.635+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.635+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.636+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.639+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:02.641+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:02.648+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.649+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.649+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.650+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:02.651+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:02.657+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.657+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.657+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.658+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:02.659+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:02.672+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:54:02.675+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:54:02.675+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:54:02.675+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkContext: Created broadcast 82 from start at <unknown>:0
[2025-07-18T15:54:02.676+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:02.677+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:02.678+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Got job 41 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:02.678+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Final stage: ResultStage 41 (start at <unknown>:0)
[2025-07-18T15:54:02.678+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:02.679+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:02.680+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[167] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:02.680+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:54:02.685+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:54:02.685+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:54:02.685+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:02.685+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[167] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:02.686+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
[2025-07-18T15:54:02.686+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:54:02.688+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO Executor: Running task 0.0 in stage 41.0 (TID 41)
[2025-07-18T15:54:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:02.698+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=84 untilOffset=85, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=33 taskId=41 partitionId=0
[2025-07-18T15:54:02.702+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 84 for partition reservations-0
[2025-07-18T15:54:02.705+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:54:02.782+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:02.783+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:54:02.786+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=86, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:02.787+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 41, attempt 0, stage 41.0)
[2025-07-18T15:54:02.816+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DataWritingSparkTask: Committed partition 0 (task 41, attempt 0, stage 41.0)
[2025-07-18T15:54:02.816+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 80373875 nanos, during time span of 115595042 nanos.
[2025-07-18T15:54:02.818+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO Executor: Finished task 0.0 in stage 41.0 (TID 41). 4703 bytes result sent to driver
[2025-07-18T15:54:02.824+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 133 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:02.825+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2025-07-18T15:54:02.825+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: ResultStage 41 (start at <unknown>:0) finished in 0.141 s
[2025-07-18T15:54:02.826+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:02.826+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished
[2025-07-18T15:54:02.826+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO DAGScheduler: Job 41 finished: start at <unknown>:0, took 0.143821 s
[2025-07-18T15:54:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Committing epoch 33 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:54:02.843+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:02.929+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v35.metadata.json
[2025-07-18T15:54:02.969+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SnapshotProducer: Committed snapshot 7438573234493635973 (FastAppend)
[2025-07-18T15:54:02.993+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=7438573234493635973, sequenceNumber=34, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.150070667S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=34}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=85}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2975}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=102507}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:02.993+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO SparkWrite: Committed in 150 ms
[2025-07-18T15:54:02.994+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 33, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:54:02.997+0000] {subprocess.py:93} INFO - 25/07/18 15:54:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/33 using temp file file:/tmp/checkpoints/reservations/commits/.33.877b4c6c-7e74-4262-bbf6-59f2ee74b4be.tmp
[2025-07-18T15:54:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.33.877b4c6c-7e74-4262-bbf6-59f2ee74b4be.tmp to file:/tmp/checkpoints/reservations/commits/33
[2025-07-18T15:54:03.019+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:03.019+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:54:03.019+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:54:03.019+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:03.019+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:02.581Z",
[2025-07-18T15:54:03.019+0000] {subprocess.py:93} INFO -   "batchId" : 33,
[2025-07-18T15:54:03.020+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:54:03.020+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:54:03.020+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.2988505747126435,
[2025-07-18T15:54:03.020+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:03.020+0000] {subprocess.py:93} INFO -     "addBatch" : 347,
[2025-07-18T15:54:03.020+0000] {subprocess.py:93} INFO -     "commitOffsets" : 26,
[2025-07-18T15:54:03.021+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:03.021+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:54:03.022+0000] {subprocess.py:93} INFO -     "queryPlanning" : 25,
[2025-07-18T15:54:03.022+0000] {subprocess.py:93} INFO -     "triggerExecution" : 435,
[2025-07-18T15:54:03.022+0000] {subprocess.py:93} INFO -     "walCommit" : 33
[2025-07-18T15:54:03.022+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:03.022+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:03.023+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:03.023+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:54:03.023+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:03.023+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:54:03.023+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:54:03.024+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.025+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.025+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:03.025+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:54:03.025+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:03.025+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.026+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.026+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:03.026+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:54:03.027+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:03.027+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.028+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.028+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:54:03.028+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:54:03.028+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.2988505747126435,
[2025-07-18T15:54:03.028+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:03.028+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:03.028+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:03.029+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:03.029+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:03.029+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:03.029+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:03.030+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:54:03.030+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:54:03.030+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:03.030+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:03.031+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/34 using temp file file:/tmp/checkpoints/reservations/offsets/.34.07deb9bf-d97b-41ce-91c6-3f8311bb53dc.tmp
[2025-07-18T15:54:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.34.07deb9bf-d97b-41ce-91c6-3f8311bb53dc.tmp to file:/tmp/checkpoints/reservations/offsets/34
[2025-07-18T15:54:03.039+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Committed offsets for batch 34. Metadata OffsetSeqMetadata(0,1752854043019,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.044+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.045+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.045+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.059+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.059+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.061+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:54:03.072+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.1 MiB)
[2025-07-18T15:54:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:54:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 84 from start at <unknown>:0
[2025-07-18T15:54:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:03.074+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:03.074+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Got job 42 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:03.074+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Final stage: ResultStage 42 (start at <unknown>:0)
[2025-07-18T15:54:03.074+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:03.074+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:03.075+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[171] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:03.075+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T15:54:03.078+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T15:54:03.078+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:54:03.079+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:03.079+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[171] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:03.081+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
[2025-07-18T15:54:03.081+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:54:03.081+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Running task 0.0 in stage 42.0 (TID 42)
[2025-07-18T15:54:03.086+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:03.086+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=85 untilOffset=87, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=34 taskId=42 partitionId=0
[2025-07-18T15:54:03.089+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 85 for partition reservations-0
[2025-07-18T15:54:03.089+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:54:03.091+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.091+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:54:03.091+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=87, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.092+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 86 for partition reservations-0
[2025-07-18T15:54:03.092+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:54:03.188+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/38 using temp file file:/tmp/checkpoints/checkins/offsets/.38.72c39f99-e739-4db3-8992-54d28d9a9ce9.tmp
[2025-07-18T15:54:03.201+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.38.72c39f99-e739-4db3-8992-54d28d9a9ce9.tmp to file:/tmp/checkpoints/checkins/offsets/38
[2025-07-18T15:54:03.202+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1752854043182,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:03.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.207+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.207+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.212+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.212+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.212+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.213+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.213+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.213+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.216+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.216+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.217+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.218+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.218+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.225+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:54:03.227+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:54:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:54:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 86 from start at <unknown>:0
[2025-07-18T15:54:03.228+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:03.229+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:03.229+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Got job 43 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:03.229+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Final stage: ResultStage 43 (start at <unknown>:0)
[2025-07-18T15:54:03.229+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:03.230+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:03.230+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[175] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:03.230+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T15:54:03.232+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:54:03.232+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:54:03.233+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:03.233+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[175] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:03.233+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2025-07-18T15:54:03.233+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:54:03.234+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Running task 0.0 in stage 43.0 (TID 43)
[2025-07-18T15:54:03.238+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:03.238+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=84 untilOffset=85, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=38 taskId=43 partitionId=0
[2025-07-18T15:54:03.241+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 84 for partition checkins-0
[2025-07-18T15:54:03.242+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:54:03.387+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.388+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:54:03.388+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=86, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.388+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 43, attempt 0, stage 43.0)
[2025-07-18T15:54:03.401+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DataWritingSparkTask: Committed partition 0 (task 43, attempt 0, stage 43.0)
[2025-07-18T15:54:03.401+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 145420125 nanos, during time span of 159781708 nanos.
[2025-07-18T15:54:03.402+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Finished task 0.0 in stage 43.0 (TID 43). 4822 bytes result sent to driver
[2025-07-18T15:54:03.404+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 170 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:03.405+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2025-07-18T15:54:03.405+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: ResultStage 43 (start at <unknown>:0) finished in 0.175 s
[2025-07-18T15:54:03.405+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:03.406+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
[2025-07-18T15:54:03.407+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Job 43 finished: start at <unknown>:0, took 0.176272 s
[2025-07-18T15:54:03.408+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:03.409+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committing epoch 38 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:54:03.420+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.475+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v40.metadata.json
[2025-07-18T15:54:03.496+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SnapshotProducer: Committed snapshot 2914250166151064105 (FastAppend)
[2025-07-18T15:54:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=2914250166151064105, sequenceNumber=39, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.092879333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=39}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=85}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2904}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=114996}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committed in 93 ms
[2025-07-18T15:54:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:54:03.513+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/38 using temp file file:/tmp/checkpoints/checkins/commits/.38.4c65530f-8e15-4e97-8876-3ea7ca758ce3.tmp
[2025-07-18T15:54:03.526+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.38.4c65530f-8e15-4e97-8876-3ea7ca758ce3.tmp to file:/tmp/checkpoints/checkins/commits/38
[2025-07-18T15:54:03.526+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:03.526+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:03.181Z",
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "batchId" : 38,
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.9154518950437316,
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -     "addBatch" : 300,
[2025-07-18T15:54:03.527+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T15:54:03.528+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:03.528+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:54:03.528+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:54:03.528+0000] {subprocess.py:93} INFO -     "triggerExecution" : 343,
[2025-07-18T15:54:03.528+0000] {subprocess.py:93} INFO -     "walCommit" : 19
[2025-07-18T15:54:03.528+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:03.528+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.529+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:03.530+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.9154518950437316,
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:03.531+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:03.532+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:03.532+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:03.532+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:54:03.532+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:54:03.532+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:03.532+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/39 using temp file file:/tmp/checkpoints/checkins/offsets/.39.94505659-bfdc-4e53-a281-3ed5a828a22d.tmp
[2025-07-18T15:54:03.541+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.39.94505659-bfdc-4e53-a281-3ed5a828a22d.tmp to file:/tmp/checkpoints/checkins/offsets/39
[2025-07-18T15:54:03.541+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1752854043526,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:03.545+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.546+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.546+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.549+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.549+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.552+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.552+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.552+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.553+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.553+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.557+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.558+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.558+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.564+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:54:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 433.9 MiB)
[2025-07-18T15:54:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:54:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 88 from start at <unknown>:0
[2025-07-18T15:54:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Got job 44 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Final stage: ResultStage 44 (start at <unknown>:0)
[2025-07-18T15:54:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[179] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T15:54:03.572+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T15:54:03.572+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:54:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[179] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
[2025-07-18T15:54:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:54:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Running task 0.0 in stage 44.0 (TID 44)
[2025-07-18T15:54:03.578+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=85 untilOffset=86, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=39 taskId=44 partitionId=0
[2025-07-18T15:54:03.581+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 85 for partition checkins-0
[2025-07-18T15:54:03.582+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:54:03.586+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.586+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:54:03.586+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=87, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.587+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 44, attempt 0, stage 44.0)
[2025-07-18T15:54:03.595+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.596+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:54:03.596+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=87, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.597+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 42, attempt 0, stage 42.0)
[2025-07-18T15:54:03.599+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DataWritingSparkTask: Committed partition 0 (task 44, attempt 0, stage 44.0)
[2025-07-18T15:54:03.599+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 5414541 nanos, during time span of 17933875 nanos.
[2025-07-18T15:54:03.600+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Finished task 0.0 in stage 44.0 (TID 44). 4818 bytes result sent to driver
[2025-07-18T15:54:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 28 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:03.601+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool
[2025-07-18T15:54:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: ResultStage 44 (start at <unknown>:0) finished in 0.033 s
[2025-07-18T15:54:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:03.603+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 44: Stage finished
[2025-07-18T15:54:03.604+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Job 44 finished: start at <unknown>:0, took 0.034203 s
[2025-07-18T15:54:03.604+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:03.604+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committing epoch 39 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:54:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DataWritingSparkTask: Committed partition 0 (task 42, attempt 0, stage 42.0)
[2025-07-18T15:54:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 505479458 nanos, during time span of 519892876 nanos.
[2025-07-18T15:54:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Finished task 0.0 in stage 42.0 (TID 42). 4699 bytes result sent to driver
[2025-07-18T15:54:03.611+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 531 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:03.611+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool
[2025-07-18T15:54:03.612+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: ResultStage 42 (start at <unknown>:0) finished in 0.537 s
[2025-07-18T15:54:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
[2025-07-18T15:54:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Job 42 finished: start at <unknown>:0, took 0.538567 s
[2025-07-18T15:54:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:03.614+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committing epoch 34 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:54:03.624+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:54:03.676+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v41.metadata.json
[2025-07-18T15:54:03.685+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v36.metadata.json
[2025-07-18T15:54:03.697+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SnapshotProducer: Committed snapshot 5976671781731283677 (FastAppend)
[2025-07-18T15:54:03.704+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SnapshotProducer: Committed snapshot 4111308432710803822 (FastAppend)
[2025-07-18T15:54:03.712+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=5976671781731283677, sequenceNumber=40, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.098164542S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=40}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=86}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2869}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=117865}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:03.712+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committed in 98 ms
[2025-07-18T15:54:03.713+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:54:03.716+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/39 using temp file file:/tmp/checkpoints/checkins/commits/.39.d0287d40-fdf4-4396-84ca-33164325b778.tmp
[2025-07-18T15:54:03.723+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=4111308432710803822, sequenceNumber=35, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.100154833S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=35}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=87}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3050}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=105557}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:03.725+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Committed in 100 ms
[2025-07-18T15:54:03.725+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 34, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:54:03.728+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/34 using temp file file:/tmp/checkpoints/reservations/commits/.34.7f83cdf4-60e9-4784-9ad8-c68c8bc99662.tmp
[2025-07-18T15:54:03.735+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.39.d0287d40-fdf4-4396-84ca-33164325b778.tmp to file:/tmp/checkpoints/checkins/commits/39
[2025-07-18T15:54:03.735+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:03.735+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:54:03.736+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:54:03.737+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:03.737+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:03.525Z",
[2025-07-18T15:54:03.738+0000] {subprocess.py:93} INFO -   "batchId" : 39,
[2025-07-18T15:54:03.738+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:54:03.738+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.906976744186047,
[2025-07-18T15:54:03.739+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.8076923076923075,
[2025-07-18T15:54:03.740+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:03.740+0000] {subprocess.py:93} INFO -     "addBatch" : 159,
[2025-07-18T15:54:03.741+0000] {subprocess.py:93} INFO -     "commitOffsets" : 24,
[2025-07-18T15:54:03.741+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:03.741+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:54:03.742+0000] {subprocess.py:93} INFO -     "queryPlanning" : 9,
[2025-07-18T15:54:03.742+0000] {subprocess.py:93} INFO -     "triggerExecution" : 208,
[2025-07-18T15:54:03.742+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T15:54:03.743+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:03.743+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:03.743+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:03.743+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:54:03.744+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:03.744+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:03.744+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:03.745+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.745+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.745+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:03.746+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:03.746+0000] {subprocess.py:93} INFO -         "0" : 86
[2025-07-18T15:54:03.747+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.747+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.747+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:03.748+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:03.748+0000] {subprocess.py:93} INFO -         "0" : 86
[2025-07-18T15:54:03.748+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.748+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.748+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.906976744186047,
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.8076923076923075,
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:03.749+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:03.750+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:03.750+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:54:03.750+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:54:03.750+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:03.750+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:03.750+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/40 using temp file file:/tmp/checkpoints/checkins/offsets/.40.23f75809-246e-4783-89d4-bd60e28efccd.tmp
[2025-07-18T15:54:03.750+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.34.7f83cdf4-60e9-4784-9ad8-c68c8bc99662.tmp to file:/tmp/checkpoints/reservations/commits/34
[2025-07-18T15:54:03.751+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:03.751+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:54:03.751+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:54:03.751+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:03.751+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:03.018Z",
[2025-07-18T15:54:03.751+0000] {subprocess.py:93} INFO -   "batchId" : 34,
[2025-07-18T15:54:03.752+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:54:03.752+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.576659038901602,
[2025-07-18T15:54:03.752+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.751031636863824,
[2025-07-18T15:54:03.752+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:03.752+0000] {subprocess.py:93} INFO -     "addBatch" : 674,
[2025-07-18T15:54:03.752+0000] {subprocess.py:93} INFO -     "commitOffsets" : 25,
[2025-07-18T15:54:03.753+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:03.753+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:54:03.753+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:54:03.753+0000] {subprocess.py:93} INFO -     "triggerExecution" : 727,
[2025-07-18T15:54:03.754+0000] {subprocess.py:93} INFO -     "walCommit" : 19
[2025-07-18T15:54:03.754+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:03.754+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:03.754+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:03.755+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:54:03.756+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:54:03.756+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.756+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.756+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:03.756+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:54:03.756+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:54:03.757+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:03.757+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:03.757+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:54:03.757+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.576659038901602,
[2025-07-18T15:54:03.757+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.751031636863824,
[2025-07-18T15:54:03.757+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:54:03.758+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:03.759+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:03.759+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.40.23f75809-246e-4783-89d4-bd60e28efccd.tmp to file:/tmp/checkpoints/checkins/offsets/40
[2025-07-18T15:54:03.759+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1752854043736,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:03.762+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.762+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.762+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.763+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.764+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.767+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.767+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.767+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.768+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.769+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.772+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.773+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.773+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:03.773+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.774+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.781+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:54:03.784+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:54:03.785+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:54:03.786+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 90 from start at <unknown>:0
[2025-07-18T15:54:03.786+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:03.787+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:03.791+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Got job 45 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:03.792+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Final stage: ResultStage 45 (start at <unknown>:0)
[2025-07-18T15:54:03.793+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:03.794+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:03.794+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[183] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:03.795+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 28.0 KiB, free 433.7 MiB)
[2025-07-18T15:54:03.795+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.7 MiB)
[2025-07-18T15:54:03.797+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:54:03.798+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:03.798+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[183] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:03.798+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
[2025-07-18T15:54:03.798+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:54:03.798+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Running task 0.0 in stage 45.0 (TID 45)
[2025-07-18T15:54:03.802+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:03.804+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=86 untilOffset=87, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=40 taskId=45 partitionId=0
[2025-07-18T15:54:03.805+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 86 for partition checkins-0
[2025-07-18T15:54:03.805+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:54:03.807+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/37 using temp file file:/tmp/checkpoints/feedback/offsets/.37.15d79aea-0c1b-4540-a77a-a0ea2a8a746d.tmp
[2025-07-18T15:54:03.821+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.37.15d79aea-0c1b-4540-a77a-a0ea2a8a746d.tmp to file:/tmp/checkpoints/feedback/offsets/37
[2025-07-18T15:54:03.821+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1752854043795,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:03.826+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.826+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.826+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.828+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.828+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.832+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.834+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.835+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.835+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.836+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.854+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.859+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.866+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:03.867+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.868+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:03.877+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
[2025-07-18T15:54:03.878+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.7 MiB)
[2025-07-18T15:54:03.878+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:54:03.879+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 92 from start at <unknown>:0
[2025-07-18T15:54:03.879+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:03.880+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:03.882+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Got job 46 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:03.883+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Final stage: ResultStage 46 (start at <unknown>:0)
[2025-07-18T15:54:03.883+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:03.883+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:03.883+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[187] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:03.885+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 27.5 KiB, free 433.6 MiB)
[2025-07-18T15:54:03.885+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.6 MiB)
[2025-07-18T15:54:03.886+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:54:03.887+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:03.887+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[187] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:03.887+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2025-07-18T15:54:03.889+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:54:03.891+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO Executor: Running task 0.0 in stage 46.0 (TID 46)
[2025-07-18T15:54:03.893+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:03.893+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=84 untilOffset=85, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=37 taskId=46 partitionId=0
[2025-07-18T15:54:03.895+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 84 for partition feedback-0
[2025-07-18T15:54:03.900+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:54:03.992+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.992+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:54:03.993+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=86, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:03.994+0000] {subprocess.py:93} INFO - 25/07/18 15:54:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 46, attempt 0, stage 46.0)
[2025-07-18T15:54:04.005+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DataWritingSparkTask: Committed partition 0 (task 46, attempt 0, stage 46.0)
[2025-07-18T15:54:04.005+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 97738125 nanos, during time span of 110150500 nanos.
[2025-07-18T15:54:04.005+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO Executor: Finished task 0.0 in stage 46.0 (TID 46). 4779 bytes result sent to driver
[2025-07-18T15:54:04.006+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 118 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:04.007+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2025-07-18T15:54:04.007+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: ResultStage 46 (start at <unknown>:0) finished in 0.123 s
[2025-07-18T15:54:04.007+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:04.007+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2025-07-18T15:54:04.007+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 46 finished: start at <unknown>:0, took 0.127106 s
[2025-07-18T15:54:04.007+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:04.008+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing epoch 37 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:54:04.012+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.059+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v39.metadata.json
[2025-07-18T15:54:04.074+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SnapshotProducer: Committed snapshot 8358828184005452014 (FastAppend)
[2025-07-18T15:54:04.090+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=8358828184005452014, sequenceNumber=38, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.075455S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=38}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=85}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2906}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=111865}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:04.090+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committed in 75 ms
[2025-07-18T15:54:04.090+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:54:04.093+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/37 using temp file file:/tmp/checkpoints/feedback/commits/.37.0ed7707a-3fc9-45d8-822c-a5841c0e450c.tmp
[2025-07-18T15:54:04.104+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.37.0ed7707a-3fc9-45d8-822c-a5841c0e450c.tmp to file:/tmp/checkpoints/feedback/commits/37
[2025-07-18T15:54:04.104+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:04.104+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:03.789Z",
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "batchId" : 37,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 3.1847133757961785,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -     "addBatch" : 260,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -     "commitOffsets" : 15,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:04.105+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -     "triggerExecution" : 314,
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -     "walCommit" : 25
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -         "0" : 84
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.106+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.107+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 3.1847133757961785,
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:04.108+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:04.109+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:04.109+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:54:04.109+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:54:04.109+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:04.109+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:04.109+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/38 using temp file file:/tmp/checkpoints/feedback/offsets/.38.d0630b2a-7ecc-445d-84ff-2c5a4539ff43.tmp
[2025-07-18T15:54:04.118+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.38.d0630b2a-7ecc-445d-84ff-2c5a4539ff43.tmp to file:/tmp/checkpoints/feedback/offsets/38
[2025-07-18T15:54:04.118+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1752854044105,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:04.121+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.122+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.122+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.122+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.122+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.125+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.125+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.126+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.126+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.126+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.130+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.130+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.130+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.130+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.131+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.137+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
[2025-07-18T15:54:04.138+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.6 MiB)
[2025-07-18T15:54:04.143+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.0 MiB)
[2025-07-18T15:54:04.143+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkContext: Created broadcast 94 from start at <unknown>:0
[2025-07-18T15:54:04.143+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:04.144+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:54:04.144+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:04.146+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Got job 47 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Final stage: ResultStage 47 (start at <unknown>:0)
[2025-07-18T15:54:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:04.147+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[191] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:04.148+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 27.5 KiB, free 433.6 MiB)
[2025-07-18T15:54:04.148+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_93_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:54:04.148+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.6 MiB)
[2025-07-18T15:54:04.148+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:54:04.148+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:04.149+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[191] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:04.149+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0
[2025-07-18T15:54:04.149+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:54:04.149+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO Executor: Running task 0.0 in stage 47.0 (TID 47)
[2025-07-18T15:54:04.149+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:54:04.150+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.1 MiB)
[2025-07-18T15:54:04.151+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:04.151+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=85 untilOffset=86, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=38 taskId=47 partitionId=0
[2025-07-18T15:54:04.152+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:54:04.153+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:54:04.154+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 85 for partition feedback-0
[2025-07-18T15:54:04.154+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:54:04.157+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_92_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:54:04.159+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:54:04.160+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:54:04.161+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:54:04.193+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:04.193+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:54:04.193+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=87, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:04.194+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 47, attempt 0, stage 47.0)
[2025-07-18T15:54:04.203+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DataWritingSparkTask: Committed partition 0 (task 47, attempt 0, stage 47.0)
[2025-07-18T15:54:04.203+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 39156500 nanos, during time span of 49432959 nanos.
[2025-07-18T15:54:04.204+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO Executor: Finished task 0.0 in stage 47.0 (TID 47). 4769 bytes result sent to driver
[2025-07-18T15:54:04.205+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 57 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:04.205+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool
[2025-07-18T15:54:04.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: ResultStage 47 (start at <unknown>:0) finished in 0.060 s
[2025-07-18T15:54:04.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:04.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
[2025-07-18T15:54:04.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 47 finished: start at <unknown>:0, took 0.061496 s
[2025-07-18T15:54:04.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:04.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing epoch 38 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:54:04.211+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.259+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v40.metadata.json
[2025-07-18T15:54:04.272+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SnapshotProducer: Committed snapshot 1630502019272983655 (FastAppend)
[2025-07-18T15:54:04.281+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=1630502019272983655, sequenceNumber=39, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.069028125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=39}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=86}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2913}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=114778}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:04.281+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committed in 69 ms
[2025-07-18T15:54:04.281+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:54:04.284+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/38 using temp file file:/tmp/checkpoints/feedback/commits/.38.f4066b0b-c09e-4307-a640-2d1516e5946e.tmp
[2025-07-18T15:54:04.293+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.38.f4066b0b-c09e-4307-a640-2d1516e5946e.tmp to file:/tmp/checkpoints/feedback/commits/38
[2025-07-18T15:54:04.293+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:04.104Z",
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "batchId" : 38,
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.1746031746031744,
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 5.319148936170213,
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -     "addBatch" : 157,
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -     "commitOffsets" : 12,
[2025-07-18T15:54:04.294+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -     "triggerExecution" : 188,
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -     "walCommit" : 12
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -         "0" : 85
[2025-07-18T15:54:04.295+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -         "0" : 86
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:04.296+0000] {subprocess.py:93} INFO -         "0" : 86
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.1746031746031744,
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 5.319148936170213,
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:04.297+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:04.298+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/39 using temp file file:/tmp/checkpoints/feedback/offsets/.39.d1d415af-deb9-4a95-b727-c74571d778ce.tmp
[2025-07-18T15:54:04.306+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:04.307+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:54:04.307+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=87, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:04.307+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.39.d1d415af-deb9-4a95-b727-c74571d778ce.tmp to file:/tmp/checkpoints/feedback/offsets/39
[2025-07-18T15:54:04.308+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1752854044294,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:54:04.308+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 45, attempt 0, stage 45.0)
[2025-07-18T15:54:04.311+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.312+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.312+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.313+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.313+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.317+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.317+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.317+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.318+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.318+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.318+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DataWritingSparkTask: Committed partition 0 (task 45, attempt 0, stage 45.0)
[2025-07-18T15:54:04.318+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 505761625 nanos, during time span of 517776292 nanos.
[2025-07-18T15:54:04.322+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO Executor: Finished task 0.0 in stage 45.0 (TID 45). 4836 bytes result sent to driver
[2025-07-18T15:54:04.323+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 529 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:04.323+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool
[2025-07-18T15:54:04.323+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: ResultStage 45 (start at <unknown>:0) finished in 0.534 s
[2025-07-18T15:54:04.323+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:04.323+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 45 finished: start at <unknown>:0, took 0.535762 s
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing epoch 40 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.324+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:54:04.327+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:54:04.329+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:54:04.330+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:54:04.331+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:54:04.331+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkContext: Created broadcast 96 from start at <unknown>:0
[2025-07-18T15:54:04.331+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:54:04.331+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:54:04.332+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Got job 48 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:54:04.332+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Final stage: ResultStage 48 (start at <unknown>:0)
[2025-07-18T15:54:04.332+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:54:04.333+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:54:04.333+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[195] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:54:04.333+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 27.5 KiB, free 433.9 MiB)
[2025-07-18T15:54:04.334+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.9 MiB)
[2025-07-18T15:54:04.334+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:54:04.334+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:54:04.335+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 48 (MapPartitionsRDD[195] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:54:04.335+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks resource profile 0
[2025-07-18T15:54:04.336+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:54:04.336+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO Executor: Running task 0.0 in stage 48.0 (TID 48)
[2025-07-18T15:54:04.339+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:54:04.339+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=86 untilOffset=87, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=39 taskId=48 partitionId=0
[2025-07-18T15:54:04.342+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 86 for partition feedback-0
[2025-07-18T15:54:04.342+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:54:04.376+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v42.metadata.json
[2025-07-18T15:54:04.390+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SnapshotProducer: Committed snapshot 4931438893826461010 (FastAppend)
[2025-07-18T15:54:04.406+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=4931438893826461010, sequenceNumber=41, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.075820375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=41}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=87}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2813}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=120678}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:04.407+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committed in 76 ms
[2025-07-18T15:54:04.407+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:54:04.408+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/40 using temp file file:/tmp/checkpoints/checkins/commits/.40.bfa3386d-b8e1-4c15-9e88-f74f73f9c724.tmp
[2025-07-18T15:54:04.435+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.40.bfa3386d-b8e1-4c15-9e88-f74f73f9c724.tmp to file:/tmp/checkpoints/checkins/commits/40
[2025-07-18T15:54:04.437+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:04.437+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:54:04.437+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:54:04.438+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:04.438+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:03.733Z",
[2025-07-18T15:54:04.438+0000] {subprocess.py:93} INFO -   "batchId" : 40,
[2025-07-18T15:54:04.439+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:54:04.439+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.8076923076923075,
[2025-07-18T15:54:04.441+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.4409221902017293,
[2025-07-18T15:54:04.441+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:04.442+0000] {subprocess.py:93} INFO -     "addBatch" : 639,
[2025-07-18T15:54:04.443+0000] {subprocess.py:93} INFO -     "commitOffsets" : 24,
[2025-07-18T15:54:04.444+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:04.446+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:54:04.446+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:54:04.446+0000] {subprocess.py:93} INFO -     "triggerExecution" : 694,
[2025-07-18T15:54:04.447+0000] {subprocess.py:93} INFO -     "walCommit" : 21
[2025-07-18T15:54:04.447+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:04.448+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:04.449+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:04.449+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:54:04.450+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:04.450+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:04.451+0000] {subprocess.py:93} INFO -         "0" : 86
[2025-07-18T15:54:04.451+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.451+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.451+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:04.451+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:04.452+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:54:04.452+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.452+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.453+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:04.454+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:54:04.454+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:54:04.455+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:04.456+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:04.456+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:54:04.457+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.8076923076923075,
[2025-07-18T15:54:04.457+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.4409221902017293,
[2025-07-18T15:54:04.459+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:04.459+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:04.460+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:04.460+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:04.460+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:04.461+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:04.461+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:04.462+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:54:04.462+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:54:04.462+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:04.462+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:04.845+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:04.846+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:54:04.846+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=87, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:54:04.849+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 48, attempt 0, stage 48.0)
[2025-07-18T15:54:04.873+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DataWritingSparkTask: Committed partition 0 (task 48, attempt 0, stage 48.0)
[2025-07-18T15:54:04.874+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 504582709 nanos, during time span of 531763458 nanos.
[2025-07-18T15:54:04.875+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO Executor: Finished task 0.0 in stage 48.0 (TID 48). 4777 bytes result sent to driver
[2025-07-18T15:54:04.875+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 540 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:54:04.880+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2025-07-18T15:54:04.880+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: ResultStage 48 (start at <unknown>:0) finished in 0.545 s
[2025-07-18T15:54:04.881+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:54:04.881+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
[2025-07-18T15:54:04.881+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO DAGScheduler: Job 48 finished: start at <unknown>:0, took 0.547058 s
[2025-07-18T15:54:04.881+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:54:04.882+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing epoch 39 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:54:04.900+0000] {subprocess.py:93} INFO - 25/07/18 15:54:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:54:05.040+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v41.metadata.json
[2025-07-18T15:54:05.086+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO SnapshotProducer: Committed snapshot 5738651226808051065 (FastAppend)
[2025-07-18T15:54:05.134+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=5738651226808051065, sequenceNumber=40, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.232739166S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=40}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=87}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2841}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=117619}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:54:05.138+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO SparkWrite: Committed in 233 ms
[2025-07-18T15:54:05.141+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:54:05.142+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/39 using temp file file:/tmp/checkpoints/feedback/commits/.39.e7f46df5-f70e-40a3-a649-e1a1395105f4.tmp
[2025-07-18T15:54:05.180+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.39.e7f46df5-f70e-40a3-a649-e1a1395105f4.tmp to file:/tmp/checkpoints/feedback/commits/39
[2025-07-18T15:54:05.181+0000] {subprocess.py:93} INFO - 25/07/18 15:54:05 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:54:05.181+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:54:05.182+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:54:05.183+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:54:05.186+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:54:04.293Z",
[2025-07-18T15:54:05.186+0000] {subprocess.py:93} INFO -   "batchId" : 39,
[2025-07-18T15:54:05.186+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:54:05.187+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 5.291005291005291,
[2025-07-18T15:54:05.187+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.1325028312570782,
[2025-07-18T15:54:05.187+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:54:05.189+0000] {subprocess.py:93} INFO -     "addBatch" : 816,
[2025-07-18T15:54:05.193+0000] {subprocess.py:93} INFO -     "commitOffsets" : 47,
[2025-07-18T15:54:05.194+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:54:05.194+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:54:05.194+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:54:05.194+0000] {subprocess.py:93} INFO -     "triggerExecution" : 883,
[2025-07-18T15:54:05.194+0000] {subprocess.py:93} INFO -     "walCommit" : 13
[2025-07-18T15:54:05.194+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:54:05.194+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:54:05.195+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:54:05.195+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:54:05.196+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:54:05.197+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:05.198+0000] {subprocess.py:93} INFO -         "0" : 86
[2025-07-18T15:54:05.199+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:05.201+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:05.202+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:54:05.202+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:05.204+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:54:05.205+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:05.205+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:05.205+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:54:05.205+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:54:05.205+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:54:05.205+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:54:05.206+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:54:05.206+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:54:05.206+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 5.291005291005291,
[2025-07-18T15:54:05.206+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.1325028312570782,
[2025-07-18T15:54:05.208+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:54:05.209+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:54:05.210+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:54:05.210+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:54:05.211+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:54:05.211+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:54:05.211+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:54:05.211+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:54:05.212+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:54:05.212+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:54:05.212+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:54:13.759+0000] {subprocess.py:93} INFO - 25/07/18 15:54:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:14.434+0000] {subprocess.py:93} INFO - 25/07/18 15:54:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:15.188+0000] {subprocess.py:93} INFO - 25/07/18 15:54:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:18.817+0000] {subprocess.py:93} INFO - 25/07/18 15:54:18 INFO BlockManagerInfo: Removed broadcast_94_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:54:18.822+0000] {subprocess.py:93} INFO - 25/07/18 15:54:18 INFO BlockManagerInfo: Removed broadcast_96_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:54:18.824+0000] {subprocess.py:93} INFO - 25/07/18 15:54:18 INFO BlockManagerInfo: Removed broadcast_95_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:54:18.827+0000] {subprocess.py:93} INFO - 25/07/18 15:54:18 INFO BlockManagerInfo: Removed broadcast_91_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T15:54:18.830+0000] {subprocess.py:93} INFO - 25/07/18 15:54:18 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:54:18.832+0000] {subprocess.py:93} INFO - 25/07/18 15:54:18 INFO BlockManagerInfo: Removed broadcast_97_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:54:23.768+0000] {subprocess.py:93} INFO - 25/07/18 15:54:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:24.437+0000] {subprocess.py:93} INFO - 25/07/18 15:54:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:25.194+0000] {subprocess.py:93} INFO - 25/07/18 15:54:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:33.776+0000] {subprocess.py:93} INFO - 25/07/18 15:54:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:34.450+0000] {subprocess.py:93} INFO - 25/07/18 15:54:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:35.206+0000] {subprocess.py:93} INFO - 25/07/18 15:54:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:43.789+0000] {subprocess.py:93} INFO - 25/07/18 15:54:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:44.452+0000] {subprocess.py:93} INFO - 25/07/18 15:54:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:45.208+0000] {subprocess.py:93} INFO - 25/07/18 15:54:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:53.792+0000] {subprocess.py:93} INFO - 25/07/18 15:54:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:54.453+0000] {subprocess.py:93} INFO - 25/07/18 15:54:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:54:55.212+0000] {subprocess.py:93} INFO - 25/07/18 15:54:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:03.798+0000] {subprocess.py:93} INFO - 25/07/18 15:55:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:04.459+0000] {subprocess.py:93} INFO - 25/07/18 15:55:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:05.218+0000] {subprocess.py:93} INFO - 25/07/18 15:55:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:13.810+0000] {subprocess.py:93} INFO - 25/07/18 15:55:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:14.454+0000] {subprocess.py:93} INFO - 25/07/18 15:55:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:15.217+0000] {subprocess.py:93} INFO - 25/07/18 15:55:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:23.818+0000] {subprocess.py:93} INFO - 25/07/18 15:55:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:24.456+0000] {subprocess.py:93} INFO - 25/07/18 15:55:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:25.226+0000] {subprocess.py:93} INFO - 25/07/18 15:55:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:33.825+0000] {subprocess.py:93} INFO - 25/07/18 15:55:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:34.463+0000] {subprocess.py:93} INFO - 25/07/18 15:55:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:35.230+0000] {subprocess.py:93} INFO - 25/07/18 15:55:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:43.819+0000] {subprocess.py:93} INFO - 25/07/18 15:55:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:44.470+0000] {subprocess.py:93} INFO - 25/07/18 15:55:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:45.235+0000] {subprocess.py:93} INFO - 25/07/18 15:55:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:53.828+0000] {subprocess.py:93} INFO - 25/07/18 15:55:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:54.482+0000] {subprocess.py:93} INFO - 25/07/18 15:55:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:55:55.238+0000] {subprocess.py:93} INFO - 25/07/18 15:55:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:01.888+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/35 using temp file file:/tmp/checkpoints/reservations/offsets/.35.1fd64fe7-5dc2-4407-87d1-6a57ef8c3876.tmp
[2025-07-18T15:56:01.915+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.35.1fd64fe7-5dc2-4407-87d1-6a57ef8c3876.tmp to file:/tmp/checkpoints/reservations/offsets/35
[2025-07-18T15:56:01.917+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO MicroBatchExecution: Committed offsets for batch 35. Metadata OffsetSeqMetadata(0,1752854161875,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:01.949+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.949+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.950+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.950+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:01.952+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:01.958+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.959+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.959+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.962+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:01.962+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:01.967+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.968+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.968+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:01.968+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:01.969+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:01.986+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:56:01.988+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T15:56:01.989+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:56:01.989+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkContext: Created broadcast 98 from start at <unknown>:0
[2025-07-18T15:56:01.990+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:01.991+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:01.992+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO DAGScheduler: Got job 49 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:01.992+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO DAGScheduler: Final stage: ResultStage 49 (start at <unknown>:0)
[2025-07-18T15:56:01.992+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:01.993+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:01.993+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[199] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:01.995+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:56:01.998+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:56:01.998+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:56:01.999+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:01.999+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[199] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:01.999+0000] {subprocess.py:93} INFO - 25/07/18 15:56:01 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0
[2025-07-18T15:56:02.002+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:56:02.003+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Running task 0.0 in stage 49.0 (TID 49)
[2025-07-18T15:56:02.012+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:02.013+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=87 untilOffset=88, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=35 taskId=49 partitionId=0
[2025-07-18T15:56:02.016+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 87 for partition reservations-0
[2025-07-18T15:56:02.022+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:56:02.071+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.072+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:56:02.072+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=89, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.078+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 49, attempt 0, stage 49.0)
[2025-07-18T15:56:02.103+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Committed partition 0 (task 49, attempt 0, stage 49.0)
[2025-07-18T15:56:02.103+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 54926500 nanos, during time span of 85356292 nanos.
[2025-07-18T15:56:02.103+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Finished task 0.0 in stage 49.0 (TID 49). 4744 bytes result sent to driver
[2025-07-18T15:56:02.108+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 104 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:02.108+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2025-07-18T15:56:02.109+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: ResultStage 49 (start at <unknown>:0) finished in 0.113 s
[2025-07-18T15:56:02.109+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:02.109+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished
[2025-07-18T15:56:02.110+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 49 finished: start at <unknown>:0, took 0.116774 s
[2025-07-18T15:56:02.110+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:02.110+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing epoch 35 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:56:02.119+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.192+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v37.metadata.json
[2025-07-18T15:56:02.218+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SnapshotProducer: Committed snapshot 5483487679547744819 (FastAppend)
[2025-07-18T15:56:02.239+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5483487679547744819, sequenceNumber=36, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.119362208S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=36}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=88}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3004}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=108561}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:02.242+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committed in 119 ms
[2025-07-18T15:56:02.242+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 35, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:56:02.246+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/35 using temp file file:/tmp/checkpoints/reservations/commits/.35.1dc0c318-d814-4d2e-91be-079f097bbefb.tmp
[2025-07-18T15:56:02.262+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.35.1dc0c318-d814-4d2e-91be-079f097bbefb.tmp to file:/tmp/checkpoints/reservations/commits/35
[2025-07-18T15:56:02.263+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:02.264+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:56:02.264+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:56:02.264+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:02.264+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:01.874Z",
[2025-07-18T15:56:02.264+0000] {subprocess.py:93} INFO -   "batchId" : 35,
[2025-07-18T15:56:02.264+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:02.264+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:56:02.265+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.5906735751295336,
[2025-07-18T15:56:02.265+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:02.265+0000] {subprocess.py:93} INFO -     "addBatch" : 284,
[2025-07-18T15:56:02.266+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T15:56:02.266+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T15:56:02.268+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:56:02.269+0000] {subprocess.py:93} INFO -     "queryPlanning" : 33,
[2025-07-18T15:56:02.270+0000] {subprocess.py:93} INFO -     "triggerExecution" : 386,
[2025-07-18T15:56:02.272+0000] {subprocess.py:93} INFO -     "walCommit" : 40
[2025-07-18T15:56:02.273+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:02.273+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:02.275+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:02.276+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:56:02.276+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:02.276+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:02.277+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:56:02.277+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.277+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.277+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:02.278+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:02.278+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:02.278+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.279+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.280+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:02.280+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:02.281+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:02.281+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.281+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.281+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:02.281+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 90.90909090909092,
[2025-07-18T15:56:02.282+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.5906735751295336,
[2025-07-18T15:56:02.282+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:02.282+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:02.283+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:02.283+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:02.283+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:02.283+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:02.283+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:02.283+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:56:02.283+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:02.284+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:02.284+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:02.284+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/36 using temp file file:/tmp/checkpoints/reservations/offsets/.36.34deb410-f238-4cd8-ad23-27a76763310b.tmp
[2025-07-18T15:56:02.299+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.36.34deb410-f238-4cd8-ad23-27a76763310b.tmp to file:/tmp/checkpoints/reservations/offsets/36
[2025-07-18T15:56:02.300+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MicroBatchExecution: Committed offsets for batch 36. Metadata OffsetSeqMetadata(0,1752854162263,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:02.300+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.300+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.301+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.301+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.302+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.305+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.305+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.306+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.306+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.307+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.310+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.310+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.310+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.311+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.312+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.317+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:56:02.322+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.1 MiB)
[2025-07-18T15:56:02.323+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:56:02.323+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 100 from start at <unknown>:0
[2025-07-18T15:56:02.323+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:02.323+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Got job 50 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Final stage: ResultStage 50 (start at <unknown>:0)
[2025-07-18T15:56:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[203] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T15:56:02.327+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T15:56:02.327+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:56:02.328+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:02.329+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 50 (MapPartitionsRDD[203] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:02.329+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks resource profile 0
[2025-07-18T15:56:02.329+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:56:02.329+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Running task 0.0 in stage 50.0 (TID 50)
[2025-07-18T15:56:02.332+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:02.333+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=88 untilOffset=89, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=36 taskId=50 partitionId=0
[2025-07-18T15:56:02.335+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 88 for partition reservations-0
[2025-07-18T15:56:02.335+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:56:02.336+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.336+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:56:02.336+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=90, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.337+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 50, attempt 0, stage 50.0)
[2025-07-18T15:56:02.347+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Committed partition 0 (task 50, attempt 0, stage 50.0)
[2025-07-18T15:56:02.347+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 1704791 nanos, during time span of 11140458 nanos.
[2025-07-18T15:56:02.347+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Finished task 0.0 in stage 50.0 (TID 50). 4691 bytes result sent to driver
[2025-07-18T15:56:02.348+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 19 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:02.348+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool
[2025-07-18T15:56:02.348+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: ResultStage 50 (start at <unknown>:0) finished in 0.025 s
[2025-07-18T15:56:02.349+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:02.349+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished
[2025-07-18T15:56:02.349+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 50 finished: start at <unknown>:0, took 0.026115 s
[2025-07-18T15:56:02.349+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:02.349+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing epoch 36 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:56:02.357+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.427+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v38.metadata.json
[2025-07-18T15:56:02.445+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SnapshotProducer: Committed snapshot 6861002677683993350 (FastAppend)
[2025-07-18T15:56:02.458+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=6861002677683993350, sequenceNumber=37, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.102161333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=37}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=89}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2934}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=111495}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:02.458+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committed in 102 ms
[2025-07-18T15:56:02.459+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 36, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:56:02.462+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/36 using temp file file:/tmp/checkpoints/reservations/commits/.36.a02cf4ae-a1c3-4149-92a1-08aa5e1ec77d.tmp
[2025-07-18T15:56:02.474+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.36.a02cf4ae-a1c3-4149-92a1-08aa5e1ec77d.tmp to file:/tmp/checkpoints/reservations/commits/36
[2025-07-18T15:56:02.474+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:02.474+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:56:02.474+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:02.262Z",
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -   "batchId" : 36,
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.5773195876288657,
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.739336492890995,
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -     "addBatch" : 155,
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T15:56:02.475+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -     "triggerExecution" : 211,
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -     "walCommit" : 32
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:56:02.476+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:02.477+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.5773195876288657,
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.739336492890995,
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:02.478+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:02.479+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/37 using temp file file:/tmp/checkpoints/reservations/offsets/.37.5377fd5b-c589-4d90-8cf5-0b83fd647550.tmp
[2025-07-18T15:56:02.486+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/41 using temp file file:/tmp/checkpoints/checkins/offsets/.41.b54270f3-3512-4f57-bc05-3b211b3f9bc1.tmp
[2025-07-18T15:56:02.491+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.37.5377fd5b-c589-4d90-8cf5-0b83fd647550.tmp to file:/tmp/checkpoints/reservations/offsets/37
[2025-07-18T15:56:02.491+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MicroBatchExecution: Committed offsets for batch 37. Metadata OffsetSeqMetadata(0,1752854162474,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:02.495+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.496+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.496+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.497+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.497+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.500+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.41.b54270f3-3512-4f57-bc05-3b211b3f9bc1.tmp to file:/tmp/checkpoints/checkins/offsets/41
[2025-07-18T15:56:02.500+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1752854162482,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:02.501+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.502+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.502+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.503+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.503+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.503+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.503+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.503+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.505+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.505+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.506+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.506+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.507+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:02.507+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.508+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.508+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.508+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.508+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.509+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.510+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.513+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.514+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.514+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.515+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.516+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:56:02.516+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.517+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T15:56:02.522+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:56:02.523+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 102 from start at <unknown>:0
[2025-07-18T15:56:02.523+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:02.524+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:02.524+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Got job 51 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:02.524+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Final stage: ResultStage 51 (start at <unknown>:0)
[2025-07-18T15:56:02.524+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:02.524+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:02.525+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[207] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:02.525+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 28.6 KiB, free 433.9 MiB)
[2025-07-18T15:56:02.525+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 433.9 MiB)
[2025-07-18T15:56:02.526+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:56:02.526+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:56:02.526+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:02.526+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[207] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:02.526+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
[2025-07-18T15:56:02.526+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:56:02.527+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Running task 0.0 in stage 51.0 (TID 51)
[2025-07-18T15:56:02.527+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 433.9 MiB)
[2025-07-18T15:56:02.527+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:56:02.527+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 104 from start at <unknown>:0
[2025-07-18T15:56:02.527+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:02.528+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:02.528+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Got job 52 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:02.528+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Final stage: ResultStage 52 (start at <unknown>:0)
[2025-07-18T15:56:02.528+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:02.529+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:02.529+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[211] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:02.529+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T15:56:02.531+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T15:56:02.536+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:56:02.537+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:02.537+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[211] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:02.537+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
[2025-07-18T15:56:02.537+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:56:02.537+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Running task 0.0 in stage 52.0 (TID 52)
[2025-07-18T15:56:02.538+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:02.538+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=89 untilOffset=90, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=37 taskId=51 partitionId=0
[2025-07-18T15:56:02.538+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:02.538+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 89 for partition reservations-0
[2025-07-18T15:56:02.538+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:56:02.538+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=87 untilOffset=88, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=41 taskId=52 partitionId=0
[2025-07-18T15:56:02.540+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 87 for partition checkins-0
[2025-07-18T15:56:02.541+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:56:02.678+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.680+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:56:02.680+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=89, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.681+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 52, attempt 0, stage 52.0)
[2025-07-18T15:56:02.695+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Committed partition 0 (task 52, attempt 0, stage 52.0)
[2025-07-18T15:56:02.695+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 137249292 nanos, during time span of 154604416 nanos.
[2025-07-18T15:56:02.695+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Finished task 0.0 in stage 52.0 (TID 52). 4797 bytes result sent to driver
[2025-07-18T15:56:02.696+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 164 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool
[2025-07-18T15:56:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: ResultStage 52 (start at <unknown>:0) finished in 0.168 s
[2025-07-18T15:56:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
[2025-07-18T15:56:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 52 finished: start at <unknown>:0, took 0.169741 s
[2025-07-18T15:56:02.697+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:02.698+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing epoch 41 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:56:02.705+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.765+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v43.metadata.json
[2025-07-18T15:56:02.778+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SnapshotProducer: Committed snapshot 212761863100652041 (FastAppend)
[2025-07-18T15:56:02.792+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=212761863100652041, sequenceNumber=42, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.087936792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=42}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=88}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2827}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=123505}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:02.792+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committed in 89 ms
[2025-07-18T15:56:02.792+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:56:02.795+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/41 using temp file file:/tmp/checkpoints/checkins/commits/.41.b11477d2-4de5-4510-9c0e-e03519801373.tmp
[2025-07-18T15:56:02.806+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.41.b11477d2-4de5-4510-9c0e-e03519801373.tmp to file:/tmp/checkpoints/checkins/commits/41
[2025-07-18T15:56:02.806+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:02.806+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:56:02.806+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:02.481Z",
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -   "batchId" : 41,
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 3.0864197530864197,
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -     "addBatch" : 286,
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -     "commitOffsets" : 14,
[2025-07-18T15:56:02.807+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -     "triggerExecution" : 324,
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -     "walCommit" : 17
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:02.808+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:02.809+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:56:02.810+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 3.0864197530864197,
[2025-07-18T15:56:02.811+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:02.811+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:02.811+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:02.811+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:02.811+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:02.811+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:02.811+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:02.812+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:56:02.812+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:02.812+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:02.812+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:02.812+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/42 using temp file file:/tmp/checkpoints/checkins/offsets/.42.4782d968-4c72-4350-9a80-9a13437b2f45.tmp
[2025-07-18T15:56:02.822+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.42.4782d968-4c72-4350-9a80-9a13437b2f45.tmp to file:/tmp/checkpoints/checkins/offsets/42
[2025-07-18T15:56:02.822+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1752854162806,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:02.826+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.826+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.826+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.827+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.830+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.830+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.831+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.831+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.832+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.835+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.836+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.836+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.837+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.838+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:02.847+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:56:02.850+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:56:02.851+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:56:02.851+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 106 from start at <unknown>:0
[2025-07-18T15:56:02.851+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 42, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:02.852+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:02.852+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Got job 53 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:02.852+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Final stage: ResultStage 53 (start at <unknown>:0)
[2025-07-18T15:56:02.852+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:02.853+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:02.853+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[215] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:02.853+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 28.0 KiB, free 433.7 MiB)
[2025-07-18T15:56:02.855+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.7 MiB)
[2025-07-18T15:56:02.855+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:56:02.855+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (MapPartitionsRDD[215] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
[2025-07-18T15:56:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 53) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:56:02.856+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Running task 0.0 in stage 53.0 (TID 53)
[2025-07-18T15:56:02.860+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:02.861+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=88 untilOffset=89, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=42 taskId=53 partitionId=0
[2025-07-18T15:56:02.863+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 88 for partition checkins-0
[2025-07-18T15:56:02.863+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:56:02.878+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.879+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:56:02.879+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=90, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:02.880+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 53, attempt 0, stage 53.0)
[2025-07-18T15:56:02.891+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DataWritingSparkTask: Committed partition 0 (task 53, attempt 0, stage 53.0)
[2025-07-18T15:56:02.891+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 15994709 nanos, during time span of 27115959 nanos.
[2025-07-18T15:56:02.892+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO Executor: Finished task 0.0 in stage 53.0 (TID 53). 4832 bytes result sent to driver
[2025-07-18T15:56:02.895+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 53) in 37 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:02.896+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool
[2025-07-18T15:56:02.896+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: ResultStage 53 (start at <unknown>:0) finished in 0.042 s
[2025-07-18T15:56:02.896+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:02.896+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
[2025-07-18T15:56:02.896+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO DAGScheduler: Job 53 finished: start at <unknown>:0, took 0.044631 s
[2025-07-18T15:56:02.897+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:02.897+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing epoch 42 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:56:02.905+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:02.966+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v44.metadata.json
[2025-07-18T15:56:02.979+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SnapshotProducer: Committed snapshot 9054936334384992020 (FastAppend)
[2025-07-18T15:56:02.989+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=9054936334384992020, sequenceNumber=43, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.083264166S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=43}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=89}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2960}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=126465}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:02.989+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO SparkWrite: Committed in 83 ms
[2025-07-18T15:56:02.989+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:56:02.991+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/42 using temp file file:/tmp/checkpoints/checkins/commits/.42.07ab92e8-c87a-4e66-b661-2b60b1f0f3d3.tmp
[2025-07-18T15:56:03.000+0000] {subprocess.py:93} INFO - 25/07/18 15:56:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.42.07ab92e8-c87a-4e66-b661-2b60b1f0f3d3.tmp to file:/tmp/checkpoints/checkins/commits/42
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:02.805Z",
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "batchId" : 42,
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.0864197530864197,
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 5.154639175257731,
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:03.001+0000] {subprocess.py:93} INFO -     "addBatch" : 159,
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "commitOffsets" : 12,
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "triggerExecution" : 194,
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "walCommit" : 15
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:56:03.002+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.003+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.0864197530864197,
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 5.154639175257731,
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:03.004+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:03.005+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/43 using temp file file:/tmp/checkpoints/checkins/offsets/.43.1ad55dcf-9b1a-4c52-81e2-14122d8469b6.tmp
[2025-07-18T15:56:03.014+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.43.1ad55dcf-9b1a-4c52-81e2-14122d8469b6.tmp to file:/tmp/checkpoints/checkins/offsets/43
[2025-07-18T15:56:03.014+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1752854163001,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:03.017+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.017+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.020+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.022+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.022+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.022+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.023+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.024+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.027+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.027+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.028+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.033+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
[2025-07-18T15:56:03.034+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 433.7 MiB)
[2025-07-18T15:56:03.034+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.034+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 108 from start at <unknown>:0
[2025-07-18T15:56:03.034+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 43, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:03.035+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Got job 54 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Final stage: ResultStage 54 (start at <unknown>:0)
[2025-07-18T15:56:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[219] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:03.036+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 28.0 KiB, free 433.6 MiB)
[2025-07-18T15:56:03.037+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.6 MiB)
[2025-07-18T15:56:03.037+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[219] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
[2025-07-18T15:56:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 54) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:56:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Running task 0.0 in stage 54.0 (TID 54)
[2025-07-18T15:56:03.039+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.039+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:56:03.040+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=90, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.041+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 51, attempt 0, stage 51.0)
[2025-07-18T15:56:03.042+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:03.043+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=89 untilOffset=90, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=43 taskId=54 partitionId=0
[2025-07-18T15:56:03.045+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 89 for partition checkins-0
[2025-07-18T15:56:03.046+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:56:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Committed partition 0 (task 51, attempt 0, stage 51.0)
[2025-07-18T15:56:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 503693792 nanos, during time span of 513486792 nanos.
[2025-07-18T15:56:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Finished task 0.0 in stage 51.0 (TID 51). 4685 bytes result sent to driver
[2025-07-18T15:56:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 526 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool
[2025-07-18T15:56:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: ResultStage 51 (start at <unknown>:0) finished in 0.531 s
[2025-07-18T15:56:03.051+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
[2025-07-18T15:56:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 51 finished: start at <unknown>:0, took 0.533044 s
[2025-07-18T15:56:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing epoch 37 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:56:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:56:03.085+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/40 using temp file file:/tmp/checkpoints/feedback/offsets/.40.9ff9a593-14d0-4765-908f-80eeacea3449.tmp
[2025-07-18T15:56:03.096+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.40.9ff9a593-14d0-4765-908f-80eeacea3449.tmp to file:/tmp/checkpoints/feedback/offsets/40
[2025-07-18T15:56:03.096+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1752854163081,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:03.099+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.100+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.100+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.100+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.101+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.104+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.104+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.104+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.105+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.105+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.108+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v39.metadata.json
[2025-07-18T15:56:03.109+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.109+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.109+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.110+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.110+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.117+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
[2025-07-18T15:56:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.6 MiB)
[2025-07-18T15:56:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.0 MiB)
[2025-07-18T15:56:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 110 from start at <unknown>:0
[2025-07-18T15:56:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:03.118+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:03.119+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Got job 55 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:03.119+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Final stage: ResultStage 55 (start at <unknown>:0)
[2025-07-18T15:56:03.119+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:03.119+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:03.119+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[223] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:03.121+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 27.5 KiB, free 433.5 MiB)
[2025-07-18T15:56:03.121+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.5 MiB)
[2025-07-18T15:56:03.121+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.0 MiB)
[2025-07-18T15:56:03.122+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:03.122+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[223] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:03.122+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
[2025-07-18T15:56:03.122+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 55) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:56:03.122+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Running task 0.0 in stage 55.0 (TID 55)
[2025-07-18T15:56:03.126+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:03.134+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=87 untilOffset=88, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=40 taskId=55 partitionId=0
[2025-07-18T15:56:03.134+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SnapshotProducer: Committed snapshot 3350474034303221435 (FastAppend)
[2025-07-18T15:56:03.135+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_100_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 87 for partition feedback-0
[2025-07-18T15:56:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:56:03.137+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_99_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.139+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_103_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.141+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_101_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.143+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_98_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.145+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_105_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.147+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_106_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:56:03.149+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:56:03.150+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Removed broadcast_107_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:56:03.151+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=3350474034303221435, sequenceNumber=38, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.094526375S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=38}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=90}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2913}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=114408}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:03.151+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committed in 95 ms
[2025-07-18T15:56:03.151+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 37, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:56:03.154+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/37 using temp file file:/tmp/checkpoints/reservations/commits/.37.fb6784a0-bd10-465d-92b9-b393ef2c56d0.tmp
[2025-07-18T15:56:03.163+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.37.fb6784a0-bd10-465d-92b9-b393ef2c56d0.tmp to file:/tmp/checkpoints/reservations/commits/37
[2025-07-18T15:56:03.163+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:03.163+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:02.473Z",
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "batchId" : 37,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.739336492890995,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.451378809869376,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -     "addBatch" : 652,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -     "commitOffsets" : 12,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:03.164+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -     "triggerExecution" : 689,
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -     "walCommit" : 15
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:03.165+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.739336492890995,
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.451378809869376,
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:03.166+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:03.167+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:03.167+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:56:03.167+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:03.167+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:03.167+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:03.283+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.283+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:56:03.284+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=89, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.285+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 55, attempt 0, stage 55.0)
[2025-07-18T15:56:03.298+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Committed partition 0 (task 55, attempt 0, stage 55.0)
[2025-07-18T15:56:03.299+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 149066000 nanos, during time span of 163312166 nanos.
[2025-07-18T15:56:03.299+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Finished task 0.0 in stage 55.0 (TID 55). 4818 bytes result sent to driver
[2025-07-18T15:56:03.301+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 55) in 178 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:03.302+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool
[2025-07-18T15:56:03.302+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: ResultStage 55 (start at <unknown>:0) finished in 0.181 s
[2025-07-18T15:56:03.303+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished
[2025-07-18T15:56:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 55 finished: start at <unknown>:0, took 0.182729 s
[2025-07-18T15:56:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:03.304+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing epoch 40 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:56:03.314+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.362+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v42.metadata.json
[2025-07-18T15:56:03.378+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SnapshotProducer: Committed snapshot 5122638944935815605 (FastAppend)
[2025-07-18T15:56:03.387+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=5122638944935815605, sequenceNumber=41, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.074143416S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=41}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=88}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2906}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=120525}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:03.387+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committed in 74 ms
[2025-07-18T15:56:03.388+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:56:03.395+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/40 using temp file file:/tmp/checkpoints/feedback/commits/.40.352fe09f-0f4e-488e-88ef-3037577d6409.tmp
[2025-07-18T15:56:03.417+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.40.352fe09f-0f4e-488e-88ef-3037577d6409.tmp to file:/tmp/checkpoints/feedback/commits/40
[2025-07-18T15:56:03.418+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:03.418+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:03.081Z",
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "batchId" : 40,
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.994011976047904,
[2025-07-18T15:56:03.419+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:03.420+0000] {subprocess.py:93} INFO -     "addBatch" : 286,
[2025-07-18T15:56:03.420+0000] {subprocess.py:93} INFO -     "commitOffsets" : 28,
[2025-07-18T15:56:03.420+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:03.420+0000] {subprocess.py:93} INFO -     "latestOffset" : 0,
[2025-07-18T15:56:03.421+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:56:03.421+0000] {subprocess.py:93} INFO -     "triggerExecution" : 334,
[2025-07-18T15:56:03.421+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T15:56:03.421+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:03.422+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:03.422+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:03.423+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:56:03.423+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:03.423+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:03.424+0000] {subprocess.py:93} INFO -         "0" : 87
[2025-07-18T15:56:03.426+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.427+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.427+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:03.428+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:03.429+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:03.429+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.430+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.430+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:03.431+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:03.431+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:03.432+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.432+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.433+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:03.433+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T15:56:03.434+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.994011976047904,
[2025-07-18T15:56:03.434+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:03.435+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:03.435+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:03.435+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:03.436+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:03.436+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:03.436+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:03.437+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:56:03.437+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:03.437+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:03.438+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:03.438+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/41 using temp file file:/tmp/checkpoints/feedback/offsets/.41.5602ae5e-c937-49a8-b25d-8390c9b2e424.tmp
[2025-07-18T15:56:03.448+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.41.5602ae5e-c937-49a8-b25d-8390c9b2e424.tmp to file:/tmp/checkpoints/feedback/offsets/41
[2025-07-18T15:56:03.449+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1752854163420,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:03.454+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.457+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.462+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.469+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.469+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.471+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.472+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.475+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:56:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:56:03.480+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:56:03.484+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 112 from start at <unknown>:0
[2025-07-18T15:56:03.484+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:03.485+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:03.485+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Got job 56 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:03.485+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Final stage: ResultStage 56 (start at <unknown>:0)
[2025-07-18T15:56:03.485+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[227] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 27.5 KiB, free 433.9 MiB)
[2025-07-18T15:56:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.9 MiB)
[2025-07-18T15:56:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:56:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[227] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0
[2025-07-18T15:56:03.488+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 56) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:56:03.488+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Running task 0.0 in stage 56.0 (TID 56)
[2025-07-18T15:56:03.491+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:03.491+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=88 untilOffset=89, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=41 taskId=56 partitionId=0
[2025-07-18T15:56:03.493+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 88 for partition feedback-0
[2025-07-18T15:56:03.494+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:56:03.495+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.495+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:56:03.496+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=90, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.497+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 56, attempt 0, stage 56.0)
[2025-07-18T15:56:03.507+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Committed partition 0 (task 56, attempt 0, stage 56.0)
[2025-07-18T15:56:03.507+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 2272584 nanos, during time span of 14031958 nanos.
[2025-07-18T15:56:03.508+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Finished task 0.0 in stage 56.0 (TID 56). 4763 bytes result sent to driver
[2025-07-18T15:56:03.508+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 56) in 24 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:03.508+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool
[2025-07-18T15:56:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: ResultStage 56 (start at <unknown>:0) finished in 0.029 s
[2025-07-18T15:56:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:03.509+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
[2025-07-18T15:56:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 56 finished: start at <unknown>:0, took 0.030334 s
[2025-07-18T15:56:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:03.510+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing epoch 41 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:56:03.516+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.548+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.548+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:56:03.549+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=90, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:03.550+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 54, attempt 0, stage 54.0)
[2025-07-18T15:56:03.566+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DataWritingSparkTask: Committed partition 0 (task 54, attempt 0, stage 54.0)
[2025-07-18T15:56:03.567+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 503123251 nanos, during time span of 519894042 nanos.
[2025-07-18T15:56:03.567+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Finished task 0.0 in stage 54.0 (TID 54). 4865 bytes result sent to driver
[2025-07-18T15:56:03.567+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 54) in 528 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool
[2025-07-18T15:56:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: ResultStage 54 (start at <unknown>:0) finished in 0.531 s
[2025-07-18T15:56:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:03.568+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
[2025-07-18T15:56:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Job 54 finished: start at <unknown>:0, took 0.531790 s
[2025-07-18T15:56:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:03.569+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing epoch 43 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:56:03.580+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:56:03.592+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v43.metadata.json
[2025-07-18T15:56:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SnapshotProducer: Committed snapshot 7638586033167241182 (FastAppend)
[2025-07-18T15:56:03.628+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7638586033167241182, sequenceNumber=42, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.112391916S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=42}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=89}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2801}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=123326}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:03.629+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committed in 112 ms
[2025-07-18T15:56:03.629+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:56:03.634+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/41 using temp file file:/tmp/checkpoints/feedback/commits/.41.cd19d553-ba88-4b09-a79d-c7947fb12f43.tmp
[2025-07-18T15:56:03.646+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v45.metadata.json
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.41.cd19d553-ba88-4b09-a79d-c7947fb12f43.tmp to file:/tmp/checkpoints/feedback/commits/41
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:03.416Z",
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO -   "batchId" : 41,
[2025-07-18T15:56:03.648+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.9850746268656714,
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 4.3478260869565215,
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -     "addBatch" : 171,
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -     "commitOffsets" : 18,
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -     "latestOffset" : 4,
[2025-07-18T15:56:03.649+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -     "triggerExecution" : 230,
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -     "walCommit" : 27
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -         "0" : 88
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.650+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:03.651+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.9850746268656714,
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 4.3478260869565215,
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:03.652+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:03.654+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/42 using temp file file:/tmp/checkpoints/feedback/offsets/.42.160fa0c3-3801-4295-be9f-83142d80ead6.tmp
[2025-07-18T15:56:03.666+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SnapshotProducer: Committed snapshot 679664857824603953 (FastAppend)
[2025-07-18T15:56:03.668+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.42.160fa0c3-3801-4295-be9f-83142d80ead6.tmp to file:/tmp/checkpoints/feedback/offsets/42
[2025-07-18T15:56:03.669+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Committed offsets for batch 42. Metadata OffsetSeqMetadata(0,1752854163649,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:56:03.674+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.674+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.678+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.681+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.684+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.687+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.687+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.690+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:03.691+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.692+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:56:03.695+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:56:03.696+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=679664857824603953, sequenceNumber=44, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.1174355S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=44}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=90}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2883}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=129348}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:03.696+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkWrite: Committed in 118 ms
[2025-07-18T15:56:03.701+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 114 from start at <unknown>:0
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 42, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Got job 57 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Final stage: ResultStage 57 (start at <unknown>:0)
[2025-07-18T15:56:03.702+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:56:03.703+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:56:03.703+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[231] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:56:03.703+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 27.5 KiB, free 433.8 MiB)
[2025-07-18T15:56:03.703+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.8 MiB)
[2025-07-18T15:56:03.703+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:56:03.705+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:56:03.708+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 57 (MapPartitionsRDD[231] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:56:03.709+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks resource profile 0
[2025-07-18T15:56:03.709+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/43 using temp file file:/tmp/checkpoints/checkins/commits/.43.32c37a82-c505-4cb7-8aa4-627e32632809.tmp
[2025-07-18T15:56:03.709+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 57) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:56:03.710+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO Executor: Running task 0.0 in stage 57.0 (TID 57)
[2025-07-18T15:56:03.712+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:56:03.712+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=89 untilOffset=90, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=42 taskId=57 partitionId=0
[2025-07-18T15:56:03.714+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 89 for partition feedback-0
[2025-07-18T15:56:03.716+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:56:03.726+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.43.32c37a82-c505-4cb7-8aa4-627e32632809.tmp to file:/tmp/checkpoints/checkins/commits/43
[2025-07-18T15:56:03.726+0000] {subprocess.py:93} INFO - 25/07/18 15:56:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:03.726+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:03.000Z",
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "batchId" : 43,
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 5.128205128205128,
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.3793103448275863,
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -     "addBatch" : 676,
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -     "commitOffsets" : 30,
[2025-07-18T15:56:03.727+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:03.728+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:56:03.728+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T15:56:03.728+0000] {subprocess.py:93} INFO -     "triggerExecution" : 725,
[2025-07-18T15:56:03.731+0000] {subprocess.py:93} INFO -     "walCommit" : 12
[2025-07-18T15:56:03.731+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.732+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.733+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:03.733+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:56:03.733+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:56:03.733+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:03.733+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:03.733+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:03.734+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 5.128205128205128,
[2025-07-18T15:56:03.734+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.3793103448275863,
[2025-07-18T15:56:03.734+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:03.734+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:03.734+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:03.735+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:03.735+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:03.736+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:03.737+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:03.737+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:56:03.737+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:03.737+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:03.738+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:04.216+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:04.219+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:56:04.219+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=90, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:56:04.219+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 57, attempt 0, stage 57.0)
[2025-07-18T15:56:04.234+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO DataWritingSparkTask: Committed partition 0 (task 57, attempt 0, stage 57.0)
[2025-07-18T15:56:04.234+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 503546417 nanos, during time span of 520420751 nanos.
[2025-07-18T15:56:04.234+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO Executor: Finished task 0.0 in stage 57.0 (TID 57). 4769 bytes result sent to driver
[2025-07-18T15:56:04.235+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 57) in 530 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:56:04.236+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool
[2025-07-18T15:56:04.238+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO DAGScheduler: ResultStage 57 (start at <unknown>:0) finished in 0.537 s
[2025-07-18T15:56:04.239+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:56:04.239+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished
[2025-07-18T15:56:04.239+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO DAGScheduler: Job 57 finished: start at <unknown>:0, took 0.537733 s
[2025-07-18T15:56:04.239+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:56:04.240+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO SparkWrite: Committing epoch 42 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:56:04.244+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:56:04.303+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v44.metadata.json
[2025-07-18T15:56:04.326+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO SnapshotProducer: Committed snapshot 2942117785726560671 (FastAppend)
[2025-07-18T15:56:04.339+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=2942117785726560671, sequenceNumber=43, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.094423333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=43}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=90}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3050}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=126376}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:56:04.340+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO SparkWrite: Committed in 94 ms
[2025-07-18T15:56:04.340+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 42, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:56:04.343+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/42 using temp file file:/tmp/checkpoints/feedback/commits/.42.ba9df8eb-8939-4b0b-8157-c3d4099ec626.tmp
[2025-07-18T15:56:04.359+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.42.ba9df8eb-8939-4b0b-8157-c3d4099ec626.tmp to file:/tmp/checkpoints/feedback/commits/42
[2025-07-18T15:56:04.359+0000] {subprocess.py:93} INFO - 25/07/18 15:56:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:56:04.359+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:56:04.359+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:56:04.359+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:56:04.361+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:56:03.647Z",
[2025-07-18T15:56:04.361+0000] {subprocess.py:93} INFO -   "batchId" : 42,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.329004329004329,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.4084507042253522,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -     "addBatch" : 663,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -     "commitOffsets" : 19,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -     "triggerExecution" : 710,
[2025-07-18T15:56:04.362+0000] {subprocess.py:93} INFO -     "walCommit" : 18
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -         "0" : 89
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:04.363+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:56:04.364+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:56:04.364+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:56:04.364+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:56:04.364+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:56:04.364+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:56:04.364+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.329004329004329,
[2025-07-18T15:56:04.365+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.4084507042253522,
[2025-07-18T15:56:04.365+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:56:04.365+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:56:04.366+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:56:13.173+0000] {subprocess.py:93} INFO - 25/07/18 15:56:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:13.730+0000] {subprocess.py:93} INFO - 25/07/18 15:56:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:14.358+0000] {subprocess.py:93} INFO - 25/07/18 15:56:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:15.699+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_109_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:56:15.701+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_108_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:56:15.703+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_112_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:56:15.705+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_111_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:56:15.707+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_113_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:56:15.709+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_114_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:56:15.711+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_110_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:56:15.712+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_102_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:56:15.713+0000] {subprocess.py:93} INFO - 25/07/18 15:56:15 INFO BlockManagerInfo: Removed broadcast_115_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:56:23.179+0000] {subprocess.py:93} INFO - 25/07/18 15:56:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:23.742+0000] {subprocess.py:93} INFO - 25/07/18 15:56:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:24.366+0000] {subprocess.py:93} INFO - 25/07/18 15:56:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:33.186+0000] {subprocess.py:93} INFO - 25/07/18 15:56:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:33.744+0000] {subprocess.py:93} INFO - 25/07/18 15:56:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:34.380+0000] {subprocess.py:93} INFO - 25/07/18 15:56:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:43.191+0000] {subprocess.py:93} INFO - 25/07/18 15:56:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:43.753+0000] {subprocess.py:93} INFO - 25/07/18 15:56:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:44.382+0000] {subprocess.py:93} INFO - 25/07/18 15:56:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:53.190+0000] {subprocess.py:93} INFO - 25/07/18 15:56:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:53.757+0000] {subprocess.py:93} INFO - 25/07/18 15:56:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:56:54.391+0000] {subprocess.py:93} INFO - 25/07/18 15:56:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:03.195+0000] {subprocess.py:93} INFO - 25/07/18 15:57:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:03.769+0000] {subprocess.py:93} INFO - 25/07/18 15:57:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:04.399+0000] {subprocess.py:93} INFO - 25/07/18 15:57:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:13.205+0000] {subprocess.py:93} INFO - 25/07/18 15:57:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:13.778+0000] {subprocess.py:93} INFO - 25/07/18 15:57:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:14.400+0000] {subprocess.py:93} INFO - 25/07/18 15:57:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:23.215+0000] {subprocess.py:93} INFO - 25/07/18 15:57:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:23.782+0000] {subprocess.py:93} INFO - 25/07/18 15:57:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:24.407+0000] {subprocess.py:93} INFO - 25/07/18 15:57:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:33.217+0000] {subprocess.py:93} INFO - 25/07/18 15:57:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:33.788+0000] {subprocess.py:93} INFO - 25/07/18 15:57:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:34.414+0000] {subprocess.py:93} INFO - 25/07/18 15:57:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:43.229+0000] {subprocess.py:93} INFO - 25/07/18 15:57:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:43.792+0000] {subprocess.py:93} INFO - 25/07/18 15:57:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:44.421+0000] {subprocess.py:93} INFO - 25/07/18 15:57:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:53.233+0000] {subprocess.py:93} INFO - 25/07/18 15:57:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:53.793+0000] {subprocess.py:93} INFO - 25/07/18 15:57:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:57:54.435+0000] {subprocess.py:93} INFO - 25/07/18 15:57:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:01.915+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/38 using temp file file:/tmp/checkpoints/reservations/offsets/.38.b2b6b4a5-a6d1-448b-8a56-667d425facbd.tmp
[2025-07-18T15:58:01.935+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.38.b2b6b4a5-a6d1-448b-8a56-667d425facbd.tmp to file:/tmp/checkpoints/reservations/offsets/38
[2025-07-18T15:58:01.936+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO MicroBatchExecution: Committed offsets for batch 38. Metadata OffsetSeqMetadata(0,1752854281903,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:58:01.969+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:01.970+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:01.970+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:01.984+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:01.987+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:01.992+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:01.993+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:01.994+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:01.995+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:01.995+0000] {subprocess.py:93} INFO - 25/07/18 15:58:01 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.002+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.003+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.003+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.004+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.004+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.021+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T15:58:02.022+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T15:58:02.023+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:58:02.024+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Created broadcast 116 from start at <unknown>:0
[2025-07-18T15:58:02.024+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:58:02.025+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:58:02.026+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Got job 58 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:58:02.026+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Final stage: ResultStage 58 (start at <unknown>:0)
[2025-07-18T15:58:02.027+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:58:02.027+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:58:02.028+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[235] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:58:02.029+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T15:58:02.030+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T15:58:02.031+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T15:58:02.031+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:58:02.031+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[235] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:58:02.034+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0
[2025-07-18T15:58:02.035+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 58) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:58:02.036+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO Executor: Running task 0.0 in stage 58.0 (TID 58)
[2025-07-18T15:58:02.042+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:58:02.045+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=90 untilOffset=91, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=38 taskId=58 partitionId=0
[2025-07-18T15:58:02.046+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 90 for partition reservations-0
[2025-07-18T15:58:02.052+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:58:02.107+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:02.108+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:58:02.108+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=92, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:02.109+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 58, attempt 0, stage 58.0)
[2025-07-18T15:58:02.131+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DataWritingSparkTask: Committed partition 0 (task 58, attempt 0, stage 58.0)
[2025-07-18T15:58:02.132+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 1 records), taking 58675208 nanos, during time span of 85575792 nanos.
[2025-07-18T15:58:02.133+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO Executor: Finished task 0.0 in stage 58.0 (TID 58). 4703 bytes result sent to driver
[2025-07-18T15:58:02.134+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 58) in 102 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:58:02.135+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool
[2025-07-18T15:58:02.135+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: ResultStage 58 (start at <unknown>:0) finished in 0.108 s
[2025-07-18T15:58:02.135+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:58:02.136+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
[2025-07-18T15:58:02.136+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Job 58 finished: start at <unknown>:0, took 0.111367 s
[2025-07-18T15:58:02.136+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:58:02.136+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Committing epoch 38 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:58:02.144+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.225+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v40.metadata.json
[2025-07-18T15:58:02.267+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SnapshotProducer: Committed snapshot 2736952352444654766 (FastAppend)
[2025-07-18T15:58:02.325+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=2736952352444654766, sequenceNumber=39, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.174558625S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=39}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=91}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2997}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=117405}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:58:02.326+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Committed in 176 ms
[2025-07-18T15:58:02.327+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 38, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:58:02.337+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/38 using temp file file:/tmp/checkpoints/reservations/commits/.38.580c428f-7108-486e-b535-c97d24cd0a12.tmp
[2025-07-18T15:58:02.388+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.38.580c428f-7108-486e-b535-c97d24cd0a12.tmp to file:/tmp/checkpoints/reservations/commits/38
[2025-07-18T15:58:02.390+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:58:02.391+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:58:02.391+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:58:02.391+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:58:02.392+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:58:01.902Z",
[2025-07-18T15:58:02.392+0000] {subprocess.py:93} INFO -   "batchId" : 38,
[2025-07-18T15:58:02.392+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:58:02.393+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:58:02.393+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.066115702479339,
[2025-07-18T15:58:02.393+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:58:02.394+0000] {subprocess.py:93} INFO -     "addBatch" : 333,
[2025-07-18T15:58:02.394+0000] {subprocess.py:93} INFO -     "commitOffsets" : 65,
[2025-07-18T15:58:02.395+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:58:02.395+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:58:02.396+0000] {subprocess.py:93} INFO -     "queryPlanning" : 50,
[2025-07-18T15:58:02.396+0000] {subprocess.py:93} INFO -     "triggerExecution" : 484,
[2025-07-18T15:58:02.397+0000] {subprocess.py:93} INFO -     "walCommit" : 32
[2025-07-18T15:58:02.398+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:58:02.399+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:58:02.401+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:58:02.402+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:58:02.402+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:58:02.402+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:58:02.403+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:58:02.403+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:02.404+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:02.404+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:58:02.404+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:58:02.405+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:02.405+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:02.405+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:02.406+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:58:02.406+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:58:02.406+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:02.407+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:02.407+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:02.407+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:58:02.407+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:58:02.408+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.066115702479339,
[2025-07-18T15:58:02.408+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:58:02.408+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:58:02.408+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:58:02.409+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:58:02.409+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:58:02.410+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:58:02.410+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:58:02.411+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:58:02.411+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:58:02.411+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:58:02.412+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:58:02.412+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/39 using temp file file:/tmp/checkpoints/reservations/offsets/.39.70350b3e-2237-4736-9798-8ceb92cca216.tmp
[2025-07-18T15:58:02.434+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.39.70350b3e-2237-4736-9798-8ceb92cca216.tmp to file:/tmp/checkpoints/reservations/offsets/39
[2025-07-18T15:58:02.435+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MicroBatchExecution: Committed offsets for batch 39. Metadata OffsetSeqMetadata(0,1752854282389,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:58:02.443+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.446+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.448+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.450+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.451+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.454+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.456+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.457+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.464+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.468+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.469+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.470+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.473+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:02.475+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.476+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.477+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:58:02.503+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:58:02.504+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:58:02.505+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Created broadcast 118 from start at <unknown>:0
[2025-07-18T15:58:02.505+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:58:02.507+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:58:02.508+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Got job 59 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:58:02.509+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Final stage: ResultStage 59 (start at <unknown>:0)
[2025-07-18T15:58:02.509+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:58:02.510+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:58:02.510+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[239] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:58:02.512+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T15:58:02.517+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T15:58:02.518+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:58:02.518+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:58:02.519+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[239] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:58:02.520+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0
[2025-07-18T15:58:02.522+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 59) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T15:58:02.522+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO Executor: Running task 0.0 in stage 59.0 (TID 59)
[2025-07-18T15:58:02.529+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/44 using temp file file:/tmp/checkpoints/checkins/offsets/.44.27f034b9-745a-4d63-b6a3-82901e625547.tmp
[2025-07-18T15:58:02.533+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:58:02.534+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=91 untilOffset=93, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=39 taskId=59 partitionId=0
[2025-07-18T15:58:02.536+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 91 for partition reservations-0
[2025-07-18T15:58:02.536+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:58:02.538+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:02.539+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:58:02.540+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:02.541+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 92 for partition reservations-0
[2025-07-18T15:58:02.542+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T15:58:02.553+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.44.27f034b9-745a-4d63-b6a3-82901e625547.tmp to file:/tmp/checkpoints/checkins/offsets/44
[2025-07-18T15:58:02.554+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1752854282519,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:58:02.557+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.558+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.559+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.560+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.560+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.566+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.567+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.570+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.571+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.572+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.573+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.574+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:02.580+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T15:58:02.583+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T15:58:02.584+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T15:58:02.584+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Created broadcast 120 from start at <unknown>:0
[2025-07-18T15:58:02.585+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 44, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:58:02.585+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:58:02.586+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Got job 60 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:58:02.586+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Final stage: ResultStage 60 (start at <unknown>:0)
[2025-07-18T15:58:02.587+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:58:02.587+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:58:02.588+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[243] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:58:02.588+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 28.0 KiB, free 433.9 MiB)
[2025-07-18T15:58:02.589+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T15:58:02.590+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:58:02.590+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:58:02.590+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[243] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:58:02.591+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
[2025-07-18T15:58:02.591+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 60) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:58:02.591+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO Executor: Running task 0.0 in stage 60.0 (TID 60)
[2025-07-18T15:58:02.597+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:58:02.601+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=90 untilOffset=91, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=44 taskId=60 partitionId=0
[2025-07-18T15:58:02.601+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 90 for partition checkins-0
[2025-07-18T15:58:02.602+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:58:02.712+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:02.713+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:58:02.713+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=92, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:02.714+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 60, attempt 0, stage 60.0)
[2025-07-18T15:58:02.724+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DataWritingSparkTask: Committed partition 0 (task 60, attempt 0, stage 60.0)
[2025-07-18T15:58:02.725+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 1 records), taking 114672458 nanos, during time span of 126982291 nanos.
[2025-07-18T15:58:02.725+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO Executor: Finished task 0.0 in stage 60.0 (TID 60). 4811 bytes result sent to driver
[2025-07-18T15:58:02.727+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 60) in 137 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:58:02.727+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool
[2025-07-18T15:58:02.728+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: ResultStage 60 (start at <unknown>:0) finished in 0.142 s
[2025-07-18T15:58:02.729+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:58:02.729+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
[2025-07-18T15:58:02.730+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO DAGScheduler: Job 60 finished: start at <unknown>:0, took 0.144209 s
[2025-07-18T15:58:02.730+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:58:02.730+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Committing epoch 44 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:58:02.741+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:02.816+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v46.metadata.json
[2025-07-18T15:58:02.869+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SnapshotProducer: Committed snapshot 1899787637430584131 (FastAppend)
[2025-07-18T15:58:02.898+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=1899787637430584131, sequenceNumber=45, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.153049792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=45}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=91}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2897}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=132245}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:58:02.903+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO SparkWrite: Committed in 153 ms
[2025-07-18T15:58:02.904+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:58:02.913+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/44 using temp file file:/tmp/checkpoints/checkins/commits/.44.1ec94c67-af42-44a8-91e1-1ee6b1f552b7.tmp
[2025-07-18T15:58:02.950+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.44.1ec94c67-af42-44a8-91e1-1ee6b1f552b7.tmp to file:/tmp/checkpoints/checkins/commits/44
[2025-07-18T15:58:02.951+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:58:02.953+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:58:02.954+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:58:02.954+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:58:02.954+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:58:02.516Z",
[2025-07-18T15:58:02.955+0000] {subprocess.py:93} INFO -   "batchId" : 44,
[2025-07-18T15:58:02.955+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:58:02.955+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T15:58:02.955+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.320185614849188,
[2025-07-18T15:58:02.958+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:58:02.958+0000] {subprocess.py:93} INFO -     "addBatch" : 332,
[2025-07-18T15:58:02.959+0000] {subprocess.py:93} INFO -     "commitOffsets" : 55,
[2025-07-18T15:58:02.959+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:58:02.960+0000] {subprocess.py:93} INFO -     "latestOffset" : 3,
[2025-07-18T15:58:02.960+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:58:02.961+0000] {subprocess.py:93} INFO -     "triggerExecution" : 431,
[2025-07-18T15:58:02.963+0000] {subprocess.py:93} INFO -     "walCommit" : 34
[2025-07-18T15:58:02.964+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:58:02.964+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:58:02.964+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:58:02.964+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:58:02.965+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:58:02.968+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:58:02.969+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:58:02.969+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:02.969+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:02.970+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:58:02.971+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:58:02.979+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:02.982+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:02.983+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:02.983+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:58:02.983+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:58:02.983+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:02.983+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 66.66666666666667,
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.320185614849188,
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:58:02.984+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:58:02.985+0000] {subprocess.py:93} INFO - 25/07/18 15:58:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/45 using temp file file:/tmp/checkpoints/checkins/offsets/.45.0421edff-6438-4b7c-a7da-ae72e6a34c0c.tmp
[2025-07-18T15:58:03.007+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.45.0421edff-6438-4b7c-a7da-ae72e6a34c0c.tmp to file:/tmp/checkpoints/checkins/offsets/45
[2025-07-18T15:58:03.008+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1752854282951,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:58:03.010+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.012+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.014+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.018+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.019+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.020+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.020+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.023+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.024+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.025+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.025+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.025+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.025+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.026+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.026+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.035+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T15:58:03.038+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T15:58:03.045+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:58:03.046+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 122 from start at <unknown>:0
[2025-07-18T15:58:03.047+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 45, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:58:03.049+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:58:03.050+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Got job 61 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:58:03.052+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Final stage: ResultStage 61 (start at <unknown>:0)
[2025-07-18T15:58:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:58:03.053+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:58:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[247] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:58:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T15:58:03.054+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T15:58:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.055+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:58:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[247] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:58:03.056+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0
[2025-07-18T15:58:03.057+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 61) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T15:58:03.057+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Running task 0.0 in stage 61.0 (TID 61)
[2025-07-18T15:58:03.058+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.059+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T15:58:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.060+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 59, attempt 0, stage 59.0)
[2025-07-18T15:58:03.062+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:58:03.063+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=91 untilOffset=93, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=45 taskId=61 partitionId=0
[2025-07-18T15:58:03.065+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 91 for partition checkins-0
[2025-07-18T15:58:03.066+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:58:03.067+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:58:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 92 for partition checkins-0
[2025-07-18T15:58:03.069+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T15:58:03.070+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Committed partition 0 (task 59, attempt 0, stage 59.0)
[2025-07-18T15:58:03.070+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 2 polls (polled  out 2 records), taking 513379875 nanos, during time span of 531640542 nanos.
[2025-07-18T15:58:03.070+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Finished task 0.0 in stage 59.0 (TID 59). 4701 bytes result sent to driver
[2025-07-18T15:58:03.070+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 59) in 550 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:58:03.071+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool
[2025-07-18T15:58:03.072+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: ResultStage 59 (start at <unknown>:0) finished in 0.563 s
[2025-07-18T15:58:03.072+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:58:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
[2025-07-18T15:58:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 59 finished: start at <unknown>:0, took 0.565477 s
[2025-07-18T15:58:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T15:58:03.073+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing epoch 39 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T15:58:03.080+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T15:58:03.122+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/43 using temp file file:/tmp/checkpoints/feedback/offsets/.43.2c186abc-6b9d-4fd3-ad6f-b42c1f904fd5.tmp
[2025-07-18T15:58:03.135+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.43.2c186abc-6b9d-4fd3-ad6f-b42c1f904fd5.tmp to file:/tmp/checkpoints/feedback/offsets/43
[2025-07-18T15:58:03.136+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Committed offsets for batch 43. Metadata OffsetSeqMetadata(0,1752854283118,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:58:03.139+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.140+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.141+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.141+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.142+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.144+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v41.metadata.json
[2025-07-18T15:58:03.146+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.146+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.146+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.150+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.151+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.155+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.155+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.156+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.156+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.158+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.162+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T15:58:03.167+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T15:58:03.167+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.175+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 124 from start at <unknown>:0
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 43, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Got job 62 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Final stage: ResultStage 62 (start at <unknown>:0)
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SnapshotProducer: Committed snapshot 5484973831843560108 (FastAppend)
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:58:03.176+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[251] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:58:03.177+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 27.5 KiB, free 433.7 MiB)
[2025-07-18T15:58:03.179+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.7 MiB)
[2025-07-18T15:58:03.180+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.180+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:58:03.181+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 62 (MapPartitionsRDD[251] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:58:03.181+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks resource profile 0
[2025-07-18T15:58:03.184+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 62) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:58:03.184+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Running task 0.0 in stage 62.0 (TID 62)
[2025-07-18T15:58:03.192+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:58:03.192+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=90 untilOffset=91, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=43 taskId=62 partitionId=0
[2025-07-18T15:58:03.193+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=5484973831843560108, sequenceNumber=40, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.10956175S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=40}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=93}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2997}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=120402}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:58:03.194+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committed in 110 ms
[2025-07-18T15:58:03.194+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 39, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T15:58:03.194+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 90 for partition feedback-0
[2025-07-18T15:58:03.197+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/39 using temp file file:/tmp/checkpoints/reservations/commits/.39.9a218b40-e3b7-4389-a313-4b0e7da93912.tmp
[2025-07-18T15:58:03.197+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:58:03.209+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.39.9a218b40-e3b7-4389-a313-4b0e7da93912.tmp to file:/tmp/checkpoints/reservations/commits/39
[2025-07-18T15:58:03.209+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:58:03.209+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T15:58:03.209+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:58:02.388Z",
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -   "batchId" : 39,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.11522633744856,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.4390243902439024,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -     "addBatch" : 749,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -     "commitOffsets" : 19,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:58:03.210+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -     "triggerExecution" : 820,
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -     "walCommit" : 44
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T15:58:03.211+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.11522633744856,
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.4390243902439024,
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:58:03.212+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:58:03.213+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:58:03.213+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T15:58:03.213+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:58:03.213+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:58:03.213+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:58:03.318+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.319+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:58:03.319+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=92, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.321+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 62, attempt 0, stage 62.0)
[2025-07-18T15:58:03.332+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Committed partition 0 (task 62, attempt 0, stage 62.0)
[2025-07-18T15:58:03.333+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 126808250 nanos, during time span of 140505042 nanos.
[2025-07-18T15:58:03.334+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Finished task 0.0 in stage 62.0 (TID 62). 4767 bytes result sent to driver
[2025-07-18T15:58:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 62) in 153 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:58:03.335+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2025-07-18T15:58:03.336+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: ResultStage 62 (start at <unknown>:0) finished in 0.165 s
[2025-07-18T15:58:03.336+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:58:03.336+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished
[2025-07-18T15:58:03.336+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 62 finished: start at <unknown>:0, took 0.166774 s
[2025-07-18T15:58:03.336+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:58:03.336+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing epoch 43 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:58:03.343+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.397+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v45.metadata.json
[2025-07-18T15:58:03.411+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SnapshotProducer: Committed snapshot 7477006411225590936 (FastAppend)
[2025-07-18T15:58:03.424+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7477006411225590936, sequenceNumber=44, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.078879333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=44}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=91}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2821}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=129197}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:58:03.424+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committed in 79 ms
[2025-07-18T15:58:03.424+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 43, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:58:03.427+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/43 using temp file file:/tmp/checkpoints/feedback/commits/.43.f1b841f3-b199-4f8e-9f8f-61b7d2902392.tmp
[2025-07-18T15:58:03.438+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.43.f1b841f3-b199-4f8e-9f8f-61b7d2902392.tmp to file:/tmp/checkpoints/feedback/commits/43
[2025-07-18T15:58:03.438+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:58:03.438+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:58:03.117Z",
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "batchId" : 43,
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 3.125,
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:58:03.439+0000] {subprocess.py:93} INFO -     "addBatch" : 280,
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -     "commitOffsets" : 15,
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -     "queryPlanning" : 8,
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -     "triggerExecution" : 320,
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -     "walCommit" : 16
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:58:03.440+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -         "0" : 90
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.441+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T15:58:03.442+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 3.125,
[2025-07-18T15:58:03.443+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:58:03.443+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:58:03.443+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:58:03.443+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:58:03.443+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:58:03.443+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:58:03.444+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:58:03.444+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:58:03.444+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:58:03.444+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:58:03.444+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:58:03.444+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/44 using temp file file:/tmp/checkpoints/feedback/offsets/.44.cbe7b127-9bab-43c8-9cf6-69700a0e3e2c.tmp
[2025-07-18T15:58:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.44.cbe7b127-9bab-43c8-9cf6-69700a0e3e2c.tmp to file:/tmp/checkpoints/feedback/offsets/44
[2025-07-18T15:58:03.455+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Committed offsets for batch 44. Metadata OffsetSeqMetadata(0,1752854283438,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:58:03.458+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.458+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.458+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.459+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.459+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.462+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.463+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.464+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.468+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.469+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.469+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.476+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
[2025-07-18T15:58:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.7 MiB)
[2025-07-18T15:58:03.477+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 126 from start at <unknown>:0
[2025-07-18T15:58:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 44, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:58:03.478+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:58:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Got job 63 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:58:03.479+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Final stage: ResultStage 63 (start at <unknown>:0)
[2025-07-18T15:58:03.480+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:58:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:58:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[255] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:58:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 27.5 KiB, free 433.6 MiB)
[2025-07-18T15:58:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.6 MiB)
[2025-07-18T15:58:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.481+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:58:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[255] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:58:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks resource profile 0
[2025-07-18T15:58:03.486+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 63) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:58:03.487+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Running task 0.0 in stage 63.0 (TID 63)
[2025-07-18T15:58:03.490+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:58:03.491+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=91 untilOffset=92, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=44 taskId=63 partitionId=0
[2025-07-18T15:58:03.493+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 91 for partition feedback-0
[2025-07-18T15:58:03.493+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:58:03.519+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.519+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:58:03.519+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.522+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 63, attempt 0, stage 63.0)
[2025-07-18T15:58:03.530+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Committed partition 0 (task 63, attempt 0, stage 63.0)
[2025-07-18T15:58:03.530+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 26491666 nanos, during time span of 37307916 nanos.
[2025-07-18T15:58:03.530+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Finished task 0.0 in stage 63.0 (TID 63). 4763 bytes result sent to driver
[2025-07-18T15:58:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 63) in 46 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:58:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool
[2025-07-18T15:58:03.531+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: ResultStage 63 (start at <unknown>:0) finished in 0.053 s
[2025-07-18T15:58:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:58:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
[2025-07-18T15:58:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 63 finished: start at <unknown>:0, took 0.053419 s
[2025-07-18T15:58:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:58:03.532+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing epoch 44 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:58:03.538+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.571+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.573+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T15:58:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:03.574+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 61, attempt 0, stage 61.0)
[2025-07-18T15:58:03.578+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DataWritingSparkTask: Committed partition 0 (task 61, attempt 0, stage 61.0)
[2025-07-18T15:58:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 2 records through 2 polls (polled  out 2 records), taking 511779042 nanos, during time span of 522112834 nanos.
[2025-07-18T15:58:03.579+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Finished task 0.0 in stage 61.0 (TID 61). 4825 bytes result sent to driver
[2025-07-18T15:58:03.581+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 61) in 534 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:58:03.582+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool
[2025-07-18T15:58:03.582+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: ResultStage 61 (start at <unknown>:0) finished in 0.541 s
[2025-07-18T15:58:03.582+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 61 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:58:03.582+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished
[2025-07-18T15:58:03.583+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Job 61 finished: start at <unknown>:0, took 0.542498 s
[2025-07-18T15:58:03.583+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T15:58:03.583+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing epoch 45 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T15:58:03.585+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v46.metadata.json
[2025-07-18T15:58:03.587+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T15:58:03.599+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SnapshotProducer: Committed snapshot 5072599096182629818 (FastAppend)
[2025-07-18T15:58:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=5072599096182629818, sequenceNumber=45, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.071309125S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=45}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=92}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2771}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=131968}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:58:03.609+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committed in 71 ms
[2025-07-18T15:58:03.610+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 44, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:58:03.613+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/44 using temp file file:/tmp/checkpoints/feedback/commits/.44.770e72a2-780d-4c43-9d5c-30589cf37dfa.tmp
[2025-07-18T15:58:03.627+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.44.770e72a2-780d-4c43-9d5c-30589cf37dfa.tmp to file:/tmp/checkpoints/feedback/commits/44
[2025-07-18T15:58:03.628+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_116_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.628+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:58:03.628+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:58:03.628+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:58:03.628+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:58:03.628+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:58:03.437Z",
[2025-07-18T15:58:03.629+0000] {subprocess.py:93} INFO -   "batchId" : 44,
[2025-07-18T15:58:03.629+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:58:03.629+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.125,
[2025-07-18T15:58:03.629+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 5.291005291005291,
[2025-07-18T15:58:03.629+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:58:03.629+0000] {subprocess.py:93} INFO -     "addBatch" : 148,
[2025-07-18T15:58:03.629+0000] {subprocess.py:93} INFO -     "commitOffsets" : 18,
[2025-07-18T15:58:03.630+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:58:03.630+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:58:03.631+0000] {subprocess.py:93} INFO -     "queryPlanning" : 5,
[2025-07-18T15:58:03.631+0000] {subprocess.py:93} INFO -     "triggerExecution" : 189,
[2025-07-18T15:58:03.631+0000] {subprocess.py:93} INFO -     "walCommit" : 15
[2025-07-18T15:58:03.631+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:58:03.632+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:58:03.632+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:58:03.632+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:58:03.633+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:58:03.633+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:03.633+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:03.633+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.633+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.633+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:58:03.634+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:03.634+0000] {subprocess.py:93} INFO -         "0" : 92
[2025-07-18T15:58:03.634+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.634+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.634+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:58:03.635+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:03.635+0000] {subprocess.py:93} INFO -         "0" : 92
[2025-07-18T15:58:03.635+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.635+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.635+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.125,
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 5.291005291005291,
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:58:03.636+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:58:03.637+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:58:03.637+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:58:03.637+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:58:03.637+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:58:03.637+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:58:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_117_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/45 using temp file file:/tmp/checkpoints/feedback/offsets/.45.f5f8d903-8774-4a99-9ba9-f2a518edce88.tmp
[2025-07-18T15:58:03.638+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T15:58:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:58:03.639+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:58:03.641+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T15:58:03.643+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T15:58:03.647+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v47.metadata.json
[2025-07-18T15:58:03.647+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_119_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T15:58:03.648+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.45.f5f8d903-8774-4a99-9ba9-f2a518edce88.tmp to file:/tmp/checkpoints/feedback/offsets/45
[2025-07-18T15:58:03.649+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Committed offsets for batch 45. Metadata OffsetSeqMetadata(0,1752854283628,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T15:58:03.649+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_120_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T15:58:03.651+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Removed broadcast_118_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:58:03.652+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.653+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.653+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.655+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.655+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.658+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.658+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.658+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.660+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.662+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.664+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.665+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.665+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:03.665+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.666+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T15:58:03.672+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T15:58:03.673+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T15:58:03.674+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:58:03.674+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 128 from start at <unknown>:0
[2025-07-18T15:58:03.674+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 45, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T15:58:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SnapshotProducer: Committed snapshot 8697927749318427108 (FastAppend)
[2025-07-18T15:58:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T15:58:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Got job 64 (start at <unknown>:0) with 1 output partitions
[2025-07-18T15:58:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Final stage: ResultStage 64 (start at <unknown>:0)
[2025-07-18T15:58:03.675+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T15:58:03.676+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T15:58:03.676+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[259] at start at <unknown>:0), which has no missing parents
[2025-07-18T15:58:03.678+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 27.5 KiB, free 434.1 MiB)
[2025-07-18T15:58:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.1 MiB)
[2025-07-18T15:58:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.2 MiB)
[2025-07-18T15:58:03.679+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1611
[2025-07-18T15:58:03.680+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[259] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T15:58:03.680+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0
[2025-07-18T15:58:03.682+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 64) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T15:58:03.682+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO Executor: Running task 0.0 in stage 64.0 (TID 64)
[2025-07-18T15:58:03.685+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T15:58:03.686+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=92 untilOffset=93, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=45 taskId=64 partitionId=0
[2025-07-18T15:58:03.687+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 92 for partition feedback-0
[2025-07-18T15:58:03.688+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T15:58:03.690+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=8697927749318427108, sequenceNumber=46, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.102789083S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=46}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=93}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2972}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=135217}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:58:03.691+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO SparkWrite: Committed in 103 ms
[2025-07-18T15:58:03.691+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T15:58:03.694+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/45 using temp file file:/tmp/checkpoints/checkins/commits/.45.1c4cd849-7e42-4557-a78f-9a71dff7948f.tmp
[2025-07-18T15:58:03.704+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.45.1c4cd849-7e42-4557-a78f-9a71dff7948f.tmp to file:/tmp/checkpoints/checkins/commits/45
[2025-07-18T15:58:03.704+0000] {subprocess.py:93} INFO - 25/07/18 15:58:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:58:03.704+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T15:58:03.704+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:58:02.949Z",
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -   "batchId" : 45,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 4.618937644341801,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.6525198938992043,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -     "addBatch" : 681,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -     "commitOffsets" : 13,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -     "triggerExecution" : 754,
[2025-07-18T15:58:03.705+0000] {subprocess.py:93} INFO -     "walCommit" : 51
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -         "0" : 91
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.706+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 4.618937644341801,
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.6525198938992043,
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T15:58:03.707+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T15:58:03.708+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:58:03.708+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:58:04.195+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:04.196+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T15:58:04.197+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=93, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T15:58:04.198+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 64, attempt 0, stage 64.0)
[2025-07-18T15:58:04.201+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO DataWritingSparkTask: Committed partition 0 (task 64, attempt 0, stage 64.0)
[2025-07-18T15:58:04.201+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 503209250 nanos, during time span of 513837250 nanos.
[2025-07-18T15:58:04.202+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO Executor: Finished task 0.0 in stage 64.0 (TID 64). 4771 bytes result sent to driver
[2025-07-18T15:58:04.203+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 64) in 522 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T15:58:04.204+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool
[2025-07-18T15:58:04.204+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO DAGScheduler: ResultStage 64 (start at <unknown>:0) finished in 0.526 s
[2025-07-18T15:58:04.204+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T15:58:04.204+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
[2025-07-18T15:58:04.206+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO DAGScheduler: Job 64 finished: start at <unknown>:0, took 0.528741 s
[2025-07-18T15:58:04.207+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T15:58:04.207+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO SparkWrite: Committing epoch 45 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T15:58:04.209+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T15:58:04.263+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v47.metadata.json
[2025-07-18T15:58:04.285+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO SnapshotProducer: Committed snapshot 357394500462329818 (FastAppend)
[2025-07-18T15:58:04.294+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=357394500462329818, sequenceNumber=46, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.083248708S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=46}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=93}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=134917}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T15:58:04.294+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO SparkWrite: Committed in 83 ms
[2025-07-18T15:58:04.295+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 45, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T15:58:04.297+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/45 using temp file file:/tmp/checkpoints/feedback/commits/.45.977a09f6-14ee-4d3b-89e6-2a10d34f435b.tmp
[2025-07-18T15:58:04.310+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.45.977a09f6-14ee-4d3b-89e6-2a10d34f435b.tmp to file:/tmp/checkpoints/feedback/commits/45
[2025-07-18T15:58:04.311+0000] {subprocess.py:93} INFO - 25/07/18 15:58:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T15:58:04.311+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T15:58:04.311+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T15:58:04.311+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T15:58:04.312+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T15:58:03.627Z",
[2025-07-18T15:58:04.312+0000] {subprocess.py:93} INFO -   "batchId" : 45,
[2025-07-18T15:58:04.312+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T15:58:04.313+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 5.2631578947368425,
[2025-07-18T15:58:04.314+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.4705882352941175,
[2025-07-18T15:58:04.315+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T15:58:04.315+0000] {subprocess.py:93} INFO -     "addBatch" : 637,
[2025-07-18T15:58:04.315+0000] {subprocess.py:93} INFO -     "commitOffsets" : 15,
[2025-07-18T15:58:04.315+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T15:58:04.315+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T15:58:04.315+0000] {subprocess.py:93} INFO -     "queryPlanning" : 7,
[2025-07-18T15:58:04.315+0000] {subprocess.py:93} INFO -     "triggerExecution" : 680,
[2025-07-18T15:58:04.316+0000] {subprocess.py:93} INFO -     "walCommit" : 20
[2025-07-18T15:58:04.316+0000] {subprocess.py:93} INFO -   },
[2025-07-18T15:58:04.316+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T15:58:04.316+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T15:58:04.316+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T15:58:04.316+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T15:58:04.316+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -         "0" : 92
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T15:58:04.317+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -       }
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     },
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 5.2631578947368425,
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.4705882352941175,
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     }
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T15:58:04.318+0000] {subprocess.py:93} INFO -   }
[2025-07-18T15:58:04.319+0000] {subprocess.py:93} INFO - }
[2025-07-18T15:58:13.221+0000] {subprocess.py:93} INFO - 25/07/18 15:58:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:13.715+0000] {subprocess.py:93} INFO - 25/07/18 15:58:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:14.313+0000] {subprocess.py:93} INFO - 25/07/18 15:58:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:20.499+0000] {subprocess.py:93} INFO - 25/07/18 15:58:20 INFO BlockManagerInfo: Removed broadcast_129_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T15:58:20.503+0000] {subprocess.py:93} INFO - 25/07/18 15:58:20 INFO BlockManagerInfo: Removed broadcast_128_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:58:20.506+0000] {subprocess.py:93} INFO - 25/07/18 15:58:20 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T15:58:23.233+0000] {subprocess.py:93} INFO - 25/07/18 15:58:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:23.717+0000] {subprocess.py:93} INFO - 25/07/18 15:58:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:24.318+0000] {subprocess.py:93} INFO - 25/07/18 15:58:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:33.246+0000] {subprocess.py:93} INFO - 25/07/18 15:58:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:33.727+0000] {subprocess.py:93} INFO - 25/07/18 15:58:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:34.329+0000] {subprocess.py:93} INFO - 25/07/18 15:58:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:43.251+0000] {subprocess.py:93} INFO - 25/07/18 15:58:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:43.731+0000] {subprocess.py:93} INFO - 25/07/18 15:58:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:44.335+0000] {subprocess.py:93} INFO - 25/07/18 15:58:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:53.259+0000] {subprocess.py:93} INFO - 25/07/18 15:58:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:53.743+0000] {subprocess.py:93} INFO - 25/07/18 15:58:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:58:54.339+0000] {subprocess.py:93} INFO - 25/07/18 15:58:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:03.269+0000] {subprocess.py:93} INFO - 25/07/18 15:59:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:03.744+0000] {subprocess.py:93} INFO - 25/07/18 15:59:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:04.351+0000] {subprocess.py:93} INFO - 25/07/18 15:59:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:13.272+0000] {subprocess.py:93} INFO - 25/07/18 15:59:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:13.748+0000] {subprocess.py:93} INFO - 25/07/18 15:59:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:14.354+0000] {subprocess.py:93} INFO - 25/07/18 15:59:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:23.284+0000] {subprocess.py:93} INFO - 25/07/18 15:59:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:23.760+0000] {subprocess.py:93} INFO - 25/07/18 15:59:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:24.354+0000] {subprocess.py:93} INFO - 25/07/18 15:59:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:33.296+0000] {subprocess.py:93} INFO - 25/07/18 15:59:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:33.767+0000] {subprocess.py:93} INFO - 25/07/18 15:59:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:34.436+0000] {subprocess.py:93} INFO - 25/07/18 15:59:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:43.302+0000] {subprocess.py:93} INFO - 25/07/18 15:59:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:43.770+0000] {subprocess.py:93} INFO - 25/07/18 15:59:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:44.432+0000] {subprocess.py:93} INFO - 25/07/18 15:59:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:53.312+0000] {subprocess.py:93} INFO - 25/07/18 15:59:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:53.777+0000] {subprocess.py:93} INFO - 25/07/18 15:59:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T15:59:54.444+0000] {subprocess.py:93} INFO - 25/07/18 15:59:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:02.058+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/40 using temp file file:/tmp/checkpoints/reservations/offsets/.40.24bfbe03-e75f-455b-a9c9-d907cf8cbf7a.tmp
[2025-07-18T16:00:02.099+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.40.24bfbe03-e75f-455b-a9c9-d907cf8cbf7a.tmp to file:/tmp/checkpoints/reservations/offsets/40
[2025-07-18T16:00:02.102+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MicroBatchExecution: Committed offsets for batch 40. Metadata OffsetSeqMetadata(0,1752854402041,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:00:02.160+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.161+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.162+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.171+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.175+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.184+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.185+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.186+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.187+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.187+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.194+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.196+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.197+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.202+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.210+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.269+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:00:02.274+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:00:02.276+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:00:02.280+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkContext: Created broadcast 130 from start at <unknown>:0
[2025-07-18T16:00:02.283+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:00:02.285+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:00:02.285+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Got job 65 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:00:02.286+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Final stage: ResultStage 65 (start at <unknown>:0)
[2025-07-18T16:00:02.287+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:00:02.288+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:00:02.291+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[263] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:00:02.291+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T16:00:02.292+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T16:00:02.293+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:00:02.293+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:00:02.295+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[263] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:00:02.296+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0
[2025-07-18T16:00:02.300+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 65) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T16:00:02.305+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO Executor: Running task 0.0 in stage 65.0 (TID 65)
[2025-07-18T16:00:02.324+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:00:02.353+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=93 untilOffset=94, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=40 taskId=65 partitionId=0
[2025-07-18T16:00:02.367+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 93 for partition reservations-0
[2025-07-18T16:00:02.370+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:00:02.434+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:02.436+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:00:02.438+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:02.451+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 65, attempt 0, stage 65.0)
[2025-07-18T16:00:02.506+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DataWritingSparkTask: Committed partition 0 (task 65, attempt 0, stage 65.0)
[2025-07-18T16:00:02.509+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 1 records through 1 polls (polled  out 2 records), taking 80402792 nanos, during time span of 141809125 nanos.
[2025-07-18T16:00:02.509+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO Executor: Finished task 0.0 in stage 65.0 (TID 65). 4738 bytes result sent to driver
[2025-07-18T16:00:02.511+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 65) in 210 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:00:02.514+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool
[2025-07-18T16:00:02.516+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: ResultStage 65 (start at <unknown>:0) finished in 0.228 s
[2025-07-18T16:00:02.518+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:00:02.519+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
[2025-07-18T16:00:02.530+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Job 65 finished: start at <unknown>:0, took 0.236349 s
[2025-07-18T16:00:02.533+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:00:02.536+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Committing epoch 40 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T16:00:02.566+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:02.689+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/46 using temp file file:/tmp/checkpoints/checkins/offsets/.46.8fe532ba-80e2-43e4-acfc-94f9fc822802.tmp
[2025-07-18T16:00:02.712+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v42.metadata.json
[2025-07-18T16:00:02.757+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.46.8fe532ba-80e2-43e4-acfc-94f9fc822802.tmp to file:/tmp/checkpoints/checkins/offsets/46
[2025-07-18T16:00:02.758+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1752854402650,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:00:02.758+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.758+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.758+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.759+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.760+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.817+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.818+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.819+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.821+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.822+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SnapshotProducer: Committed snapshot 1036223804101400437 (FastAppend)
[2025-07-18T16:00:02.824+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.845+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.846+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.846+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:02.850+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.851+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:02.867+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=1036223804101400437, sequenceNumber=41, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.301775001S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=41}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=94}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2948}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=123350}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:00:02.869+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Committed in 303 ms
[2025-07-18T16:00:02.870+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 40, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:00:02.871+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:00:02.871+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.1 MiB)
[2025-07-18T16:00:02.872+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:00:02.872+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkContext: Created broadcast 132 from start at <unknown>:0
[2025-07-18T16:00:02.873+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 46, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:00:02.873+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:00:02.886+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Got job 66 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:00:02.892+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Final stage: ResultStage 66 (start at <unknown>:0)
[2025-07-18T16:00:02.892+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:00:02.894+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:00:02.895+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[267] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:00:02.895+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/40 using temp file file:/tmp/checkpoints/reservations/commits/.40.be18b90c-2556-48f5-9930-beb8e0ab663e.tmp
[2025-07-18T16:00:02.896+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T16:00:02.896+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.0 MiB)
[2025-07-18T16:00:02.899+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:00:02.899+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:00:02.900+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[267] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:00:02.900+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks resource profile 0
[2025-07-18T16:00:02.900+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 66) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:00:02.900+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO Executor: Running task 0.0 in stage 66.0 (TID 66)
[2025-07-18T16:00:02.904+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:00:02.905+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=93 untilOffset=94, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=46 taskId=66 partitionId=0
[2025-07-18T16:00:02.909+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 93 for partition checkins-0
[2025-07-18T16:00:02.911+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:00:02.925+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.40.be18b90c-2556-48f5-9930-beb8e0ab663e.tmp to file:/tmp/checkpoints/reservations/commits/40
[2025-07-18T16:00:02.928+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:00:02.929+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T16:00:02.929+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T16:00:02.930+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:00:02.930+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:00:02.040Z",
[2025-07-18T16:00:02.930+0000] {subprocess.py:93} INFO -   "batchId" : 40,
[2025-07-18T16:00:02.931+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:00:02.931+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:00:02.932+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.1312217194570136,
[2025-07-18T16:00:02.932+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:00:02.932+0000] {subprocess.py:93} INFO -     "addBatch" : 683,
[2025-07-18T16:00:02.932+0000] {subprocess.py:93} INFO -     "commitOffsets" : 64,
[2025-07-18T16:00:02.932+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:00:02.932+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:00:02.932+0000] {subprocess.py:93} INFO -     "queryPlanning" : 78,
[2025-07-18T16:00:02.933+0000] {subprocess.py:93} INFO -     "triggerExecution" : 884,
[2025-07-18T16:00:02.933+0000] {subprocess.py:93} INFO -     "walCommit" : 55
[2025-07-18T16:00:02.935+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:00:02.936+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:00:02.937+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:00:02.937+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:00:02.938+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:00:02.938+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:00:02.939+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T16:00:02.939+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:02.940+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:02.940+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:00:02.940+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:00:02.942+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:02.944+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:02.944+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:02.945+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:00:02.945+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:00:02.946+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:02.946+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:02.946+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:02.947+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:00:02.947+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 76.92307692307692,
[2025-07-18T16:00:02.947+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.1312217194570136,
[2025-07-18T16:00:02.947+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:00:02.948+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:00:02.948+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:00:02.949+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:00:02.949+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:00:02.949+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:00:02.950+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:00:02.954+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:00:02.954+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:00:02.954+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:00:02.954+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:00:02.954+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/offsets/41 using temp file file:/tmp/checkpoints/reservations/offsets/.41.47a7b3d0-d645-4fff-af31-0d9ea8ec49d7.tmp
[2025-07-18T16:00:02.979+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/offsets/.41.47a7b3d0-d645-4fff-af31-0d9ea8ec49d7.tmp to file:/tmp/checkpoints/reservations/offsets/41
[2025-07-18T16:00:02.980+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO MicroBatchExecution: Committed offsets for batch 41. Metadata OffsetSeqMetadata(0,1752854402929,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:00:03.002+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.003+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.004+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.005+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.005+0000] {subprocess.py:93} INFO - 25/07/18 16:00:02 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.006+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.007+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.008+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.008+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.008+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.035+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.039+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.042+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.046+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.049+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.050+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:00:03.052+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T16:00:03.053+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:00:03.063+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 134 from start at <unknown>:0
[2025-07-18T16:00:03.067+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:00:03.070+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:00:03.072+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Got job 67 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:00:03.073+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Final stage: ResultStage 67 (start at <unknown>:0)
[2025-07-18T16:00:03.073+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:00:03.073+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:00:03.076+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting ResultStage 67 (MapPartitionsRDD[271] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:00:03.081+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 28.6 KiB, free 433.9 MiB)
[2025-07-18T16:00:03.082+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.084+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:00:03.086+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 433.9 MiB)
[2025-07-18T16:00:03.086+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on 77cb57a6bd53:38973 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:00:03.087+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:00:03.088+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 67 (MapPartitionsRDD[271] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:00:03.093+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks resource profile 0
[2025-07-18T16:00:03.094+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.094+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 67) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9938 bytes)
[2025-07-18T16:00:03.095+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 66, attempt 0, stage 66.0)
[2025-07-18T16:00:03.095+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Running task 0.0 in stage 67.0 (TID 67)
[2025-07-18T16:00:03.096+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:00:03.096+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=94 untilOffset=96, for query queryId=0314df7c-5598-4928-8d91-374ee67989d1 batchId=41 taskId=67 partitionId=0
[2025-07-18T16:00:03.096+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to offset 95 for partition reservations-0
[2025-07-18T16:00:03.096+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:00:03.097+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Committed partition 0 (task 66, attempt 0, stage 66.0)
[2025-07-18T16:00:03.097+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 1 records through 1 polls (polled  out 2 records), taking 156510083 nanos, during time span of 189536834 nanos.
[2025-07-18T16:00:03.098+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Finished task 0.0 in stage 66.0 (TID 66). 4859 bytes result sent to driver
[2025-07-18T16:00:03.104+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 66) in 207 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:00:03.105+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool
[2025-07-18T16:00:03.105+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: ResultStage 66 (start at <unknown>:0) finished in 0.226 s
[2025-07-18T16:00:03.106+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:00:03.106+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished
[2025-07-18T16:00:03.107+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 66 finished: start at <unknown>:0, took 0.229224 s
[2025-07-18T16:00:03.107+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:00:03.107+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing epoch 46 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T16:00:03.118+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.183+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v48.metadata.json
[2025-07-18T16:00:03.197+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SnapshotProducer: Committed snapshot 3841451463348852527 (FastAppend)
[2025-07-18T16:00:03.208+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=3841451463348852527, sequenceNumber=47, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.090536875S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=47}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=94}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2862}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=138079}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:00:03.209+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committed in 91 ms
[2025-07-18T16:00:03.209+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:00:03.212+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/46 using temp file file:/tmp/checkpoints/checkins/commits/.46.7d2b1dca-daa2-4f3d-a52a-ea74f499cf97.tmp
[2025-07-18T16:00:03.223+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.46.7d2b1dca-daa2-4f3d-a52a-ea74f499cf97.tmp to file:/tmp/checkpoints/checkins/commits/46
[2025-07-18T16:00:03.224+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:00:03.224+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T16:00:03.224+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T16:00:03.224+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:00:02.648Z",
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -   "batchId" : 46,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.7391304347826089,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -     "addBatch" : 449,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -     "commitOffsets" : 16,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:00:03.225+0000] {subprocess.py:93} INFO -     "queryPlanning" : 15,
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -     "triggerExecution" : 575,
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -     "walCommit" : 92
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:00:03.226+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:00:03.227+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T16:00:03.227+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.227+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.227+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:00:03.227+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:00:03.227+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:03.227+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 58.8235294117647,
[2025-07-18T16:00:03.228+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.7391304347826089,
[2025-07-18T16:00:03.229+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:00:03.229+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:00:03.229+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:00:03.229+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:00:03.229+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:00:03.229+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:00:03.229+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:00:03.230+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:00:03.230+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:00:03.230+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:00:03.230+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:00:03.230+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/offsets/47 using temp file file:/tmp/checkpoints/checkins/offsets/.47.3b9455c8-d171-4752-a5bd-442e2cff4360.tmp
[2025-07-18T16:00:03.240+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/offsets/.47.3b9455c8-d171-4752-a5bd-442e2cff4360.tmp to file:/tmp/checkpoints/checkins/offsets/47
[2025-07-18T16:00:03.240+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1752854403224,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:00:03.244+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.244+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.244+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.245+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.246+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.248+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.249+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.249+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.250+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.250+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.253+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.253+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.254+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.254+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.255+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.259+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/46 using temp file file:/tmp/checkpoints/feedback/offsets/.46.943bf0fc-3710-402f-95a7-87962527d59e.tmp
[2025-07-18T16:00:03.260+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T16:00:03.264+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 433.9 MiB)
[2025-07-18T16:00:03.264+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 77cb57a6bd53:38973 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:00:03.264+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 136 from start at <unknown>:0
[2025-07-18T16:00:03.264+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 47, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:00:03.264+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:00:03.267+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Got job 68 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:00:03.267+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Final stage: ResultStage 68 (start at <unknown>:0)
[2025-07-18T16:00:03.268+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:00:03.268+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:00:03.268+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[275] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:00:03.268+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 28.0 KiB, free 433.8 MiB)
[2025-07-18T16:00:03.272+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.8 MiB)
[2025-07-18T16:00:03.272+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 77cb57a6bd53:38973 (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.273+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:00:03.273+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[275] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:00:03.273+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0
[2025-07-18T16:00:03.274+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 68) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:00:03.274+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Running task 0.0 in stage 68.0 (TID 68)
[2025-07-18T16:00:03.274+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.46.943bf0fc-3710-402f-95a7-87962527d59e.tmp to file:/tmp/checkpoints/feedback/offsets/46
[2025-07-18T16:00:03.275+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Committed offsets for batch 46. Metadata OffsetSeqMetadata(0,1752854403253,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:00:03.277+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:00:03.278+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=94 untilOffset=96, for query queryId=a9a9b3af-d3c2-4704-81e0-4163831ae683 batchId=47 taskId=68 partitionId=0
[2025-07-18T16:00:03.278+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.279+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.279+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.279+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.280+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.281+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to offset 95 for partition checkins-0
[2025-07-18T16:00:03.282+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:00:03.283+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.284+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.284+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.284+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.285+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.287+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.287+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.287+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.288+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.288+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.293+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 32.0 KiB, free 433.8 MiB)
[2025-07-18T16:00:03.296+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.8 MiB)
[2025-07-18T16:00:03.296+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.296+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 138 from start at <unknown>:0
[2025-07-18T16:00:03.296+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 46, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:00:03.296+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:00:03.297+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Got job 69 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:00:03.297+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Final stage: ResultStage 69 (start at <unknown>:0)
[2025-07-18T16:00:03.297+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:00:03.297+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:00:03.297+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting ResultStage 69 (MapPartitionsRDD[279] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:00:03.298+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 27.5 KiB, free 433.7 MiB)
[2025-07-18T16:00:03.300+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.7 MiB)
[2025-07-18T16:00:03.300+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.301+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:00:03.301+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 69 (MapPartitionsRDD[279] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:00:03.301+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks resource profile 0
[2025-07-18T16:00:03.301+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 69) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:00:03.302+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Running task 0.0 in stage 69.0 (TID 69)
[2025-07-18T16:00:03.306+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:00:03.306+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=93 untilOffset=94, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=46 taskId=69 partitionId=0
[2025-07-18T16:00:03.308+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 93 for partition feedback-0
[2025-07-18T16:00:03.309+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:00:03.450+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.451+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:00:03.451+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=95, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.452+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 69, attempt 0, stage 69.0)
[2025-07-18T16:00:03.466+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Committed partition 0 (task 69, attempt 0, stage 69.0)
[2025-07-18T16:00:03.466+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 142426250 nanos, during time span of 157065083 nanos.
[2025-07-18T16:00:03.467+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Finished task 0.0 in stage 69.0 (TID 69). 4779 bytes result sent to driver
[2025-07-18T16:00:03.467+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 69) in 165 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:00:03.469+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool
[2025-07-18T16:00:03.469+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: ResultStage 69 (start at <unknown>:0) finished in 0.169 s
[2025-07-18T16:00:03.470+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:00:03.470+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 69: Stage finished
[2025-07-18T16:00:03.470+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 69 finished: start at <unknown>:0, took 0.170537 s
[2025-07-18T16:00:03.471+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:00:03.471+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing epoch 46 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T16:00:03.475+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.523+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v48.metadata.json
[2025-07-18T16:00:03.545+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SnapshotProducer: Committed snapshot 7981832774861571039 (FastAppend)
[2025-07-18T16:00:03.557+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=7981832774861571039, sequenceNumber=47, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.082745S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=47}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=94}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2996}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=137913}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:00:03.558+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committed in 83 ms
[2025-07-18T16:00:03.558+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:00:03.564+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/46 using temp file file:/tmp/checkpoints/feedback/commits/.46.4c8d0d0a-f072-49db-bcd2-16dcfe348d4f.tmp
[2025-07-18T16:00:03.572+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.46.4c8d0d0a-f072-49db-bcd2-16dcfe348d4f.tmp to file:/tmp/checkpoints/feedback/commits/46
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:00:03.252Z",
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "batchId" : 46,
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 3.134796238244514,
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:00:03.573+0000] {subprocess.py:93} INFO -     "addBatch" : 276,
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -     "commitOffsets" : 14,
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -     "queryPlanning" : 5,
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -     "triggerExecution" : 319,
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -     "walCommit" : 21
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:00:03.574+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -         "0" : 93
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.575+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.576+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:00:03.576+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:03.576+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:03.577+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.577+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.577+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:00:03.577+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:00:03.577+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 3.134796238244514,
[2025-07-18T16:00:03.577+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:00:03.577+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:00:03.578+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:00:03.579+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/47 using temp file file:/tmp/checkpoints/feedback/offsets/.47.84db8f13-5b9c-490e-b441-eaf96b077811.tmp
[2025-07-18T16:00:03.587+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.47.84db8f13-5b9c-490e-b441-eaf96b077811.tmp to file:/tmp/checkpoints/feedback/offsets/47
[2025-07-18T16:00:03.587+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Committed offsets for batch 47. Metadata OffsetSeqMetadata(0,1752854403573,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:00:03.590+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.590+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.590+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.591+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.592+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.592+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:00:03.592+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.593+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor-2, groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.593+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 67, attempt 0, stage 67.0)
[2025-07-18T16:00:03.595+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.596+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.596+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.596+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.597+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.600+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.601+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.601+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.602+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.604+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.604+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Committed partition 0 (task 67, attempt 0, stage 67.0)
[2025-07-18T16:00:03.604+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-2f99cd48-841f-44fa-87c7-be77b992ce19-858357054-executor read 2 records through 1 polls (polled  out 1 records), taking 504135959 nanos, during time span of 516574959 nanos.
[2025-07-18T16:00:03.606+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Finished task 0.0 in stage 67.0 (TID 67). 4699 bytes result sent to driver
[2025-07-18T16:00:03.606+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 67) in 541 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:00:03.606+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
[2025-07-18T16:00:03.607+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: ResultStage 67 (start at <unknown>:0) finished in 0.555 s
[2025-07-18T16:00:03.607+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:00:03.607+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 67: Stage finished
[2025-07-18T16:00:03.607+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 67 finished: start at <unknown>:0, took 0.558873 s
[2025-07-18T16:00:03.608+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:00:03.608+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing epoch 41 for query 0314df7c-5598-4928-8d91-374ee67989d1 in append mode
[2025-07-18T16:00:03.609+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_140 stored as values in memory (estimated size 32.0 KiB, free 433.7 MiB)
[2025-07-18T16:00:03.610+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.7 MiB)
[2025-07-18T16:00:03.610+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.610+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 140 from start at <unknown>:0
[2025-07-18T16:00:03.611+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 47, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:00:03.611+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:00:03.612+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Got job 70 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:00:03.613+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Final stage: ResultStage 70 (start at <unknown>:0)
[2025-07-18T16:00:03.613+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:00:03.613+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:00:03.614+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[283] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:00:03.614+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 27.5 KiB, free 433.6 MiB)
[2025-07-18T16:00:03.615+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.6 MiB)
[2025-07-18T16:00:03.615+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.615+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:00:03.615+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[283] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:00:03.615+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0
[2025-07-18T16:00:03.616+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:00:03.616+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 70) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:00:03.616+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Running task 0.0 in stage 70.0 (TID 70)
[2025-07-18T16:00:03.619+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:00:03.620+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=94 untilOffset=95, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=47 taskId=70 partitionId=0
[2025-07-18T16:00:03.621+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 94 for partition feedback-0
[2025-07-18T16:00:03.622+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:00:03.651+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.651+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:00:03.651+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.652+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 70, attempt 0, stage 70.0)
[2025-07-18T16:00:03.660+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Committed partition 0 (task 70, attempt 0, stage 70.0)
[2025-07-18T16:00:03.660+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 29681875 nanos, during time span of 38885375 nanos.
[2025-07-18T16:00:03.661+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Finished task 0.0 in stage 70.0 (TID 70). 4763 bytes result sent to driver
[2025-07-18T16:00:03.662+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 70) in 47 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:00:03.662+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool
[2025-07-18T16:00:03.663+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: ResultStage 70 (start at <unknown>:0) finished in 0.050 s
[2025-07-18T16:00:03.663+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:00:03.663+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished
[2025-07-18T16:00:03.663+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 70 finished: start at <unknown>:0, took 0.051444 s
[2025-07-18T16:00:03.663+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:00:03.663+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing epoch 47 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T16:00:03.665+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v43.metadata.json
[2025-07-18T16:00:03.669+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.681+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SnapshotProducer: Committed snapshot 2387776368187248385 (FastAppend)
[2025-07-18T16:00:03.691+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=2387776368187248385, sequenceNumber=42, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.075855917S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=42}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=96}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3053}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=126403}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:00:03.691+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committed in 76 ms
[2025-07-18T16:00:03.691+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 41, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:00:03.695+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/commits/41 using temp file file:/tmp/checkpoints/reservations/commits/.41.8f841c00-b3e3-48f3-90de-7db70d590416.tmp
[2025-07-18T16:00:03.706+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/commits/.41.8f841c00-b3e3-48f3-90de-7db70d590416.tmp to file:/tmp/checkpoints/reservations/commits/41
[2025-07-18T16:00:03.707+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "id" : "0314df7c-5598-4928-8d91-374ee67989d1",
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "runId" : "af558342-e931-459f-b082-cde32c42e687",
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:00:02.927Z",
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "batchId" : 41,
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 2.254791431792559,
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.570694087403599,
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:00:03.708+0000] {subprocess.py:93} INFO -     "addBatch" : 692,
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "commitOffsets" : 15,
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "latestOffset" : 2,
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "queryPlanning" : 20,
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "triggerExecution" : 778,
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "walCommit" : 49
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:03.709+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -         "0" : 96
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -         "0" : 96
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.710+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 2.254791431792559,
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.570694087403599,
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:00:03.711+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:00:03.712+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:00:03.718+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v49.metadata.json
[2025-07-18T16:00:03.734+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SnapshotProducer: Committed snapshot 6076880568710443885 (FastAppend)
[2025-07-18T16:00:03.743+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=6076880568710443885, sequenceNumber=48, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.074003541S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=48}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=95}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2878}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=140791}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:00:03.744+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committed in 74 ms
[2025-07-18T16:00:03.744+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:00:03.747+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/47 using temp file file:/tmp/checkpoints/feedback/commits/.47.2cc08de3-06ef-4893-bb13-d9cbb2f53992.tmp
[2025-07-18T16:00:03.758+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.47.2cc08de3-06ef-4893-bb13-d9cbb2f53992.tmp to file:/tmp/checkpoints/feedback/commits/47
[2025-07-18T16:00:03.758+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:00:03.758+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T16:00:03.758+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T16:00:03.758+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:00:03.572Z",
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -   "batchId" : 47,
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.125,
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 5.434782608695652,
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -     "addBatch" : 150,
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -     "commitOffsets" : 14,
[2025-07-18T16:00:03.759+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:00:03.760+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:00:03.760+0000] {subprocess.py:93} INFO -     "queryPlanning" : 5,
[2025-07-18T16:00:03.760+0000] {subprocess.py:93} INFO -     "triggerExecution" : 184,
[2025-07-18T16:00:03.760+0000] {subprocess.py:93} INFO -     "walCommit" : 13
[2025-07-18T16:00:03.760+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:00:03.760+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:00:03.760+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -         "0" : 95
[2025-07-18T16:00:03.761+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -         "0" : 95
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:00:03.762+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.125,
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 5.434782608695652,
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:00:03.763+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:00:03.764+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:00:03.764+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:00:03.764+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:00:03.764+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/offsets/48 using temp file file:/tmp/checkpoints/feedback/offsets/.48.d9713906-6d40-4842-ab06-97b9771205e4.tmp
[2025-07-18T16:00:03.774+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/offsets/.48.d9713906-6d40-4842-ab06-97b9771205e4.tmp to file:/tmp/checkpoints/feedback/offsets/48
[2025-07-18T16:00:03.774+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Committed offsets for batch 48. Metadata OffsetSeqMetadata(0,1752854403758,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:00:03.776+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.777+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.777+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.777+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.778+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.781+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.782+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.782+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.783+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.784+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.785+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.785+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:00:03.785+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor-1, groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:03.786+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 68, attempt 0, stage 68.0)
[2025-07-18T16:00:03.788+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.788+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.788+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:03.789+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.789+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:00:03.795+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DataWritingSparkTask: Committed partition 0 (task 68, attempt 0, stage 68.0)
[2025-07-18T16:00:03.796+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 32.0 KiB, free 433.6 MiB)
[2025-07-18T16:00:03.797+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-0a8c0cf8-ab6e-4551-83a6-6daa95020f09-603580605-executor read 2 records through 1 polls (polled  out 1 records), taking 503761167 nanos, during time span of 515371542 nanos.
[2025-07-18T16:00:03.798+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Finished task 0.0 in stage 68.0 (TID 68). 4835 bytes result sent to driver
[2025-07-18T16:00:03.798+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.6 MiB)
[2025-07-18T16:00:03.798+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on 77cb57a6bd53:38973 (size: 29.5 KiB, free: 434.0 MiB)
[2025-07-18T16:00:03.799+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 142 from start at <unknown>:0
[2025-07-18T16:00:03.799+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 48, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:00:03.799+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 68) in 524 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:00:03.799+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool
[2025-07-18T16:00:03.800+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: ResultStage 68 (start at <unknown>:0) finished in 0.530 s
[2025-07-18T16:00:03.800+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:00:03.802+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:00:03.802+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
[2025-07-18T16:00:03.802+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Got job 71 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:00:03.802+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Final stage: ResultStage 71 (start at <unknown>:0)
[2025-07-18T16:00:03.803+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:00:03.803+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:00:03.803+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Job 68 finished: start at <unknown>:0, took 0.533063 s
[2025-07-18T16:00:03.803+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting ResultStage 71 (MapPartitionsRDD[287] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:00:03.803+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:00:03.804+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing epoch 47 for query a9a9b3af-d3c2-4704-81e0-4163831ae683 in append mode
[2025-07-18T16:00:03.805+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_143 stored as values in memory (estimated size 27.5 KiB, free 433.5 MiB)
[2025-07-18T16:00:03.806+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.5 MiB)
[2025-07-18T16:00:03.807+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on 77cb57a6bd53:38973 (size: 12.1 KiB, free: 434.0 MiB)
[2025-07-18T16:00:03.807+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_139_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.0 MiB)
[2025-07-18T16:00:03.808+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:00:03.809+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 71 (MapPartitionsRDD[287] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:00:03.809+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks resource profile 0
[2025-07-18T16:00:03.809+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 71) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9935 bytes)
[2025-07-18T16:00:03.809+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO Executor: Running task 0.0 in stage 71.0 (TID 71)
[2025-07-18T16:00:03.810+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.812+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:00:03.813+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_133_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.813+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=95 untilOffset=96, for query queryId=d3aff090-24bc-4a1c-938f-fc839231598c batchId=48 taskId=71 partitionId=0
[2025-07-18T16:00:03.815+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_131_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.815+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to offset 95 for partition feedback-0
[2025-07-18T16:00:03.815+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:00:03.815+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:00:03.816+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_141_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.818+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 77cb57a6bd53:38973 in memory (size: 12.5 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.820+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_134_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.1 MiB)
[2025-07-18T16:00:03.822+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:00:03.824+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_140_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:00:03.825+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_137_piece0 on 77cb57a6bd53:38973 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:00:03.831+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:00:03.866+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v49.metadata.json
[2025-07-18T16:00:03.884+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SnapshotProducer: Committed snapshot 8915234208101638653 (FastAppend)
[2025-07-18T16:00:03.897+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=8915234208101638653, sequenceNumber=48, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.081932792S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=48}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=96}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2966}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=141045}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:00:03.897+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO SparkWrite: Committed in 82 ms
[2025-07-18T16:00:03.897+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 47, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:00:03.900+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/commits/47 using temp file file:/tmp/checkpoints/checkins/commits/.47.96bd9530-c84f-42b8-a79a-0f56813125fe.tmp
[2025-07-18T16:00:03.910+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/commits/.47.96bd9530-c84f-42b8-a79a-0f56813125fe.tmp to file:/tmp/checkpoints/checkins/commits/47
[2025-07-18T16:00:03.912+0000] {subprocess.py:93} INFO - 25/07/18 16:00:03 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:00:03.914+0000] {subprocess.py:93} INFO -   "id" : "a9a9b3af-d3c2-4704-81e0-4163831ae683",
[2025-07-18T16:00:03.914+0000] {subprocess.py:93} INFO -   "runId" : "01143f57-ab8a-4afe-9039-32fa7b4eca3f",
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:00:03.223Z",
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   "batchId" : 47,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 3.4782608695652177,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.9154518950437316,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -     "addBatch" : 651,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -     "commitOffsets" : 13,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -     "triggerExecution" : 686,
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T16:00:03.915+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -         "0" : 94
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -         "0" : 96
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -         "0" : 96
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:03.916+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 3.4782608695652177,
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.9154518950437316,
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:00:03.917+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:00:04.319+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:04.319+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:00:04.319+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor-3, groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:00:04.321+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 71, attempt 0, stage 71.0)
[2025-07-18T16:00:04.329+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO DataWritingSparkTask: Committed partition 0 (task 71, attempt 0, stage 71.0)
[2025-07-18T16:00:04.331+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-b7010a1f-fbf8-4e24-adb7-22f3f5df8016-2028037020-executor read 1 records through 1 polls (polled  out 1 records), taking 502964250 nanos, during time span of 512675417 nanos.
[2025-07-18T16:00:04.331+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO Executor: Finished task 0.0 in stage 71.0 (TID 71). 4773 bytes result sent to driver
[2025-07-18T16:00:04.332+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 71) in 520 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:00:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool
[2025-07-18T16:00:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO DAGScheduler: ResultStage 71 (start at <unknown>:0) finished in 0.530 s
[2025-07-18T16:00:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:00:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 71: Stage finished
[2025-07-18T16:00:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO DAGScheduler: Job 71 finished: start at <unknown>:0, took 0.531601 s
[2025-07-18T16:00:04.333+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:00:04.334+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO SparkWrite: Committing epoch 48 for query d3aff090-24bc-4a1c-938f-fc839231598c in append mode
[2025-07-18T16:00:04.338+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:00:04.390+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v50.metadata.json
[2025-07-18T16:00:04.407+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO SnapshotProducer: Committed snapshot 3031837132239953958 (FastAppend)
[2025-07-18T16:00:04.419+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=3031837132239953958, sequenceNumber=49, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.084807584S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=49}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=96}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2926}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=143717}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752853328708, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:00:04.420+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO SparkWrite: Committed in 85 ms
[2025-07-18T16:00:04.420+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 48, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:00:04.426+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/commits/48 using temp file file:/tmp/checkpoints/feedback/commits/.48.a8d96b60-fb54-41cf-b014-81a15ef979f9.tmp
[2025-07-18T16:00:04.441+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/commits/.48.a8d96b60-fb54-41cf-b014-81a15ef979f9.tmp to file:/tmp/checkpoints/feedback/commits/48
[2025-07-18T16:00:04.442+0000] {subprocess.py:93} INFO - 25/07/18 16:00:04 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:00:04.443+0000] {subprocess.py:93} INFO -   "id" : "d3aff090-24bc-4a1c-938f-fc839231598c",
[2025-07-18T16:00:04.443+0000] {subprocess.py:93} INFO -   "runId" : "1d90f249-7c40-4a15-bd2f-2ed3427fbacd",
[2025-07-18T16:00:04.444+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:00:04.444+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:00:03.757Z",
[2025-07-18T16:00:04.444+0000] {subprocess.py:93} INFO -   "batchId" : 48,
[2025-07-18T16:00:04.444+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:00:04.444+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 5.405405405405405,
[2025-07-18T16:00:04.445+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.461988304093567,
[2025-07-18T16:00:04.445+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:00:04.445+0000] {subprocess.py:93} INFO -     "addBatch" : 641,
[2025-07-18T16:00:04.446+0000] {subprocess.py:93} INFO -     "commitOffsets" : 22,
[2025-07-18T16:00:04.446+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:00:04.446+0000] {subprocess.py:93} INFO -     "latestOffset" : 1,
[2025-07-18T16:00:04.447+0000] {subprocess.py:93} INFO -     "queryPlanning" : 6,
[2025-07-18T16:00:04.447+0000] {subprocess.py:93} INFO -     "triggerExecution" : 684,
[2025-07-18T16:00:04.447+0000] {subprocess.py:93} INFO -     "walCommit" : 14
[2025-07-18T16:00:04.447+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:00:04.448+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:00:04.448+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:00:04.448+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:00:04.448+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -         "0" : 95
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -         "0" : 96
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -         "0" : 96
[2025-07-18T16:00:04.449+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:00:04.450+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:00:04.450+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:00:04.450+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 5.405405405405405,
[2025-07-18T16:00:04.450+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.461988304093567,
[2025-07-18T16:00:04.450+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:00:04.451+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:00:04.451+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:00:04.451+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:00:04.451+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:00:04.451+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:00:04.451+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:00:04.452+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:00:04.452+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:00:04.452+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:00:04.452+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:00:13.711+0000] {subprocess.py:93} INFO - 25/07/18 16:00:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:13.921+0000] {subprocess.py:93} INFO - 25/07/18 16:00:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:14.445+0000] {subprocess.py:93} INFO - 25/07/18 16:00:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:20.810+0000] {subprocess.py:93} INFO - 25/07/18 16:00:20 INFO BlockManagerInfo: Removed broadcast_142_piece0 on 77cb57a6bd53:38973 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:00:20.814+0000] {subprocess.py:93} INFO - 25/07/18 16:00:20 INFO BlockManagerInfo: Removed broadcast_143_piece0 on 77cb57a6bd53:38973 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T16:00:20.817+0000] {subprocess.py:93} INFO - 25/07/18 16:00:20 INFO BlockManagerInfo: Removed broadcast_136_piece0 on 77cb57a6bd53:38973 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:00:23.713+0000] {subprocess.py:93} INFO - 25/07/18 16:00:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:23.929+0000] {subprocess.py:93} INFO - 25/07/18 16:00:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:24.456+0000] {subprocess.py:93} INFO - 25/07/18 16:00:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:33.720+0000] {subprocess.py:93} INFO - 25/07/18 16:00:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:33.930+0000] {subprocess.py:93} INFO - 25/07/18 16:00:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:34.460+0000] {subprocess.py:93} INFO - 25/07/18 16:00:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:43.732+0000] {subprocess.py:93} INFO - 25/07/18 16:00:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:43.931+0000] {subprocess.py:93} INFO - 25/07/18 16:00:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:44.470+0000] {subprocess.py:93} INFO - 25/07/18 16:00:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:53.736+0000] {subprocess.py:93} INFO - 25/07/18 16:00:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:53.942+0000] {subprocess.py:93} INFO - 25/07/18 16:00:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:00:54.473+0000] {subprocess.py:93} INFO - 25/07/18 16:00:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:01:03.757+0000] {subprocess.py:93} INFO - 25/07/18 16:01:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:01:03.947+0000] {subprocess.py:93} INFO - 25/07/18 16:01:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:01:04.476+0000] {subprocess.py:93} INFO - 25/07/18 16:01:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:01:05.062+0000] {subprocess.py:97} INFO - Command exited with return code 137
[2025-07-18T16:01:05.104+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 137.
[2025-07-18T16:01:05.112+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=restaurant_pipeline, task_id=stream_to_bronze, execution_date=20250718T154000, start_date=20250718T154205, end_date=20250718T160105
[2025-07-18T16:01:05.129+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 126 for task stream_to_bronze (Bash command failed. The command returned a non-zero exit code 137.; 618)
[2025-07-18T16:01:05.156+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-07-18T16:01:05.218+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
