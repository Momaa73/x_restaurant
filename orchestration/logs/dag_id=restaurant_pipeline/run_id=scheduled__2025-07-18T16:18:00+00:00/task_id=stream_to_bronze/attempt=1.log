[2025-07-18T16:20:07.618+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:18:00+00:00 [queued]>
[2025-07-18T16:20:07.642+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:18:00+00:00 [queued]>
[2025-07-18T16:20:07.642+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 1
[2025-07-18T16:20:07.658+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): stream_to_bronze> on 2025-07-18 16:18:00+00:00
[2025-07-18T16:20:07.667+0000] {standard_task_runner.py:57} INFO - Started process 1354 to run task
[2025-07-18T16:20:07.674+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'restaurant_pipeline', 'stream_to_bronze', 'scheduled__2025-07-18T16:18:00+00:00', '--job-id', '189', '--raw', '--subdir', 'DAGS_FOLDER/restaurant_pipeline.py', '--cfg-path', '/tmp/tmpze0hsw_7']
[2025-07-18T16:20:07.680+0000] {standard_task_runner.py:85} INFO - Job 189: Subtask stream_to_bronze
[2025-07-18T16:20:07.758+0000] {task_command.py:416} INFO - Running <TaskInstance: restaurant_pipeline.stream_to_bronze scheduled__2025-07-18T16:18:00+00:00 [running]> on host 9bcfb43e0ab7
[2025-07-18T16:20:07.919+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='moran' AIRFLOW_CTX_DAG_ID='restaurant_pipeline' AIRFLOW_CTX_TASK_ID='stream_to_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-07-18T16:18:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T16:18:00+00:00'
[2025-07-18T16:20:07.922+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-07-18T16:20:07.925+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', "docker exec -e AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-07-18T16:18:00+00:00' spark-iceberg spark-submit /home/iceberg/spark/stream_to_bronze.py"]
[2025-07-18T16:20:07.953+0000] {subprocess.py:86} INFO - Output:
[2025-07-18T16:20:15.700+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SparkContext: Running Spark version 3.5.6
[2025-07-18T16:20:15.703+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:20:15.703+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SparkContext: Java version 17.0.15
[2025-07-18T16:20:15.723+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO ResourceUtils: ==============================================================
[2025-07-18T16:20:15.723+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-18T16:20:15.724+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO ResourceUtils: ==============================================================
[2025-07-18T16:20:15.724+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SparkContext: Submitted application: StreamToBronze
[2025-07-18T16:20:15.737+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-18T16:20:15.742+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO ResourceProfile: Limiting resource is cpu
[2025-07-18T16:20:15.743+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-18T16:20:15.784+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SecurityManager: Changing view acls to: root,spark
[2025-07-18T16:20:15.785+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SecurityManager: Changing modify acls to: root,spark
[2025-07-18T16:20:15.785+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SecurityManager: Changing view acls groups to:
[2025-07-18T16:20:15.785+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SecurityManager: Changing modify acls groups to:
[2025-07-18T16:20:15.785+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
[2025-07-18T16:20:15.831+0000] {subprocess.py:93} INFO - 25/07/18 16:20:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-18T16:20:16.032+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Utils: Successfully started service 'sparkDriver' on port 41329.
[2025-07-18T16:20:16.055+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO SparkEnv: Registering MapOutputTracker
[2025-07-18T16:20:16.085+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-18T16:20:16.097+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-18T16:20:16.097+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-18T16:20:16.099+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-18T16:20:16.115+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f17543a5-59a9-44bf-9af3-a3513a2ebe5b
[2025-07-18T16:20:16.125+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-18T16:20:16.134+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-18T16:20:16.235+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-18T16:20:16.273+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-07-18T16:20:16.274+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2025-07-18T16:20:16.274+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
[2025-07-18T16:20:16.274+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.
[2025-07-18T16:20:16.279+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Utils: Successfully started service 'SparkUI' on port 4044.
[2025-07-18T16:20:16.349+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Executor: Starting executor ID driver on host 77cb57a6bd53
[2025-07-18T16:20:16.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Executor: OS info Linux, 6.10.14-linuxkit, aarch64
[2025-07-18T16:20:16.351+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Executor: Java version 17.0.15
[2025-07-18T16:20:16.353+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-18T16:20:16.354+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@15cf902e for default.
[2025-07-18T16:20:16.366+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39921.
[2025-07-18T16:20:16.367+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO NettyBlockTransferService: Server created on 77cb57a6bd53:39921
[2025-07-18T16:20:16.368+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-18T16:20:16.379+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 77cb57a6bd53, 39921, None)
[2025-07-18T16:20:16.383+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO BlockManagerMasterEndpoint: Registering block manager 77cb57a6bd53:39921 with 434.4 MiB RAM, BlockManagerId(driver, 77cb57a6bd53, 39921, None)
[2025-07-18T16:20:16.384+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 77cb57a6bd53, 39921, None)
[2025-07-18T16:20:16.385+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 77cb57a6bd53, 39921, None)
[2025-07-18T16:20:16.698+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-18T16:20:16.705+0000] {subprocess.py:93} INFO - 25/07/18 16:20:16 INFO SharedState: Warehouse path is 'file:/app/spark-warehouse'.
[2025-07-18T16:20:18.625+0000] {subprocess.py:93} INFO - 25/07/18 16:20:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-07-18T16:20:18.633+0000] {subprocess.py:93} INFO - 25/07/18 16:20:18 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-07-18T16:20:18.634+0000] {subprocess.py:93} INFO - 25/07/18 16:20:18 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-07-18T16:20:19.484+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:19.529+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2025-07-18T16:20:19.622+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00 resolved to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00.
[2025-07-18T16:20:19.622+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:20:19.699+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/metadata using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/.metadata.d74fef3a-05de-42d8-b03c-4149366821b4.tmp
[2025-07-18T16:20:19.808+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/.metadata.d74fef3a-05de-42d8-b03c-4149366821b4.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/metadata
[2025-07-18T16:20:19.860+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO MicroBatchExecution: Starting [id = e935316f-aa2a-44cf-869a-4315cd970f5e, runId = da834d14-1857-4e45-8d37-84093eeb2466]. Use file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00 to store the query checkpoint.
[2025-07-18T16:20:19.872+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@365739f1] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@22efc0d8]
[2025-07-18T16:20:19.932+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:20:19.939+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:20:19.940+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:20:19.941+0000] {subprocess.py:93} INFO - 25/07/18 16:20:19 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:20:20.064+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:20.073+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00 resolved to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00.
[2025-07-18T16:20:20.074+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:20:20.088+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/metadata using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/.metadata.4216bc2e-2e82-4c91-b926-71e85a7fec5d.tmp
[2025-07-18T16:20:20.129+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/.metadata.4216bc2e-2e82-4c91-b926-71e85a7fec5d.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/metadata
[2025-07-18T16:20:20.155+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Starting [id = be2c6861-99cb-4fa3-9890-9efceeee772e, runId = 91ffa111-59ed-4773-9f51-8d852278d143]. Use file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00 to store the query checkpoint.
[2025-07-18T16:20:20.157+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@30b182fc] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3e767b42]
[2025-07-18T16:20:20.161+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:20:20.162+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:20:20.163+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:20:20.164+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:20:20.282+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:20:20.283+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:20:20.283+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:20:20.284+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:20:20.285+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:20:20.287+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:20:20.288+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:20:20.289+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:20:20.290+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:20:20.291+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:20:20.293+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:20:20.295+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:20:20.296+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:20:20.297+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:20:20.298+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:20:20.298+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:20:20.299+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:20:20.299+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:20:20.300+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:20:20.301+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:20:20.302+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:20:20.303+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:20:20.304+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:20:20.304+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:20:20.304+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:20:20.305+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:20:20.305+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:20:20.305+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:20:20.305+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:20:20.306+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:20:20.308+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:20:20.309+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:20:20.310+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:20:20.311+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:20:20.311+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:20:20.312+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:20:20.312+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:20:20.314+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:20:20.318+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:20:20.319+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:20:20.319+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:20:20.320+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:20:20.321+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:20:20.323+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:20:20.324+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:20:20.324+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:20:20.325+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:20:20.325+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:20:20.325+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:20:20.326+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:20:20.326+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:20:20.327+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:20:20.328+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:20:20.329+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:20:20.329+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:20:20.330+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:20:20.330+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:20:20.330+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:20:20.331+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:20:20.332+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:20:20.332+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:20:20.333+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:20:20.333+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:20:20.335+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:20:20.338+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:20:20.339+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:20:20.339+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:20:20.339+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:20:20.340+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:20:20.340+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:20:20.340+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:20:20.340+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:20:20.341+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:20:20.341+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:20:20.341+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:20:20.342+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:20:20.342+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:20:20.342+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:20:20.343+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:20:20.343+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:20:20.343+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:20:20.343+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:20:20.345+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:20:20.345+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:20:20.345+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:20:20.346+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:20:20.346+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:20:20.347+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:20:20.347+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:20:20.348+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:20:20.348+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:20:20.349+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:20:20.350+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:20:20.352+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:20:20.353+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:20:20.355+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:20:20.356+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:20:20.356+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:20:20.356+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:20:20.357+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:20:20.357+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:20:20.358+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:20:20.359+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:20:20.359+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:20:20.361+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:20:20.362+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:20:20.362+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:20:20.363+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:20:20.363+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:20:20.364+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:20:20.365+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:20:20.365+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:20:20.366+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:20:20.366+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:20:20.366+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:20:20.367+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:20:20.367+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:20:20.367+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:20:20.368+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:20:20.368+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:20:20.368+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:20:20.369+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:20:20.369+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:20:20.370+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:20:20.371+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:20:20.372+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:20:20.373+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:20:20.373+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:20:20.374+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:20:20.374+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:20:20.375+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:20:20.376+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:20:20.376+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:20:20.378+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:20:20.378+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:20:20.379+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:20:20.379+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:20:20.379+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:20:20.380+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:20:20.380+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:20:20.380+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:20:20.381+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:20:20.382+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO BaseMetastoreCatalog: Table loaded by catalog: my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:20.383+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO ResolveWriteToStream: Checkpoint root /tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00 resolved to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00.
[2025-07-18T16:20:20.383+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-07-18T16:20:20.394+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/metadata using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/.metadata.52f33e78-6eee-48ea-b932-c70cc850dffc.tmp
[2025-07-18T16:20:20.394+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/.metadata.52f33e78-6eee-48ea-b932-c70cc850dffc.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/metadata
[2025-07-18T16:20:20.395+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Starting [id = a780aed6-ab09-4528-a3e0-eb09bfed40c5, runId = 7ef0bf89-4533-4669-b108-eeffb0902811]. Use file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00 to store the query checkpoint.
[2025-07-18T16:20:20.395+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7b7cb3ab] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@75bb536c]
[2025-07-18T16:20:20.395+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:20:20.395+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO OffsetSeqLog: BatchIds found from listing:
[2025-07-18T16:20:20.396+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Starting new streaming query.
[2025-07-18T16:20:20.396+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Stream started from {}
[2025-07-18T16:20:20.396+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:20:20.396+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:20:20.397+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:20:20.397+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:20:20.397+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:20:20.397+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:20:20.397+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:20:20.397+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:20:20.397+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:20:20.398+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:20:20.398+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:20:20.398+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:20:20.398+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:20:20.398+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:20:20.398+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:20:20.398+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:20:20.399+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:20:20.399+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:20:20.399+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:20:20.399+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:20:20.400+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:20:20.400+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:20:20.401+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:20:20.401+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:20:20.401+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:20:20.402+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:20:20.402+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:20:20.405+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:20:20.406+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:20:20.408+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:20:20.409+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:20:20.410+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:20:20.410+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:20:20.410+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:20:20.410+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:20:20.411+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:20:20.411+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:20:20.411+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:20:20.411+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:20:20.411+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:20:20.412+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:20:20.412+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:20:20.412+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:20:20.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:20:20.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:20:20.413+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:20:20.413+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:20:20.413+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:20:20.414+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:20:20.414+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:20:20.414+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:20:20.414+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:20:20.415+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:20:20.415+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:20:20.415+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:20:20.415+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:20:20.415+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:20:20.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:20:20.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:20:20.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:20:20.416+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:20:20.417+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:20:20.417+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:20:20.418+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:20:20.418+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:20:20.419+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:20:20.419+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:20:20.419+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:20:20.419+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:20:20.420+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:20:20.420+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:20:20.421+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:20:20.422+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:20:20.422+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:20:20.422+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:20:20.423+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:20:20.423+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka startTimeMs: 1752855620399
[2025-07-18T16:20:20.423+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:20:20.424+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:20:20.424+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka startTimeMs: 1752855620399
[2025-07-18T16:20:20.424+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:20:20.424+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:20:20.425+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO AppInfoParser: Kafka startTimeMs: 1752855620400
[2025-07-18T16:20:20.621+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/sources/0/.0.c090fa66-6192-4cc6-a002-710dc73b98fe.tmp
[2025-07-18T16:20:20.623+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/sources/0/.0.29be5e04-3778-458f-a69d-97a1940be319.tmp
[2025-07-18T16:20:20.625+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/sources/0/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/sources/0/.0.7294d1be-f61e-4b17-b808-980c6dc6b830.tmp
[2025-07-18T16:20:20.661+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/sources/0/.0.29be5e04-3778-458f-a69d-97a1940be319.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/sources/0/0
[2025-07-18T16:20:20.661+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/sources/0/.0.c090fa66-6192-4cc6-a002-710dc73b98fe.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/sources/0/0
[2025-07-18T16:20:20.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO KafkaMicroBatchStream: Initial offsets: {"reservations":{"0":0}}
[2025-07-18T16:20:20.662+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO KafkaMicroBatchStream: Initial offsets: {"feedback":{"0":0}}
[2025-07-18T16:20:20.663+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/sources/0/.0.7294d1be-f61e-4b17-b808-980c6dc6b830.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/sources/0/0
[2025-07-18T16:20:20.663+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO KafkaMicroBatchStream: Initial offsets: {"checkins":{"0":0}}
[2025-07-18T16:20:20.683+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/.0.b70586e0-82f7-4327-a3df-b4009c7aee0b.tmp
[2025-07-18T16:20:20.686+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/.0.384c60f6-1e52-41a1-863b-a8f016a81361.tmp
[2025-07-18T16:20:20.688+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/.0.51b3b26f-c7e2-43c2-b8af-6529ba6ed5d5.tmp
[2025-07-18T16:20:20.715+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/.0.b70586e0-82f7-4327-a3df-b4009c7aee0b.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/0
[2025-07-18T16:20:20.716+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/.0.384c60f6-1e52-41a1-863b-a8f016a81361.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/0
[2025-07-18T16:20:20.717+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855620669,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:20.718+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855620669,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:20.718+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/.0.51b3b26f-c7e2-43c2-b8af-6529ba6ed5d5.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/0
[2025-07-18T16:20:20.719+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1752855620669,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:20:20.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:20.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:20.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:20.918+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:20.919+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:20.919+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:20.920+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:20.920+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:20.921+0000] {subprocess.py:93} INFO - 25/07/18 16:20:20 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:21.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.010+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.014+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.056+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.056+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.056+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.094+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:21.095+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:21.095+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:21.096+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:21.096+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:21.097+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:21.097+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:21.097+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:21.098+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:21.105+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.106+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.106+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.107+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.113+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.114+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.149+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:21.150+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:21.150+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:21.150+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:21.151+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:21.151+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:21.151+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:21.152+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:21.152+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:21.152+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.152+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.155+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.158+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.159+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.160+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:20:21.522+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO CodeGenerator: Code generated in 268.29875 ms
[2025-07-18T16:20:21.523+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO CodeGenerator: Code generated in 268.598167 ms
[2025-07-18T16:20:21.524+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO CodeGenerator: Code generated in 266.969458 ms
[2025-07-18T16:20:21.914+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 434.4 MiB)
[2025-07-18T16:20:21.924+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:20:21.936+0000] {subprocess.py:93} INFO - 25/07/18 16:20:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.0 KiB, free 434.3 MiB)
[2025-07-18T16:20:22.015+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:22.016+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:22.016+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:22.019+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 77cb57a6bd53:39921 (size: 29.5 KiB, free: 434.4 MiB)
[2025-07-18T16:20:22.025+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 77cb57a6bd53:39921 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:22.027+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 77cb57a6bd53:39921 (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:22.029+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Created broadcast 1 from start at <unknown>:0
[2025-07-18T16:20:22.034+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Created broadcast 0 from start at <unknown>:0
[2025-07-18T16:20:22.035+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Created broadcast 2 from start at <unknown>:0
[2025-07-18T16:20:22.036+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:22.040+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:22.043+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:20:22.100+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:22.101+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:22.111+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:20:22.138+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Got job 1 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:22.141+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Final stage: ResultStage 0 (start at <unknown>:0)
[2025-07-18T16:20:22.142+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:22.143+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:22.151+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[11] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:22.324+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.5 KiB, free 434.2 MiB)
[2025-07-18T16:20:22.328+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 434.2 MiB)
[2025-07-18T16:20:22.333+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 77cb57a6bd53:39921 (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T16:20:22.334+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:22.364+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[11] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:22.370+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-18T16:20:22.398+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Got job 0 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:22.399+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Final stage: ResultStage 1 (start at <unknown>:0)
[2025-07-18T16:20:22.400+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:22.401+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:22.402+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:22.403+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.0 KiB, free 434.2 MiB)
[2025-07-18T16:20:22.406+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.1 MiB)
[2025-07-18T16:20:22.407+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 77cb57a6bd53:39921 (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:22.409+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:22.409+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:22.409+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-18T16:20:22.424+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9936 bytes)
[2025-07-18T16:20:22.430+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Got job 2 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:20:22.431+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Final stage: ResultStage 2 (start at <unknown>:0)
[2025-07-18T16:20:22.432+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:20:22.432+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:20:22.433+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:20:22.433+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:20:22.435+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T16:20:22.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T16:20:22.438+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 77cb57a6bd53:39921 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:22.442+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:20:22.443+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:20:22.443+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-18T16:20:22.444+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-18T16:20:22.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-18T16:20:22.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:20:22.445+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-18T16:20:22.564+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 25.497084 ms
[2025-07-18T16:20:22.565+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 25.554833 ms
[2025-07-18T16:20:22.565+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 26.338584 ms
[2025-07-18T16:20:22.581+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 16.692292 ms
[2025-07-18T16:20:22.582+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 18.29575 ms
[2025-07-18T16:20:22.585+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 20.354792 ms
[2025-07-18T16:20:22.648+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:22.648+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:22.648+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:20:22.837+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=0 untilOffset=132, for query queryId=be2c6861-99cb-4fa3-9890-9efceeee772e batchId=0 taskId=1 partitionId=0
[2025-07-18T16:20:22.838+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=0 untilOffset=132, for query queryId=a780aed6-ab09-4528-a3e0-eb09bfed40c5 batchId=0 taskId=0 partitionId=0
[2025-07-18T16:20:22.843+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=0 untilOffset=132, for query queryId=e935316f-aa2a-44cf-869a-4315cd970f5e batchId=0 taskId=2 partitionId=0
[2025-07-18T16:20:22.867+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 10.618083 ms
[2025-07-18T16:20:22.895+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO CodeGenerator: Code generated in 15.607167 ms
[2025-07-18T16:20:22.914+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:20:22.915+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:20:22.915+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:20:22.916+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:20:22.916+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:20:22.917+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:20:22.917+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:20:22.918+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:20:22.918+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1
[2025-07-18T16:20:22.918+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:20:22.918+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:20:22.918+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:20:22.919+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:20:22.919+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:20:22.919+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:20:22.919+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:20:22.919+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:20:22.920+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor
[2025-07-18T16:20:22.920+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:20:22.920+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:20:22.920+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:20:22.920+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:20:22.921+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:20:22.921+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:20:22.922+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:20:22.922+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:20:22.922+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:20:22.922+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:20:22.922+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:20:22.922+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:20:22.923+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:20:22.923+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:20:22.923+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:20:22.924+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:20:22.924+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:20:22.924+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:20:22.925+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:20:22.925+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:20:22.925+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:20:22.926+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:20:22.926+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:20:22.927+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:20:22.927+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:20:22.928+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:20:22.928+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:20:22.928+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:20:22.929+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:20:22.929+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:20:22.929+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:20:22.929+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:20:22.929+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:20:22.930+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:20:22.931+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:20:22.931+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:20:22.931+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:20:22.931+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:20:22.931+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:20:22.931+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:20:22.932+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:20:22.932+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:20:22.932+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:20:22.932+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:20:22.932+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:20:22.933+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:20:22.933+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:20:22.933+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:20:22.933+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:20:22.933+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:20:22.933+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:20:22.934+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:20:22.934+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:20:22.934+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:20:22.934+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:20:22.934+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:20:22.935+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:20:22.935+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:20:22.935+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:20:22.936+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:20:22.936+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:20:22.936+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:20:22.937+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:20:22.937+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:20:22.937+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:20:22.939+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:20:22.939+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:20:22.940+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:20:22.940+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:20:22.940+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:20:22.941+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:20:22.941+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:20:22.941+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:20:22.942+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:20:22.943+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:20:22.944+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:20:22.945+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2
[2025-07-18T16:20:22.945+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:20:22.946+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:20:22.946+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:20:22.946+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:20:22.947+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:20:22.947+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:20:22.947+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:20:22.948+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:20:22.948+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor
[2025-07-18T16:20:22.948+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:20:22.948+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:20:22.948+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:20:22.949+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:20:22.949+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:20:22.949+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:20:22.949+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:20:22.949+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:20:22.950+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:20:22.950+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:20:22.950+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:20:22.950+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:20:22.950+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:20:22.951+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:20:22.951+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:20:22.951+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:20:22.951+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:20:22.952+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:20:22.952+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:20:22.952+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:20:22.953+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:20:22.954+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:20:22.955+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:20:22.955+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:20:22.956+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:20:22.956+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:20:22.956+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:20:22.956+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:20:22.957+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:20:22.957+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:20:22.957+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:20:22.957+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:20:22.957+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:20:22.957+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:20:22.958+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:20:22.958+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:20:22.958+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:20:22.958+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:20:22.958+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:20:22.958+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:20:22.959+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:20:22.960+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:20:22.960+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:20:22.960+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:20:22.960+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:20:22.961+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:20:22.961+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:20:22.961+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:20:22.962+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:20:22.962+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:20:22.962+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:20:22.963+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:20:22.963+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:20:22.963+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:20:22.963+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:20:22.964+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:20:22.964+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:20:22.965+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:20:22.965+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:20:22.966+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:20:22.966+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:20:22.966+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:20:22.967+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:20:22.968+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:20:22.969+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:20:22.969+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:20:22.969+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:20:22.970+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:20:22.970+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO ConsumerConfig: ConsumerConfig values:
[2025-07-18T16:20:22.970+0000] {subprocess.py:93} INFO - 	allow.auto.create.topics = true
[2025-07-18T16:20:22.971+0000] {subprocess.py:93} INFO - 	auto.commit.interval.ms = 5000
[2025-07-18T16:20:22.972+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:20:22.972+0000] {subprocess.py:93} INFO - 	auto.offset.reset = none
[2025-07-18T16:20:22.972+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:20:22.972+0000] {subprocess.py:93} INFO - 	check.crcs = true
[2025-07-18T16:20:22.973+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:20:22.973+0000] {subprocess.py:93} INFO - 	client.id = consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3
[2025-07-18T16:20:22.973+0000] {subprocess.py:93} INFO - 	client.rack =
[2025-07-18T16:20:22.973+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 540000
[2025-07-18T16:20:22.973+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:20:22.973+0000] {subprocess.py:93} INFO - 	enable.auto.commit = false
[2025-07-18T16:20:22.973+0000] {subprocess.py:93} INFO - 	exclude.internal.topics = true
[2025-07-18T16:20:22.974+0000] {subprocess.py:93} INFO - 	fetch.max.bytes = 52428800
[2025-07-18T16:20:22.974+0000] {subprocess.py:93} INFO - 	fetch.max.wait.ms = 500
[2025-07-18T16:20:22.974+0000] {subprocess.py:93} INFO - 	fetch.min.bytes = 1
[2025-07-18T16:20:22.974+0000] {subprocess.py:93} INFO - 	group.id = spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor
[2025-07-18T16:20:22.974+0000] {subprocess.py:93} INFO - 	group.instance.id = null
[2025-07-18T16:20:22.974+0000] {subprocess.py:93} INFO - 	heartbeat.interval.ms = 3000
[2025-07-18T16:20:22.975+0000] {subprocess.py:93} INFO - 	interceptor.classes = []
[2025-07-18T16:20:22.975+0000] {subprocess.py:93} INFO - 	internal.leave.group.on.close = true
[2025-07-18T16:20:22.975+0000] {subprocess.py:93} INFO - 	internal.throw.on.fetch.stable.offset.unsupported = false
[2025-07-18T16:20:22.975+0000] {subprocess.py:93} INFO - 	isolation.level = read_uncommitted
[2025-07-18T16:20:22.975+0000] {subprocess.py:93} INFO - 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:20:22.976+0000] {subprocess.py:93} INFO - 	max.partition.fetch.bytes = 1048576
[2025-07-18T16:20:22.976+0000] {subprocess.py:93} INFO - 	max.poll.interval.ms = 300000
[2025-07-18T16:20:22.978+0000] {subprocess.py:93} INFO - 	max.poll.records = 500
[2025-07-18T16:20:22.980+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:20:22.980+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:20:22.980+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:20:22.981+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:20:22.981+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:20:22.981+0000] {subprocess.py:93} INFO - 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
[2025-07-18T16:20:22.982+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:20:22.983+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:20:22.983+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:20:22.984+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:20:22.984+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:20:22.985+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:20:22.986+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:20:22.986+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:20:22.986+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:20:22.986+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:20:22.986+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:20:22.987+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:20:22.988+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:20:22.988+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:20:22.988+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:20:22.988+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:20:22.988+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:20:22.988+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:20:22.988+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:20:22.989+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:20:22.989+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:20:22.989+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:20:22.990+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:20:22.990+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:20:22.990+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:20:22.990+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:20:22.991+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:20:22.991+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:20:22.991+0000] {subprocess.py:93} INFO - 	session.timeout.ms = 45000
[2025-07-18T16:20:22.994+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:20:22.994+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:20:22.995+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:20:22.995+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:20:22.995+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:20:22.995+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:20:22.996+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:20:22.996+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:20:22.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:20:22.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:20:22.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:20:22.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:20:22.996+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:20:22.997+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:20:22.997+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:20:22.997+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:20:22.997+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:20:22.997+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:20:22.997+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:20:22.998+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:20:22.998+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:20:22.998+0000] {subprocess.py:93} INFO - 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2025-07-18T16:20:22.998+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:20:22.998+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka startTimeMs: 1752855622952
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka startTimeMs: 1752855622952
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO AppInfoParser: Kafka startTimeMs: 1752855622952
[2025-07-18T16:20:22.999+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Assigned to partition(s): checkins-0
[2025-07-18T16:20:23.000+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Assigned to partition(s): feedback-0
[2025-07-18T16:20:23.000+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Assigned to partition(s): reservations-0
[2025-07-18T16:20:23.003+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Seeking to offset 0 for partition checkins-0
[2025-07-18T16:20:23.004+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Seeking to offset 0 for partition feedback-0
[2025-07-18T16:20:23.005+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Seeking to offset 0 for partition reservations-0
[2025-07-18T16:20:23.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:20:23.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:20:23.007+0000] {subprocess.py:93} INFO - 25/07/18 16:20:22 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Cluster ID: 9GRPd8zVSYOyO86EZGMDog
[2025-07-18T16:20:23.008+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:20:23.008+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:20:23.009+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:20:23.514+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:23.515+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:23.515+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:23.515+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:20:23.515+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:20:23.515+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:20:23.516+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:23.516+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:23.516+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=132, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:20:23.633+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:20:23.634+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:20:23.638+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:20:23.914+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
[2025-07-18T16:20:23.915+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
[2025-07-18T16:20:23.915+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
[2025-07-18T16:20:23.915+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor read 132 records through 1 polls (polled  out 132 records), taking 553694458 nanos, during time span of 956157626 nanos.
[2025-07-18T16:20:23.916+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor read 132 records through 1 polls (polled  out 132 records), taking 553806292 nanos, during time span of 956137418 nanos.
[2025-07-18T16:20:23.916+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor read 132 records through 1 polls (polled  out 132 records), taking 553698042 nanos, during time span of 955871959 nanos.
[2025-07-18T16:20:23.936+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4939 bytes result sent to driver
[2025-07-18T16:20:23.945+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 5027 bytes result sent to driver
[2025-07-18T16:20:23.946+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 4950 bytes result sent to driver
[2025-07-18T16:20:23.962+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1544 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:23.965+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-18T16:20:23.968+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1532 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:23.969+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-18T16:20:23.970+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1526 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:20:23.972+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-18T16:20:23.985+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: ResultStage 0 (start at <unknown>:0) finished in 1.803 s
[2025-07-18T16:20:23.986+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:23.987+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-18T16:20:23.991+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: ResultStage 1 (start at <unknown>:0) finished in 1.588 s
[2025-07-18T16:20:23.992+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: Job 1 finished: start at <unknown>:0, took 1.887000 s
[2025-07-18T16:20:23.997+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:23.998+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-18T16:20:24.000+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:24.002+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SparkWrite: Committing epoch 0 for query a780aed6-ab09-4528-a3e0-eb09bfed40c5 in append mode
[2025-07-18T16:20:24.002+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: ResultStage 2 (start at <unknown>:0) finished in 1.559 s
[2025-07-18T16:20:24.003+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:20:24.003+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-18T16:20:24.004+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: Job 0 finished: start at <unknown>:0, took 1.886491 s
[2025-07-18T16:20:24.004+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO DAGScheduler: Job 2 finished: start at <unknown>:0, took 1.882361 s
[2025-07-18T16:20:24.005+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:24.005+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SparkWrite: Committing epoch 0 for query e935316f-aa2a-44cf-869a-4315cd970f5e in append mode
[2025-07-18T16:20:24.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:20:24.006+0000] {subprocess.py:93} INFO - 25/07/18 16:20:23 INFO SparkWrite: Committing epoch 0 for query be2c6861-99cb-4fa3-9890-9efceeee772e in append mode
[2025-07-18T16:20:24.026+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:20:24.027+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:20:24.027+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:20:24.196+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 77cb57a6bd53:39921 in memory (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:20:24.201+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 77cb57a6bd53:39921 in memory (size: 12.1 KiB, free: 434.3 MiB)
[2025-07-18T16:20:24.206+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 77cb57a6bd53:39921 in memory (size: 12.3 KiB, free: 434.3 MiB)
[2025-07-18T16:20:24.368+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v93.metadata.json
[2025-07-18T16:20:24.369+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v84.metadata.json
[2025-07-18T16:20:24.369+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v93.metadata.json
[2025-07-18T16:20:24.436+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SnapshotProducer: Committed snapshot 6590017414113235267 (FastAppend)
[2025-07-18T16:20:24.436+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SnapshotProducer: Committed snapshot 4189633575074830873 (FastAppend)
[2025-07-18T16:20:24.437+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SnapshotProducer: Committed snapshot 520990198247094097 (FastAppend)
[2025-07-18T16:20:24.490+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=4189633575074830873, sequenceNumber=83, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.448238333S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=83}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=132}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=875}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=6016}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=266105}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:24.492+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SparkWrite: Committed in 458 ms
[2025-07-18T16:20:24.493+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:20:24.493+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=6590017414113235267, sequenceNumber=92, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.448100416S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=92}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=132}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=873}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=9240}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=303881}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:24.494+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SparkWrite: Committed in 459 ms
[2025-07-18T16:20:24.495+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=520990198247094097, sequenceNumber=92, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.45393675S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=92}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=132}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=876}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=7674}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=295861}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:20:24.495+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO SparkWrite: Committed in 459 ms
[2025-07-18T16:20:24.495+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:20:24.495+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:20:24.502+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/0 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/.0.e2214c36-70ba-4fb9-992b-f907194a8f7c.tmp
[2025-07-18T16:20:24.511+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/0 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/.0.c9458bd0-053a-4631-8986-e46ab36e7ef8.tmp
[2025-07-18T16:20:24.512+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/0 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/.0.53f5fda9-7b36-414b-8ee0-77a2e4c38214.tmp
[2025-07-18T16:20:24.546+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/.0.e2214c36-70ba-4fb9-992b-f907194a8f7c.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/0
[2025-07-18T16:20:24.547+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/.0.53f5fda9-7b36-414b-8ee0-77a2e4c38214.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/0
[2025-07-18T16:20:24.558+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/.0.c9458bd0-053a-4631-8986-e46ab36e7ef8.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/0
[2025-07-18T16:20:24.605+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:24.606+0000] {subprocess.py:93} INFO -   "id" : "a780aed6-ab09-4528-a3e0-eb09bfed40c5",
[2025-07-18T16:20:24.606+0000] {subprocess.py:93} INFO -   "runId" : "7ef0bf89-4533-4669-b108-eeffb0902811",
[2025-07-18T16:20:24.606+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:24.607+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:20.373Z",
[2025-07-18T16:20:24.607+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:20:24.608+0000] {subprocess.py:93} INFO -   "numInputRows" : 132,
[2025-07-18T16:20:24.608+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:20:24.608+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 31.639501438159158,
[2025-07-18T16:20:24.609+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:24.609+0000] {subprocess.py:93} INFO -     "addBatch" : 3411,
[2025-07-18T16:20:24.609+0000] {subprocess.py:93} INFO -     "commitOffsets" : 57,
[2025-07-18T16:20:24.610+0000] {subprocess.py:93} INFO -     "getBatch" : 11,
[2025-07-18T16:20:24.610+0000] {subprocess.py:93} INFO -     "latestOffset" : 293,
[2025-07-18T16:20:24.611+0000] {subprocess.py:93} INFO -     "queryPlanning" : 344,
[2025-07-18T16:20:24.611+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4172,
[2025-07-18T16:20:24.612+0000] {subprocess.py:93} INFO -     "walCommit" : 44
[2025-07-18T16:20:24.612+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:24.612+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:24.612+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:24.613+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:20:24.613+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:20:24.613+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:24.613+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:24.613+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:24.613+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -     "numInputRows" : 132,
[2025-07-18T16:20:24.614+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:20:24.615+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 31.639501438159158,
[2025-07-18T16:20:24.615+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:24.615+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:24.615+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:24.615+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:24.615+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:24.616+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:24.616+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:24.616+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:20:24.617+0000] {subprocess.py:93} INFO -     "numOutputRows" : 132
[2025-07-18T16:20:24.617+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:24.617+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:24.618+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:24.618+0000] {subprocess.py:93} INFO -   "id" : "be2c6861-99cb-4fa3-9890-9efceeee772e",
[2025-07-18T16:20:24.619+0000] {subprocess.py:93} INFO -   "runId" : "91ffa111-59ed-4773-9f51-8d852278d143",
[2025-07-18T16:20:24.619+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:24.619+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:20.158Z",
[2025-07-18T16:20:24.620+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:20:24.621+0000] {subprocess.py:93} INFO -   "numInputRows" : 132,
[2025-07-18T16:20:24.621+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:20:24.622+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 30.088899019831324,
[2025-07-18T16:20:24.623+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:24.623+0000] {subprocess.py:93} INFO -     "addBatch" : 3409,
[2025-07-18T16:20:24.624+0000] {subprocess.py:93} INFO -     "commitOffsets" : 56,
[2025-07-18T16:20:24.624+0000] {subprocess.py:93} INFO -     "getBatch" : 12,
[2025-07-18T16:20:24.625+0000] {subprocess.py:93} INFO -     "latestOffset" : 509,
[2025-07-18T16:20:24.625+0000] {subprocess.py:93} INFO -     "queryPlanning" : 344,
[2025-07-18T16:20:24.625+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4387,
[2025-07-18T16:20:24.626+0000] {subprocess.py:93} INFO -     "walCommit" : 42
[2025-07-18T16:20:24.626+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:24.627+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:24.627+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:24.628+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:20:24.630+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:20:24.630+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:24.631+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:24.631+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:24.632+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:24.632+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:24.632+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:24.632+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:20:24.632+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:24.633+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:24.633+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:24.633+0000] {subprocess.py:93} INFO -     "numInputRows" : 132,
[2025-07-18T16:20:24.633+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:20:24.633+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 30.088899019831324,
[2025-07-18T16:20:24.633+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -     "numOutputRows" : 132
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:24.634+0000] {subprocess.py:93} INFO - 25/07/18 16:20:24 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "id" : "e935316f-aa2a-44cf-869a-4315cd970f5e",
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "runId" : "da834d14-1857-4e45-8d37-84093eeb2466",
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:20:19.919Z",
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "batchId" : 0,
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "numInputRows" : 132,
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.0,
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 28.534370946822307,
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -     "addBatch" : 3411,
[2025-07-18T16:20:24.635+0000] {subprocess.py:93} INFO -     "commitOffsets" : 57,
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "getBatch" : 10,
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "latestOffset" : 730,
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "queryPlanning" : 344,
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "triggerExecution" : 4626,
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "walCommit" : 43
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "startOffset" : null,
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:20:24.636+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -     "numInputRows" : 132,
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.0,
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 28.534370946822307,
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:20:24.637+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -     "numOutputRows" : 132
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:20:24.638+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:20:34.601+0000] {subprocess.py:93} INFO - 25/07/18 16:20:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:34.603+0000] {subprocess.py:93} INFO - 25/07/18 16:20:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:34.603+0000] {subprocess.py:93} INFO - 25/07/18 16:20:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:44.613+0000] {subprocess.py:93} INFO - 25/07/18 16:20:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:44.616+0000] {subprocess.py:93} INFO - 25/07/18 16:20:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:44.617+0000] {subprocess.py:93} INFO - 25/07/18 16:20:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:54.605+0000] {subprocess.py:93} INFO - 25/07/18 16:20:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:54.616+0000] {subprocess.py:93} INFO - 25/07/18 16:20:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:20:54.617+0000] {subprocess.py:93} INFO - 25/07/18 16:20:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:04.608+0000] {subprocess.py:93} INFO - 25/07/18 16:21:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:04.627+0000] {subprocess.py:93} INFO - 25/07/18 16:21:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:04.628+0000] {subprocess.py:93} INFO - 25/07/18 16:21:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:14.614+0000] {subprocess.py:93} INFO - 25/07/18 16:21:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:14.636+0000] {subprocess.py:93} INFO - 25/07/18 16:21:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:14.638+0000] {subprocess.py:93} INFO - 25/07/18 16:21:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:24.618+0000] {subprocess.py:93} INFO - 25/07/18 16:21:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:24.646+0000] {subprocess.py:93} INFO - 25/07/18 16:21:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:24.646+0000] {subprocess.py:93} INFO - 25/07/18 16:21:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:34.620+0000] {subprocess.py:93} INFO - 25/07/18 16:21:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:34.650+0000] {subprocess.py:93} INFO - 25/07/18 16:21:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:34.657+0000] {subprocess.py:93} INFO - 25/07/18 16:21:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:44.622+0000] {subprocess.py:93} INFO - 25/07/18 16:21:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:44.657+0000] {subprocess.py:93} INFO - 25/07/18 16:21:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:44.658+0000] {subprocess.py:93} INFO - 25/07/18 16:21:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:54.629+0000] {subprocess.py:93} INFO - 25/07/18 16:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:54.666+0000] {subprocess.py:93} INFO - 25/07/18 16:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:21:54.666+0000] {subprocess.py:93} INFO - 25/07/18 16:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:02.930+0000] {subprocess.py:93} INFO - 25/07/18 16:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/.1.0d8feb88-a9f6-4d1b-bd7a-5053d07aa67c.tmp
[2025-07-18T16:22:03.007+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/.1.0d8feb88-a9f6-4d1b-bd7a-5053d07aa67c.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/1
[2025-07-18T16:22:03.008+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855722866,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.213+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.214+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.215+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.221+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.254+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.294+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.294+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.295+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.341+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.349+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.424+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.425+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.427+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:03.432+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.435+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.521+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.0 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.535+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.2 MiB)
[2025-07-18T16:22:03.538+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 77cb57a6bd53:39921 (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.538+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 6 from start at <unknown>:0
[2025-07-18T16:22:03.539+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:03.545+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:03.549+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/.1.53a4666f-4a43-418e-90ad-ae0164fea235.tmp
[2025-07-18T16:22:03.550+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Got job 3 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:03.551+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Final stage: ResultStage 3 (start at <unknown>:0)
[2025-07-18T16:22:03.551+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:03.551+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:03.552+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:03.564+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 28.6 KiB, free 434.1 MiB)
[2025-07-18T16:22:03.571+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:03.579+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 77cb57a6bd53:39921 (size: 12.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:03.580+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:03.584+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:03.585+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-07-18T16:22:03.602+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:22:03.637+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/.1.53a4666f-4a43-418e-90ad-ae0164fea235.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/1
[2025-07-18T16:22:03.638+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855723502,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:03.639+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[2025-07-18T16:22:03.709+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.711+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.712+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.722+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:03.726+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.727+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=132 untilOffset=133, for query queryId=e935316f-aa2a-44cf-869a-4315cd970f5e batchId=1 taskId=3 partitionId=0
[2025-07-18T16:22:03.734+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.753+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Seeking to offset 132 for partition reservations-0
[2025-07-18T16:22:03.772+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.777+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.779+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.781+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Seeking to earliest offset of partition reservations-0
[2025-07-18T16:22:03.818+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.821+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.890+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.895+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.896+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:03.932+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:03.932+0000] {subprocess.py:93} INFO - 25/07/18 16:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.032+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 32.0 KiB, free 434.1 MiB)
[2025-07-18T16:22:04.038+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.1 MiB)
[2025-07-18T16:22:04.038+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 77cb57a6bd53:39921 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:04.039+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 8 from start at <unknown>:0
[2025-07-18T16:22:04.039+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:04.039+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:04.039+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Got job 4 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:04.039+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Final stage: ResultStage 4 (start at <unknown>:0)
[2025-07-18T16:22:04.040+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:04.040+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:04.040+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:04.052+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:04.071+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 434.0 MiB)
[2025-07-18T16:22:04.073+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 77cb57a6bd53:39921 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:22:04.074+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:04.075+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:04.084+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-07-18T16:22:04.089+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:22:04.090+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-07-18T16:22:04.095+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:04.097+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=132 untilOffset=133, for query queryId=be2c6861-99cb-4fa3-9890-9efceeee772e batchId=1 taskId=4 partitionId=0
[2025-07-18T16:22:04.126+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Seeking to offset 132 for partition checkins-0
[2025-07-18T16:22:04.135+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/1 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/.1.d4c018ae-829d-4e58-a90a-82264082a012.tmp
[2025-07-18T16:22:04.148+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Seeking to earliest offset of partition checkins-0
[2025-07-18T16:22:04.232+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/.1.d4c018ae-829d-4e58-a90a-82264082a012.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/1
[2025-07-18T16:22:04.233+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1752855724095,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:04.270+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.271+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.273+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.280+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.280+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Seeking to latest offset of partition reservations-0
[2025-07-18T16:22:04.282+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor-2, groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor] Resetting offset for partition reservations-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.292+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:22:04.308+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.360+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.393+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.394+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.396+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.408+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.418+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.442+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
[2025-07-18T16:22:04.443+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor read 1 records through 1 polls (polled  out 3 records), taking 535227833 nanos, during time span of 695996917 nanos.
[2025-07-18T16:22:04.472+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4763 bytes result sent to driver
[2025-07-18T16:22:04.480+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.481+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.482+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:04.487+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.488+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:04.499+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 901 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.501+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.514+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:04.515+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 3 (start at <unknown>:0) finished in 0.961 s
[2025-07-18T16:22:04.516+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.517+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-07-18T16:22:04.644+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 3 finished: start at <unknown>:0, took 1.106741 s
[2025-07-18T16:22:04.648+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.648+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 1 for query e935316f-aa2a-44cf-869a-4315cd970f5e in append mode
[2025-07-18T16:22:04.660+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.662+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Seeking to latest offset of partition checkins-0
[2025-07-18T16:22:04.663+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor-1, groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor] Resetting offset for partition checkins-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:04.665+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:22:04.686+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:04.695+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 77cb57a6bd53:39921 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:04.719+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 77cb57a6bd53:39921 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:04.733+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 10 from start at <unknown>:0
[2025-07-18T16:22:04.733+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:04.734+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:04.738+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Got job 5 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:04.739+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Final stage: ResultStage 5 (start at <unknown>:0)
[2025-07-18T16:22:04.740+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:04.742+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:04.743+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:04.744+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 27.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:04.746+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.0 MiB)
[2025-07-18T16:22:04.780+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 77cb57a6bd53:39921 (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:04.790+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:04.792+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:04.796+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-07-18T16:22:04.798+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
[2025-07-18T16:22:04.799+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor read 1 records through 1 polls (polled  out 3 records), taking 533173833 nanos, during time span of 671796042 nanos.
[2025-07-18T16:22:04.819+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4864 bytes result sent to driver
[2025-07-18T16:22:04.879+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9936 bytes)
[2025-07-18T16:22:04.892+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 808 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:04.893+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-07-18T16:22:04.895+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: ResultStage 4 (start at <unknown>:0) finished in 0.839 s
[2025-07-18T16:22:04.899+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:04.900+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-07-18T16:22:04.901+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-07-18T16:22:04.902+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO DAGScheduler: Job 4 finished: start at <unknown>:0, took 0.849543 s
[2025-07-18T16:22:04.904+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:04.904+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing epoch 1 for query be2c6861-99cb-4fa3-9890-9efceeee772e in append mode
[2025-07-18T16:22:04.964+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:04.993+0000] {subprocess.py:93} INFO - 25/07/18 16:22:04 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:05.006+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=132 untilOffset=133, for query queryId=a780aed6-ab09-4528-a3e0-eb09bfed40c5 batchId=1 taskId=5 partitionId=0
[2025-07-18T16:22:05.057+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Seeking to offset 132 for partition feedback-0
[2025-07-18T16:22:05.058+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Seeking to earliest offset of partition feedback-0
[2025-07-18T16:22:05.072+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:05.609+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:05.612+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Seeking to latest offset of partition feedback-0
[2025-07-18T16:22:05.626+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor-3, groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor] Resetting offset for partition feedback-0 to position FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 1 rack: null)], epoch=0}}.
[2025-07-18T16:22:05.710+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:22:05.918+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
[2025-07-18T16:22:05.922+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor read 1 records through 1 polls (polled  out 3 records), taking 561462958 nanos, during time span of 865212917 nanos.
[2025-07-18T16:22:05.930+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4801 bytes result sent to driver
[2025-07-18T16:22:05.971+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1109 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:05.972+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-07-18T16:22:05.975+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 WARN Tasks: Retrying task after failure: Version 94 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.976+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 94 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v94.metadata.json
[2025-07-18T16:22:05.977+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:05.979+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:05.979+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:05.979+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:05.980+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:05.982+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:05.983+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:05.987+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:05.991+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:05.995+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:05.996+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:05.997+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:05.998+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.000+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.002+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.011+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.014+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.015+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.017+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.018+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.030+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.030+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.032+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.041+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.041+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.041+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.042+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.042+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.042+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.044+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.044+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.045+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.045+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.046+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.047+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.047+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.047+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.048+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.048+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.048+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.049+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: ResultStage 5 (start at <unknown>:0) finished in 1.234 s
[2025-07-18T16:22:06.050+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:06.052+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-07-18T16:22:06.052+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO DAGScheduler: Job 5 finished: start at <unknown>:0, took 1.255115 s
[2025-07-18T16:22:06.053+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:06.053+0000] {subprocess.py:93} INFO - 25/07/18 16:22:05 INFO SparkWrite: Committing epoch 1 for query a780aed6-ab09-4528-a3e0-eb09bfed40c5 in append mode
[2025-07-18T16:22:06.056+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Version 86 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json
[2025-07-18T16:22:06.057+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 86 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v86.metadata.json
[2025-07-18T16:22:06.057+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.058+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.059+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.065+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.066+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.066+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.069+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.069+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.072+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.074+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.177+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:06.550+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Version 87 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v87.metadata.json
[2025-07-18T16:22:06.551+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 87 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v87.metadata.json
[2025-07-18T16:22:06.552+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:06.553+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.553+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.553+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.554+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.554+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.554+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.556+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.558+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.564+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.571+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.574+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.575+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.577+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.578+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.579+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.581+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.585+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.585+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.586+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.587+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.590+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.597+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.598+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.604+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.617+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.617+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.618+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.619+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.619+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.619+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.625+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.625+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.629+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.630+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.632+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.636+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.637+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Version 95 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.638+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 95 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.639+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:06.641+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.645+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.647+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.652+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.654+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.655+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.658+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.659+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.663+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.673+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.673+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.674+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.674+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.677+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.678+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.679+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.680+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.681+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.682+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.684+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.688+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.691+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.695+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.697+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.697+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.697+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.698+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.698+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.699+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.703+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.705+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.706+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.707+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.707+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.708+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.709+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.710+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.711+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.712+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.712+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.713+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.714+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.715+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.716+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.716+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.717+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.718+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.718+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:06.719+0000] {subprocess.py:93} INFO - 25/07/18 16:22:06 WARN Tasks: Retrying task after failure: Version 95 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.719+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 95 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v95.metadata.json
[2025-07-18T16:22:06.720+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:06.720+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:06.720+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:06.720+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:06.721+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:06.721+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:06.721+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:06.722+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:06.722+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:06.725+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:06.726+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:06.728+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:06.729+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:06.729+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:06.729+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:06.730+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:06.730+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:06.731+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:06.732+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:06.733+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:06.734+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:06.735+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:06.738+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:06.739+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:06.740+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:06.742+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:06.747+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.755+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.756+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.757+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.759+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.762+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:06.766+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:06.768+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:06.771+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:06.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:06.805+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:06.808+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.809+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:06.810+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.812+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.813+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.814+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.815+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:06.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:06.818+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.820+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:06.821+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:06.822+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:06.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:06.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:06.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:06.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:06.825+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.826+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:06.827+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:06.828+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:06.828+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:06.828+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:06.829+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:07.398+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN Tasks: Retrying task after failure: Version 96 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.400+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 96 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.401+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:07.401+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:07.401+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:07.401+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:07.402+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:07.402+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:07.402+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:07.402+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:07.402+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:07.402+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:07.402+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:07.403+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:07.403+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:07.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:07.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:07.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:07.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:07.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:07.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:07.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:07.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:07.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:07.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:07.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:07.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:07.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:07.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:07.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:07.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:07.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:07.419+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:07.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:07.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:07.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:07.421+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:07.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:07.435+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:07.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:07.437+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN Tasks: Retrying task after failure: Version 96 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.437+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 96 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v96.metadata.json
[2025-07-18T16:22:07.437+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:07.437+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:07.438+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:07.438+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:07.438+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:07.438+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:07.438+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:07.438+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:07.439+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:07.439+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:07.439+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:07.439+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:07.440+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:07.440+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:07.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:07.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:07.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:07.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:07.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:07.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:07.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:07.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:07.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:07.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:07.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:07.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:07.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:07.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:07.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:07.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.462+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.463+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:07.464+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:07.465+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.465+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:07.465+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:07.497+0000] {subprocess.py:93} INFO - 25/07/18 16:22:07 WARN Tasks: Retrying task after failure: Version 88 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v88.metadata.json
[2025-07-18T16:22:07.498+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 88 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v88.metadata.json
[2025-07-18T16:22:07.498+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:07.499+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:07.500+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:07.500+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:07.501+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:07.501+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:07.503+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:07.504+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:07.505+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:07.505+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:07.506+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:07.506+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:07.506+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:07.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:07.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:07.506+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:07.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:07.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:07.507+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:07.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:07.510+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:07.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:07.511+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:07.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:07.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:07.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:07.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.512+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.513+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:07.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:07.514+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:07.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:07.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:07.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:07.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.517+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:07.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.519+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:07.520+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:07.520+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:07.521+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:07.523+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:07.526+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:07.528+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:07.529+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:07.531+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:07.532+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.534+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:07.535+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:08.163+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 77cb57a6bd53:39921 in memory (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.196+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 77cb57a6bd53:39921 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:22:08.323+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN Tasks: Retrying task after failure: Version 97 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v97.metadata.json
[2025-07-18T16:22:08.330+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 97 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v97.metadata.json
[2025-07-18T16:22:08.334+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:08.335+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:08.336+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:08.336+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:08.337+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:08.337+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:08.337+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:08.338+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:08.338+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:08.338+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:08.338+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:08.339+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:08.339+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:08.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:08.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:08.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:08.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:08.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:08.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:08.341+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:08.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:08.346+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:08.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:08.348+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:08.348+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:08.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:08.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.350+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.351+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.353+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.353+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:08.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:08.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:08.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.359+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.361+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.364+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.364+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.365+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.366+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.367+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:08.368+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:08.374+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:08.374+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:08.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:08.377+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:08.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:08.380+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:08.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:08.424+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN Tasks: Retrying task after failure: Version 97 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v97.metadata.json
[2025-07-18T16:22:08.425+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 97 already exists: s3a://warehouse/bronze/Checkins_raw/metadata/v97.metadata.json
[2025-07-18T16:22:08.427+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:08.428+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:08.431+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:08.432+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:08.432+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:08.432+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:08.432+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:08.432+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:08.432+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:08.433+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:08.433+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:08.433+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:08.433+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:08.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:08.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:08.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:08.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:08.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:08.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:08.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:08.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:08.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:08.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:08.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:08.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:08.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:08.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:08.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:08.438+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:08.439+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.440+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:08.446+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.447+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.448+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:08.452+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:08.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:08.453+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:08.454+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.456+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.457+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:08.458+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:08.459+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:08.460+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:08.548+0000] {subprocess.py:93} INFO - 25/07/18 16:22:08 WARN Tasks: Retrying task after failure: Version 89 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v89.metadata.json
[2025-07-18T16:22:08.552+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 89 already exists: s3a://warehouse/bronze/Reservations_raw/metadata/v89.metadata.json
[2025-07-18T16:22:08.553+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:08.554+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:08.554+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:08.555+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:08.556+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:08.556+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:08.556+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:08.556+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:08.556+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:08.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:08.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:08.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:08.557+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:08.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:08.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:08.558+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:08.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:08.559+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:08.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:08.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:08.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:08.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:08.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:08.562+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.563+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:08.564+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:08.565+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.566+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:08.567+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:08.568+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:08.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:08.569+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:09.586+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 WARN Tasks: Retrying task after failure: Version 99 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v99.metadata.json
[2025-07-18T16:22:09.587+0000] {subprocess.py:93} INFO - org.apache.iceberg.exceptions.CommitFailedException: Version 99 already exists: s3a://warehouse/bronze/Feedback_raw/metadata/v99.metadata.json
[2025-07-18T16:22:09.588+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.renameToFinal(HadoopTableOperations.java:365)
[2025-07-18T16:22:09.594+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:162)
[2025-07-18T16:22:09.595+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.lambda$commit$2(SnapshotProducer.java:400)
[2025-07-18T16:22:09.596+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)
[2025-07-18T16:22:09.597+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)
[2025-07-18T16:22:09.598+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)
[2025-07-18T16:22:09.598+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)
[2025-07-18T16:22:09.599+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.SnapshotProducer.commit(SnapshotProducer.java:374)
[2025-07-18T16:22:09.599+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.commitOperation(SparkWrite.java:233)
[2025-07-18T16:22:09.602+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite.access$1300(SparkWrite.java:84)
[2025-07-18T16:22:09.602+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:524)
[2025-07-18T16:22:09.603+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$StreamingAppend.doCommit(SparkWrite.java:568)
[2025-07-18T16:22:09.605+0000] {subprocess.py:93} INFO - 	at org.apache.iceberg.spark.source.SparkWrite$BaseStreamingWrite.commit(SparkWrite.java:516)
[2025-07-18T16:22:09.606+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite.commit(MicroBatchWrite.scala:39)
[2025-07-18T16:22:09.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:404)
[2025-07-18T16:22:09.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)
[2025-07-18T16:22:09.610+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)
[2025-07-18T16:22:09.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)
[2025-07-18T16:22:09.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[2025-07-18T16:22:09.611+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[2025-07-18T16:22:09.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[2025-07-18T16:22:09.612+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)
[2025-07-18T16:22:09.613+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-07-18T16:22:09.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)
[2025-07-18T16:22:09.614+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.615+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.616+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.618+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.618+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.619+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)
[2025-07-18T16:22:09.619+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)
[2025-07-18T16:22:09.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)
[2025-07-18T16:22:09.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-07-18T16:22:09.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-07-18T16:22:09.620+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-07-18T16:22:09.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-07-18T16:22:09.621+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.626+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.627+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.628+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
[2025-07-18T16:22:09.630+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
[2025-07-18T16:22:09.634+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:22:09.637+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:22:09.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:22:09.638+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:22:09.641+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:22:09.642+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:22:09.642+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:22:09.644+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.645+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:22:09.645+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:22:09.646+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:22:09.648+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:22:09.651+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:22:09.651+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:22:09.684+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 WARN S3AInstrumentation: Closing output stream statistics while data is still marked as pending upload in OutputStreamStatistics{counters=((stream_write_exceptions=0) (multipart_upload_completed.failures=0) (op_hsync=0) (op_hflush=0) (action_executor_acquired.failures=0) (object_multipart_aborted.failures=0) (op_abort=0) (stream_write_queue_duration=0) (stream_write_exceptions_completing_upload=0) (stream_write_total_time=0) (op_abort.failures=0) (stream_write_total_data=0) (multipart_upload_completed=0) (action_executor_acquired=0) (stream_write_bytes=8071) (stream_write_block_uploads=1) (object_multipart_aborted=0));
[2025-07-18T16:22:09.685+0000] {subprocess.py:93} INFO - gauges=((stream_write_block_uploads_pending=1) (stream_write_block_uploads_data_pending=8071));
[2025-07-18T16:22:09.686+0000] {subprocess.py:93} INFO - minimums=((multipart_upload_completed.min=-1) (object_multipart_aborted.min=-1) (action_executor_acquired.min=-1) (op_abort.failures.min=-1) (op_abort.min=-1) (object_multipart_aborted.failures.min=-1) (multipart_upload_completed.failures.min=-1) (action_executor_acquired.failures.min=-1));
[2025-07-18T16:22:09.688+0000] {subprocess.py:93} INFO - maximums=((op_abort.max=-1) (multipart_upload_completed.max=-1) (op_abort.failures.max=-1) (object_multipart_aborted.failures.max=-1) (multipart_upload_completed.failures.max=-1) (object_multipart_aborted.max=-1) (action_executor_acquired.max=-1) (action_executor_acquired.failures.max=-1));
[2025-07-18T16:22:09.689+0000] {subprocess.py:93} INFO - means=((multipart_upload_completed.mean=(samples=0, sum=0, mean=0.0000)) (op_abort.failures.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.failures.mean=(samples=0, sum=0, mean=0.0000)) (action_executor_acquired.mean=(samples=0, sum=0, mean=0.0000)) (action_executor_acquired.failures.mean=(samples=0, sum=0, mean=0.0000)) (object_multipart_aborted.mean=(samples=0, sum=0, mean=0.0000)) (op_abort.mean=(samples=0, sum=0, mean=0.0000)) (multipart_upload_completed.failures.mean=(samples=0, sum=0, mean=0.0000)));
[2025-07-18T16:22:09.689+0000] {subprocess.py:93} INFO - , blocksActive=0, blockUploadsCompleted=0, blocksAllocated=1, blocksReleased=1, blocksActivelyAllocated=0, transferDuration=0 ms, totalUploadDuration=0 ms, effectiveBandwidth=0.0 bytes/s}
[2025-07-18T16:22:09.945+0000] {subprocess.py:93} INFO - 25/07/18 16:22:09 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v99.metadata.json
[2025-07-18T16:22:10.058+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v91.metadata.json
[2025-07-18T16:22:10.216+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO SnapshotProducer: Committed snapshot 230914149651341311 (FastAppend)
[2025-07-18T16:22:10.414+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO SnapshotProducer: Committed snapshot 3619180016044385824 (FastAppend)
[2025-07-18T16:22:10.475+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=230914149651341311, sequenceNumber=98, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT5.404456502S, count=1}, attempts=CounterResult{unit=COUNT, value=5}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=98}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=884}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2881}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=313283}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:10.483+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO SparkWrite: Committed in 5408 ms
[2025-07-18T16:22:10.484+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:10.649+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/1 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/.1.fd652731-0a92-481a-913e-df6c216d007a.tmp
[2025-07-18T16:22:10.696+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=3619180016044385824, sequenceNumber=90, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT5.732109794S, count=1}, attempts=CounterResult{unit=COUNT, value=5}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=90}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=884}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2920}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=286809}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:10.697+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO SparkWrite: Committed in 5730 ms
[2025-07-18T16:22:10.697+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:10.702+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/.1.fd652731-0a92-481a-913e-df6c216d007a.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/1
[2025-07-18T16:22:10.710+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:10.711+0000] {subprocess.py:93} INFO -   "id" : "be2c6861-99cb-4fa3-9890-9efceeee772e",
[2025-07-18T16:22:10.717+0000] {subprocess.py:93} INFO -   "runId" : "91ffa111-59ed-4773-9f51-8d852278d143",
[2025-07-18T16:22:10.721+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:10.723+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:03.488Z",
[2025-07-18T16:22:10.723+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:22:10.724+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:10.724+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 50.0,
[2025-07-18T16:22:10.724+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.13865779256794233,
[2025-07-18T16:22:10.726+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:10.730+0000] {subprocess.py:93} INFO -     "addBatch" : 6751,
[2025-07-18T16:22:10.730+0000] {subprocess.py:93} INFO -     "commitOffsets" : 211,
[2025-07-18T16:22:10.731+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:10.731+0000] {subprocess.py:93} INFO -     "latestOffset" : 14,
[2025-07-18T16:22:10.732+0000] {subprocess.py:93} INFO -     "queryPlanning" : 99,
[2025-07-18T16:22:10.736+0000] {subprocess.py:93} INFO -     "triggerExecution" : 7212,
[2025-07-18T16:22:10.737+0000] {subprocess.py:93} INFO -     "walCommit" : 135
[2025-07-18T16:22:10.737+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:10.737+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:10.737+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:10.737+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:10.737+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:10.737+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:10.738+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:10.738+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.738+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.738+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:10.738+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:10.738+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.739+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:10.740+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 50.0,
[2025-07-18T16:22:10.740+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.13865779256794233,
[2025-07-18T16:22:10.740+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:10.741+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:10.748+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:10.748+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:10.749+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:10.749+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:10.749+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:10.749+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:10.751+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:10.751+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:10.751+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:10.751+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/1 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/.1.7e16b172-c6fb-453e-8f4e-12375a503eef.tmp
[2025-07-18T16:22:10.751+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/.2.a4cd0f72-b972-44b6-baf0-cc57beea6054.tmp
[2025-07-18T16:22:10.845+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/.1.7e16b172-c6fb-453e-8f4e-12375a503eef.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/1
[2025-07-18T16:22:10.847+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/.2.a4cd0f72-b972-44b6-baf0-cc57beea6054.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/offsets/2
[2025-07-18T16:22:10.849+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855730713,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:10.869+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:10.879+0000] {subprocess.py:93} INFO -   "id" : "e935316f-aa2a-44cf-869a-4315cd970f5e",
[2025-07-18T16:22:10.879+0000] {subprocess.py:93} INFO -   "runId" : "da834d14-1857-4e45-8d37-84093eeb2466",
[2025-07-18T16:22:10.898+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:10.907+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:02.861Z",
[2025-07-18T16:22:10.908+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:22:10.910+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:10.915+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:10.916+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.1252661906551422,
[2025-07-18T16:22:10.917+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:10.918+0000] {subprocess.py:93} INFO -     "addBatch" : 7430,
[2025-07-18T16:22:10.919+0000] {subprocess.py:93} INFO -     "commitOffsets" : 145,
[2025-07-18T16:22:10.919+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:22:10.920+0000] {subprocess.py:93} INFO -     "latestOffset" : 5,
[2025-07-18T16:22:10.920+0000] {subprocess.py:93} INFO -     "queryPlanning" : 245,
[2025-07-18T16:22:10.921+0000] {subprocess.py:93} INFO -     "triggerExecution" : 7983,
[2025-07-18T16:22:10.923+0000] {subprocess.py:93} INFO -     "walCommit" : 152
[2025-07-18T16:22:10.924+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:10.924+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:10.925+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:10.925+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:10.926+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:10.926+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:10.927+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:10.927+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.927+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.928+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:10.928+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:10.928+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:10.928+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.929+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.929+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:10.929+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:10.929+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:10.930+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:10.930+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:10.930+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:10.930+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 83.33333333333333,
[2025-07-18T16:22:10.930+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.1252661906551422,
[2025-07-18T16:22:10.931+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:10.931+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:10.931+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:10.931+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:10.932+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:10.932+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:10.932+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:10.932+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:10.933+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:10.933+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:10.933+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:11.004+0000] {subprocess.py:93} INFO - 25/07/18 16:22:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/.2.5b65c8ab-1e6d-4165-8a3a-b16feeaa4071.tmp
[2025-07-18T16:22:11.058+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.058+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.059+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.059+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.065+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.074+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.080+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.093+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.094+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/.2.5b65c8ab-1e6d-4165-8a3a-b16feeaa4071.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/offsets/2
[2025-07-18T16:22:11.096+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855730934,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:11.096+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.102+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.103+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.103+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.113+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.113+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v101.metadata.json
[2025-07-18T16:22:11.150+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.158+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.167+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.170+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.171+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.176+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.181+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.215+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.224+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.225+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 77cb57a6bd53:39921 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.229+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Created broadcast 12 from start at <unknown>:0
[2025-07-18T16:22:11.230+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.231+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.235+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.237+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:11.238+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:11.248+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Got job 6 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:11.251+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Final stage: ResultStage 6 (start at <unknown>:0)
[2025-07-18T16:22:11.252+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:11.253+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:11.254+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:11.265+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.268+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.274+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 28.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.301+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 433.9 MiB)
[2025-07-18T16:22:11.306+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 77cb57a6bd53:39921 (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.316+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:11.320+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:11.323+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-07-18T16:22:11.328+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.333+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9934 bytes)
[2025-07-18T16:22:11.335+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.336+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.336+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SnapshotProducer: Committed snapshot 8881877946009538654 (FastAppend)
[2025-07-18T16:22:11.337+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[2025-07-18T16:22:11.387+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.411+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.432+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:11.466+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=checkins-0 fromOffset=133 untilOffset=135, for query queryId=be2c6861-99cb-4fa3-9890-9efceeee772e batchId=2 taskId=6 partitionId=0
[2025-07-18T16:22:11.517+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:22:11.518+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 77cb57a6bd53:39921 in memory (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.519+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 77cb57a6bd53:39921 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.534+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 32.0 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.548+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 29.6 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.560+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 77cb57a6bd53:39921 (size: 29.6 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.561+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Created broadcast 14 from start at <unknown>:0
[2025-07-18T16:22:11.561+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:11.562+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:11.563+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Got job 7 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:11.564+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Final stage: ResultStage 7 (start at <unknown>:0)
[2025-07-18T16:22:11.564+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:11.565+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:11.580+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:11.581+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 28.6 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.582+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.582+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 77cb57a6bd53:39921 (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.583+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:11.583+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:11.585+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-07-18T16:22:11.585+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9939 bytes)
[2025-07-18T16:22:11.585+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-07-18T16:22:11.586+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:11.588+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=reservations-0 fromOffset=133 untilOffset=135, for query queryId=e935316f-aa2a-44cf-869a-4315cd970f5e batchId=2 taskId=7 partitionId=0
[2025-07-18T16:22:11.589+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:22:11.591+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
[2025-07-18T16:22:11.591+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaDataConsumer: From Kafka topicPartition=checkins-0 groupId=spark-kafka-source-62e86035-09e9-4437-b787-d71f66681448-359809305-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 101372458 nanos.
[2025-07-18T16:22:11.592+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4898 bytes result sent to driver
[2025-07-18T16:22:11.593+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 264 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:11.594+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-07-18T16:22:11.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: ResultStage 6 (start at <unknown>:0) finished in 0.336 s
[2025-07-18T16:22:11.596+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:11.597+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-07-18T16:22:11.607+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Job 6 finished: start at <unknown>:0, took 0.355970 s
[2025-07-18T16:22:11.610+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:11.610+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committing epoch 2 for query be2c6861-99cb-4fa3-9890-9efceeee772e in append mode
[2025-07-18T16:22:11.611+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=8881877946009538654, sequenceNumber=100, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT5.431640626S, count=1}, attempts=CounterResult{unit=COUNT, value=5}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=100}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=1}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=884}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2713}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=326128}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:11.611+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committed in 5429 ms
[2025-07-18T16:22:11.613+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:11.614+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
[2025-07-18T16:22:11.615+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaDataConsumer: From Kafka topicPartition=reservations-0 groupId=spark-kafka-source-d95969f5-2607-46b3-881b-56ae36c4705a--187688648-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 27753292 nanos.
[2025-07-18T16:22:11.615+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4741 bytes result sent to driver
[2025-07-18T16:22:11.616+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 65 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:11.616+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-07-18T16:22:11.617+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: ResultStage 7 (start at <unknown>:0) finished in 0.077 s
[2025-07-18T16:22:11.618+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:11.618+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-07-18T16:22:11.619+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Job 7 finished: start at <unknown>:0, took 0.080707 s
[2025-07-18T16:22:11.620+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:11.621+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committing epoch 2 for query e935316f-aa2a-44cf-869a-4315cd970f5e in append mode
[2025-07-18T16:22:11.622+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/1 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/.1.04ffa783-d627-446c-8cf5-0213f7e583ac.tmp
[2025-07-18T16:22:11.623+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Checkins_raw
[2025-07-18T16:22:11.649+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Reservations_raw
[2025-07-18T16:22:11.656+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/.1.04ffa783-d627-446c-8cf5-0213f7e583ac.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/1
[2025-07-18T16:22:11.657+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:11.659+0000] {subprocess.py:93} INFO -   "id" : "a780aed6-ab09-4528-a3e0-eb09bfed40c5",
[2025-07-18T16:22:11.659+0000] {subprocess.py:93} INFO -   "runId" : "7ef0bf89-4533-4669-b108-eeffb0902811",
[2025-07-18T16:22:11.660+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:11.660+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:04.065Z",
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -   "batchId" : 1,
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -   "numInputRows" : 1,
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 37.03703703703704,
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 0.13176966662274345,
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -     "addBatch" : 7241,
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -     "commitOffsets" : 51,
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:11.661+0000] {subprocess.py:93} INFO -     "latestOffset" : 30,
[2025-07-18T16:22:11.662+0000] {subprocess.py:93} INFO -     "queryPlanning" : 130,
[2025-07-18T16:22:11.662+0000] {subprocess.py:93} INFO -     "triggerExecution" : 7589,
[2025-07-18T16:22:11.662+0000] {subprocess.py:93} INFO -     "walCommit" : 136
[2025-07-18T16:22:11.662+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:11.662+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:11.663+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:11.663+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:11.664+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:11.665+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:11.667+0000] {subprocess.py:93} INFO -         "0" : 132
[2025-07-18T16:22:11.667+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.667+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.668+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:11.669+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:11.671+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:11.672+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.673+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.675+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:11.677+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:11.680+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:11.681+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:11.684+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:11.689+0000] {subprocess.py:93} INFO -     "numInputRows" : 1,
[2025-07-18T16:22:11.689+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 37.03703703703704,
[2025-07-18T16:22:11.690+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 0.13176966662274345,
[2025-07-18T16:22:11.692+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:11.695+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:11.697+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:11.697+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:11.698+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:11.699+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:11.702+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:11.704+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:11.706+0000] {subprocess.py:93} INFO -     "numOutputRows" : 1
[2025-07-18T16:22:11.707+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:11.710+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:11.711+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/2 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/.2.6f0babf6-3c8c-493a-b447-19dc3e04a931.tmp
[2025-07-18T16:22:11.729+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/.2.6f0babf6-3c8c-493a-b447-19dc3e04a931.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/offsets/2
[2025-07-18T16:22:11.730+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1752855731662,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.optimizer.pruneFiltersCanPruneStreamingSubplan -> false, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2025-07-18T16:22:11.739+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.744+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.744+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.745+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.745+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.753+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.755+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.756+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.760+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.761+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.770+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting 0 bytes advisory partition size for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.770+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.771+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Requesting [] as write ordering for table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.772+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.773+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2025-07-18T16:22:11.790+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.0 KiB, free 433.9 MiB)
[2025-07-18T16:22:11.801+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 29.5 KiB, free 433.9 MiB)
[2025-07-18T16:22:11.802+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 77cb57a6bd53:39921 (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.803+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 77cb57a6bd53:39921 in memory (size: 29.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.805+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Checkins_raw/metadata/v101.metadata.json
[2025-07-18T16:22:11.808+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 77cb57a6bd53:39921 in memory (size: 12.5 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.811+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Created broadcast 16 from start at <unknown>:0
[2025-07-18T16:22:11.811+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)]. The input RDD has 1 partitions.
[2025-07-18T16:22:11.812+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Starting job: start at <unknown>:0
[2025-07-18T16:22:11.812+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Got job 8 (start at <unknown>:0) with 1 output partitions
[2025-07-18T16:22:11.812+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Final stage: ResultStage 8 (start at <unknown>:0)
[2025-07-18T16:22:11.812+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Parents of final stage: List()
[2025-07-18T16:22:11.812+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Missing parents: List()
[2025-07-18T16:22:11.813+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0), which has no missing parents
[2025-07-18T16:22:11.817+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 77cb57a6bd53:39921 in memory (size: 12.3 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.818+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 27.5 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.821+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 12.2 KiB, free 434.0 MiB)
[2025-07-18T16:22:11.827+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 77cb57a6bd53:39921 (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:11.828+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1611
[2025-07-18T16:22:11.835+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at start at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-18T16:22:11.836+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-07-18T16:22:11.838+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (77cb57a6bd53, executor driver, partition 0, PROCESS_LOCAL, 9936 bytes)
[2025-07-18T16:22:11.839+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-07-18T16:22:11.847+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO CodecPool: Got brand-new compressor [.zstd]
[2025-07-18T16:22:11.849+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=feedback-0 fromOffset=133 untilOffset=135, for query queryId=a780aed6-ab09-4528-a3e0-eb09bfed40c5 batchId=2 taskId=8 partitionId=0
[2025-07-18T16:22:11.872+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Reservations_raw/metadata/v93.metadata.json
[2025-07-18T16:22:11.874+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DataWritingSparkTask: Commit authorized for partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:22:11.902+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SnapshotProducer: Committed snapshot 1879959095328803273 (FastAppend)
[2025-07-18T16:22:11.909+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
[2025-07-18T16:22:11.910+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO KafkaDataConsumer: From Kafka topicPartition=feedback-0 groupId=spark-kafka-source-3df54380-de7f-48ca-9cf9-4ef7a0128a44--1165658726-executor read 2 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 38010792 nanos.
[2025-07-18T16:22:11.911+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 4802 bytes result sent to driver
[2025-07-18T16:22:11.911+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 80 ms on 77cb57a6bd53 (executor driver) (1/1)
[2025-07-18T16:22:11.913+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-07-18T16:22:11.916+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: ResultStage 8 (start at <unknown>:0) finished in 0.099 s
[2025-07-18T16:22:11.916+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-18T16:22:11.923+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-07-18T16:22:11.929+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO DAGScheduler: Job 8 finished: start at <unknown>:0, took 0.103848 s
[2025-07-18T16:22:11.932+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] is committing.
[2025-07-18T16:22:11.951+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committing epoch 2 for query a780aed6-ab09-4528-a3e0-eb09bfed40c5 in append mode
[2025-07-18T16:22:11.970+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committing streaming append with 1 new data files to table my_catalog.bronze.Feedback_raw
[2025-07-18T16:22:11.981+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SnapshotProducer: Committed snapshot 410522294940133056 (FastAppend)
[2025-07-18T16:22:11.982+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Checkins_raw, snapshotId=1879959095328803273, sequenceNumber=100, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.36029275S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=100}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=888}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2949}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=319181}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:11.983+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO SparkWrite: Committed in 361 ms
[2025-07-18T16:22:11.983+0000] {subprocess.py:93} INFO - 25/07/18 16:22:11 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Checkins_raw, format=PARQUET)] committed.
[2025-07-18T16:22:12.009+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/2 using temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/.2.89eb99d1-35f7-4cff-89e5-22d8b9aed3df.tmp
[2025-07-18T16:22:12.136+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Reservations_raw, snapshotId=410522294940133056, sequenceNumber=92, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.483849042S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=92}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=888}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=3052}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=292913}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:12.139+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO SparkWrite: Committed in 484 ms
[2025-07-18T16:22:12.143+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Reservations_raw, format=PARQUET)] committed.
[2025-07-18T16:22:12.152+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/.2.89eb99d1-35f7-4cff-89e5-22d8b9aed3df.tmp to file:/tmp/checkpoints/checkins/scheduled__2025-07-18T16:18:00+00:00/commits/2
[2025-07-18T16:22:12.153+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:12.153+0000] {subprocess.py:93} INFO -   "id" : "be2c6861-99cb-4fa3-9890-9efceeee772e",
[2025-07-18T16:22:12.154+0000] {subprocess.py:93} INFO -   "runId" : "91ffa111-59ed-4773-9f51-8d852278d143",
[2025-07-18T16:22:12.154+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:12.159+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:10.709Z",
[2025-07-18T16:22:12.159+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:22:12.160+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:12.160+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.27696994876055947,
[2025-07-18T16:22:12.160+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.3879250520471893,
[2025-07-18T16:22:12.161+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:12.163+0000] {subprocess.py:93} INFO -     "addBatch" : 915,
[2025-07-18T16:22:12.165+0000] {subprocess.py:93} INFO -     "commitOffsets" : 168,
[2025-07-18T16:22:12.166+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:22:12.166+0000] {subprocess.py:93} INFO -     "latestOffset" : 4,
[2025-07-18T16:22:12.166+0000] {subprocess.py:93} INFO -     "queryPlanning" : 215,
[2025-07-18T16:22:12.166+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1441,
[2025-07-18T16:22:12.166+0000] {subprocess.py:93} INFO -     "walCommit" : 134
[2025-07-18T16:22:12.167+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:12.167+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:12.168+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:12.169+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[checkins]]",
[2025-07-18T16:22:12.169+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:12.170+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:12.170+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:12.171+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.171+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.171+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:12.171+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:12.172+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:12.172+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.173+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.175+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:12.175+0000] {subprocess.py:93} INFO -       "checkins" : {
[2025-07-18T16:22:12.176+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:12.176+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.177+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.177+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:12.177+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.27696994876055947,
[2025-07-18T16:22:12.177+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.3879250520471893,
[2025-07-18T16:22:12.177+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:12.178+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:12.178+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:12.179+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:12.180+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:12.181+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:12.181+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:12.181+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Checkins_raw",
[2025-07-18T16:22:12.183+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:12.184+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:12.184+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:12.186+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/2 using temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/.2.852b41aa-b0d8-4427-b05b-f7ea2c10ecdb.tmp
[2025-07-18T16:22:12.189+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/.2.852b41aa-b0d8-4427-b05b-f7ea2c10ecdb.tmp to file:/tmp/checkpoints/reservations/scheduled__2025-07-18T16:18:00+00:00/commits/2
[2025-07-18T16:22:12.192+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:12.193+0000] {subprocess.py:93} INFO -   "id" : "e935316f-aa2a-44cf-869a-4315cd970f5e",
[2025-07-18T16:22:12.194+0000] {subprocess.py:93} INFO -   "runId" : "da834d14-1857-4e45-8d37-84093eeb2466",
[2025-07-18T16:22:12.194+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:12.195+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:10.852Z",
[2025-07-18T16:22:12.195+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:22:12.196+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:12.196+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.25028156676260793,
[2025-07-18T16:22:12.196+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 1.4981273408239701,
[2025-07-18T16:22:12.196+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:12.196+0000] {subprocess.py:93} INFO -     "addBatch" : 951,
[2025-07-18T16:22:12.196+0000] {subprocess.py:93} INFO -     "commitOffsets" : 52,
[2025-07-18T16:22:12.197+0000] {subprocess.py:93} INFO -     "getBatch" : 0,
[2025-07-18T16:22:12.198+0000] {subprocess.py:93} INFO -     "latestOffset" : 82,
[2025-07-18T16:22:12.199+0000] {subprocess.py:93} INFO -     "queryPlanning" : 95,
[2025-07-18T16:22:12.199+0000] {subprocess.py:93} INFO -     "triggerExecution" : 1335,
[2025-07-18T16:22:12.199+0000] {subprocess.py:93} INFO -     "walCommit" : 153
[2025-07-18T16:22:12.199+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:12.200+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:12.200+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:12.201+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[reservations]]",
[2025-07-18T16:22:12.202+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:12.203+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:12.203+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:12.203+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.203+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.204+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:12.205+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:12.206+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:12.207+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.208+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.210+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:12.210+0000] {subprocess.py:93} INFO -       "reservations" : {
[2025-07-18T16:22:12.210+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:12.210+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.211+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.211+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:12.211+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.25028156676260793,
[2025-07-18T16:22:12.212+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 1.4981273408239701,
[2025-07-18T16:22:12.213+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:12.213+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:12.214+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:12.214+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:12.214+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:12.216+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:12.216+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:12.216+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Reservations_raw",
[2025-07-18T16:22:12.217+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:12.217+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:12.218+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:12.250+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO HadoopTableOperations: Committed a new metadata file s3a://warehouse/bronze/Feedback_raw/metadata/v102.metadata.json
[2025-07-18T16:22:12.299+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO SnapshotProducer: Committed snapshot 2135282792761893586 (FastAppend)
[2025-07-18T16:22:12.331+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=my_catalog.bronze.Feedback_raw, snapshotId=2135282792761893586, sequenceNumber=101, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.358756667S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=101}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=2}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=886}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=2894}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=329022}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.6, app-id=local-1752855616325, engine-name=spark, iceberg-version=Apache Iceberg 1.4.0 (commit 10367c380098c2e06a49521a33681ac7f6c64b2c)}}
[2025-07-18T16:22:12.332+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO SparkWrite: Committed in 359 ms
[2025-07-18T16:22:12.332+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: IcebergStreamingWrite(table=my_catalog.bronze.Feedback_raw, format=PARQUET)] committed.
[2025-07-18T16:22:12.336+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/2 using temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/.2.7b048539-883a-405d-a463-084b6973771a.tmp
[2025-07-18T16:22:12.361+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/.2.7b048539-883a-405d-a463-084b6973771a.tmp to file:/tmp/checkpoints/feedback/scheduled__2025-07-18T16:18:00+00:00/commits/2
[2025-07-18T16:22:12.362+0000] {subprocess.py:93} INFO - 25/07/18 16:22:12 INFO MicroBatchExecution: Streaming query made progress: {
[2025-07-18T16:22:12.363+0000] {subprocess.py:93} INFO -   "id" : "a780aed6-ab09-4528-a3e0-eb09bfed40c5",
[2025-07-18T16:22:12.363+0000] {subprocess.py:93} INFO -   "runId" : "7ef0bf89-4533-4669-b108-eeffb0902811",
[2025-07-18T16:22:12.363+0000] {subprocess.py:93} INFO -   "name" : null,
[2025-07-18T16:22:12.364+0000] {subprocess.py:93} INFO -   "timestamp" : "2025-07-18T16:22:11.656Z",
[2025-07-18T16:22:12.364+0000] {subprocess.py:93} INFO -   "batchId" : 2,
[2025-07-18T16:22:12.364+0000] {subprocess.py:93} INFO -   "numInputRows" : 2,
[2025-07-18T16:22:12.364+0000] {subprocess.py:93} INFO -   "inputRowsPerSecond" : 0.26346989856408903,
[2025-07-18T16:22:12.364+0000] {subprocess.py:93} INFO -   "processedRowsPerSecond" : 2.8409090909090913,
[2025-07-18T16:22:12.365+0000] {subprocess.py:93} INFO -   "durationMs" : {
[2025-07-18T16:22:12.365+0000] {subprocess.py:93} INFO -     "addBatch" : 583,
[2025-07-18T16:22:12.365+0000] {subprocess.py:93} INFO -     "commitOffsets" : 32,
[2025-07-18T16:22:12.365+0000] {subprocess.py:93} INFO -     "getBatch" : 1,
[2025-07-18T16:22:12.365+0000] {subprocess.py:93} INFO -     "latestOffset" : 6,
[2025-07-18T16:22:12.365+0000] {subprocess.py:93} INFO -     "queryPlanning" : 15,
[2025-07-18T16:22:12.366+0000] {subprocess.py:93} INFO -     "triggerExecution" : 704,
[2025-07-18T16:22:12.366+0000] {subprocess.py:93} INFO -     "walCommit" : 67
[2025-07-18T16:22:12.366+0000] {subprocess.py:93} INFO -   },
[2025-07-18T16:22:12.366+0000] {subprocess.py:93} INFO -   "stateOperators" : [ ],
[2025-07-18T16:22:12.366+0000] {subprocess.py:93} INFO -   "sources" : [ {
[2025-07-18T16:22:12.367+0000] {subprocess.py:93} INFO -     "description" : "KafkaV2[Subscribe[feedback]]",
[2025-07-18T16:22:12.367+0000] {subprocess.py:93} INFO -     "startOffset" : {
[2025-07-18T16:22:12.368+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:12.368+0000] {subprocess.py:93} INFO -         "0" : 133
[2025-07-18T16:22:12.369+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.369+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.369+0000] {subprocess.py:93} INFO -     "endOffset" : {
[2025-07-18T16:22:12.370+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:12.370+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:12.370+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.370+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.371+0000] {subprocess.py:93} INFO -     "latestOffset" : {
[2025-07-18T16:22:12.371+0000] {subprocess.py:93} INFO -       "feedback" : {
[2025-07-18T16:22:12.371+0000] {subprocess.py:93} INFO -         "0" : 135
[2025-07-18T16:22:12.372+0000] {subprocess.py:93} INFO -       }
[2025-07-18T16:22:12.373+0000] {subprocess.py:93} INFO -     },
[2025-07-18T16:22:12.373+0000] {subprocess.py:93} INFO -     "numInputRows" : 2,
[2025-07-18T16:22:12.374+0000] {subprocess.py:93} INFO -     "inputRowsPerSecond" : 0.26346989856408903,
[2025-07-18T16:22:12.374+0000] {subprocess.py:93} INFO -     "processedRowsPerSecond" : 2.8409090909090913,
[2025-07-18T16:22:12.374+0000] {subprocess.py:93} INFO -     "metrics" : {
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -       "avgOffsetsBehindLatest" : "0.0",
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -       "maxOffsetsBehindLatest" : "0",
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -       "minOffsetsBehindLatest" : "0"
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -     }
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -   } ],
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -   "sink" : {
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -     "description" : "my_catalog.bronze.Feedback_raw",
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -     "numOutputRows" : 2
[2025-07-18T16:22:12.375+0000] {subprocess.py:93} INFO -   }
[2025-07-18T16:22:12.376+0000] {subprocess.py:93} INFO - }
[2025-07-18T16:22:16.217+0000] {subprocess.py:93} INFO - 25/07/18 16:22:16 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 77cb57a6bd53:39921 in memory (size: 12.2 KiB, free: 434.2 MiB)
[2025-07-18T16:22:16.222+0000] {subprocess.py:93} INFO - 25/07/18 16:22:16 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 77cb57a6bd53:39921 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:16.227+0000] {subprocess.py:93} INFO - 25/07/18 16:22:16 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 77cb57a6bd53:39921 in memory (size: 29.5 KiB, free: 434.3 MiB)
[2025-07-18T16:22:16.240+0000] {subprocess.py:93} INFO - 25/07/18 16:22:16 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 77cb57a6bd53:39921 in memory (size: 29.6 KiB, free: 434.3 MiB)
[2025-07-18T16:22:22.153+0000] {subprocess.py:93} INFO - 25/07/18 16:22:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:22.198+0000] {subprocess.py:93} INFO - 25/07/18 16:22:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:22.424+0000] {subprocess.py:93} INFO - 25/07/18 16:22:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:32.183+0000] {subprocess.py:93} INFO - 25/07/18 16:22:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:32.204+0000] {subprocess.py:93} INFO - 25/07/18 16:22:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:32.403+0000] {subprocess.py:93} INFO - 25/07/18 16:22:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.763+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.838+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:42.848+0000] {subprocess.py:93} INFO - 25/07/18 16:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:57.504+0000] {subprocess.py:93} INFO - 25/07/18 16:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:22:59.047+0000] {subprocess.py:93} INFO - 25/07/18 16:22:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:00.605+0000] {subprocess.py:93} INFO - 25/07/18 16:22:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.282+0000] {subprocess.py:93} INFO - 25/07/18 16:23:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.325+0000] {subprocess.py:93} INFO - 25/07/18 16:23:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:06.328+0000] {subprocess.py:93} INFO - 25/07/18 16:23:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:07.430+0000] {subprocess.py:93} INFO - 25/07/18 16:23:07 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map()) by listener AppStatusListener took 1.069884626s.
[2025-07-18T16:23:14.877+0000] {subprocess.py:93} INFO - 25/07/18 16:23:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:14.882+0000] {subprocess.py:93} INFO - 25/07/18 16:23:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:14.883+0000] {subprocess.py:93} INFO - 25/07/18 16:23:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:24.962+0000] {subprocess.py:93} INFO - 25/07/18 16:23:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:25.014+0000] {subprocess.py:93} INFO - 25/07/18 16:23:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:25.039+0000] {subprocess.py:93} INFO - 25/07/18 16:23:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:36.240+0000] {subprocess.py:93} INFO - 25/07/18 16:23:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:37.869+0000] {subprocess.py:93} INFO - 25/07/18 16:23:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:39.439+0000] {subprocess.py:93} INFO - 25/07/18 16:23:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:54.591+0000] {subprocess.py:93} INFO - 25/07/18 16:23:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:56.602+0000] {subprocess.py:93} INFO - 25/07/18 16:23:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:23:59.369+0000] {subprocess.py:93} INFO - 25/07/18 16:23:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:05.519+0000] {subprocess.py:93} INFO - 25/07/18 16:23:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:09.380+0000] {subprocess.py:93} INFO - 25/07/18 16:23:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:02.477+0000] {subprocess.py:93} INFO - 25/07/18 16:24:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:04.499+0000] {subprocess.py:93} INFO - 25/07/18 16:24:02 INFO AsyncEventQueue: Process of event org.apache.spark.sql.streaming.StreamingQueryListener$QueryIdleEvent@11dedbe0 by listener StreamingQueryListenerBus took 1.281981001s.
[2025-07-18T16:24:07.495+0000] {subprocess.py:93} INFO - 25/07/18 16:24:04 INFO AsyncEventQueue: Process of event org.apache.spark.sql.streaming.StreamingQueryListener$QueryIdleEvent@1fc97faf by listener StreamingQueryListenerBus took 1.101895417s.
[2025-07-18T16:24:09.422+0000] {subprocess.py:93} INFO - 25/07/18 16:24:06 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@58d8b5af)) by listener AppStatusListener took 1.412560084s.
[2025-07-18T16:24:10.222+0000] {subprocess.py:93} INFO - 25/07/18 16:24:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:11.858+0000] {subprocess.py:93} INFO - 25/07/18 16:24:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:16.084+0000] {subprocess.py:93} INFO - 25/07/18 16:24:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:27.661+0000] {subprocess.py:93} INFO - 25/07/18 16:24:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:24:42.219+0000] {subprocess.py:93} INFO - 25/07/18 16:24:22 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@7c426ba6)) by listener AppStatusListener took 1.011412042s.
[2025-07-18T16:24:45.981+0000] {subprocess.py:93} INFO - 25/07/18 16:24:28 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map()) by listener AppStatusListener took 2.085607417s.
[2025-07-18T16:24:50.332+0000] {subprocess.py:93} INFO - 25/07/18 16:24:42 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@5c59f12d)) by listener AppStatusListener took 3.204155002s.
[2025-07-18T16:24:52.630+0000] {subprocess.py:93} INFO - 25/07/18 16:24:45 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map()) by listener AppStatusListener took 1.190105751s.
[2025-07-18T16:25:04.908+0000] {subprocess.py:93} INFO - 25/07/18 16:24:54 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@51f576a3)) by listener AppStatusListener took 1.085081417s.
[2025-07-18T16:25:16.122+0000] {subprocess.py:93} INFO - 25/07/18 16:25:05 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map()) by listener AppStatusListener took 1.871814293s.
[2025-07-18T16:25:22.708+0000] {subprocess.py:93} INFO - 25/07/18 16:25:10 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@4c1dd4ff)) by listener AppStatusListener took 1.313164876s.
[2025-07-18T16:25:33.021+0000] {subprocess.py:93} INFO - 25/07/18 16:25:32 INFO AsyncEventQueue: Process of event SparkListenerExecutorMetricsUpdate(driver,WrappedArray(),Map((-1,-1) -> org.apache.spark.executor.ExecutorMetrics@236d9981)) by listener AppStatusListener took 5.512021252s.
[2025-07-18T16:25:33.944+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 INFO NetworkClient: [AdminClient clientId=adminclient-3] Node -1 disconnected.
[2025-07-18T16:25:33.958+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
[2025-07-18T16:25:33.966+0000] {subprocess.py:93} INFO - 25/07/18 16:25:33 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[2025-07-18T16:25:32.500+0000] {job.py:219} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/job.py", line 215, in heartbeat
    heartbeat_callback(session)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/jobs/local_task_job_runner.py", line 246, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 866, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/future/engine.py", line 406, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-07-18T16:25:34.634+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-3] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:34.639+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-2] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:34.653+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-1] Disconnecting from node 1 due to request timeout.
[2025-07-18T16:25:34.659+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-1] Cancelled in-flight METADATA request with correlation id 32449 due to node 1 being disconnected (elapsed time since creation: 51361ms, elapsed time since send: 51361ms, request timeout: 30000ms)
[2025-07-18T16:25:34.674+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-3] Cancelled in-flight METADATA request with correlation id 32527 due to node 1 being disconnected (elapsed time since creation: 49900ms, elapsed time since send: 49900ms, request timeout: 30000ms)
[2025-07-18T16:25:34.679+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 INFO NetworkClient: [AdminClient clientId=adminclient-2] Cancelled in-flight METADATA request with correlation id 32558 due to node 1 being disconnected (elapsed time since creation: 50789ms, elapsed time since send: 50789ms, request timeout: 30000ms)
[2025-07-18T16:25:34.696+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.703+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855912714, tries=10, nextAllowedTryMs=1752855934732) timed out at 1752855934632 after 10 attempt(s)
[2025-07-18T16:25:34.712+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.720+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.724+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.728+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.730+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.796+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.799+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.801+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.804+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.808+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.813+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.814+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.820+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.821+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.821+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.821+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.821+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.822+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.822+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.822+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.822+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.823+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.823+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.825+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.827+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.828+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.829+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.831+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.832+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.833+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.834+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.834+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.843+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855912714, tries=10, nextAllowedTryMs=1752855934732) timed out at 1752855934632 after 10 attempt(s)
[2025-07-18T16:25:34.852+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 32449 due to node 1 being disconnected
[2025-07-18T16:25:34.855+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.855+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855931017, tries=5, nextAllowedTryMs=1752855934736) timed out at 1752855934636 after 5 attempt(s)
[2025-07-18T16:25:34.856+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.857+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.858+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.863+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.864+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.866+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.867+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.867+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.869+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.880+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.880+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.882+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.882+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.883+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.884+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.888+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.889+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.890+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.892+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.893+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.894+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.895+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.897+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.897+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.899+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.900+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.907+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:34.908+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.909+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:34.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:34.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:34.911+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.912+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:34.913+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:34.913+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855931017, tries=5, nextAllowedTryMs=1752855934736) timed out at 1752855934636 after 5 attempt(s)
[2025-07-18T16:25:34.914+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 32558 due to node 1 being disconnected
[2025-07-18T16:25:34.914+0000] {subprocess.py:93} INFO - 25/07/18 16:25:34 WARN KafkaOffsetReaderAdmin: Error in attempt 1 getting Kafka offsets:
[2025-07-18T16:25:34.915+0000] {subprocess.py:93} INFO - java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855921951, tries=13, nextAllowedTryMs=1752855934734) timed out at 1752855934634 after 13 attempt(s)
[2025-07-18T16:25:34.916+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.reportGet(Unknown Source)
[2025-07-18T16:25:34.917+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.CompletableFuture.get(Unknown Source)
[2025-07-18T16:25:34.917+0000] {subprocess.py:93} INFO - 	at org.apache.kafka.common.internals.KafkaFutureImpl.get(KafkaFutureImpl.java:165)
[2025-07-18T16:25:34.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.listOffsets(KafkaOffsetReaderAdmin.scala:88)
[2025-07-18T16:25:34.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$fetchLatestOffsets$1(KafkaOffsetReaderAdmin.scala:332)
[2025-07-18T16:25:34.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.$anonfun$partitionsAssignedToAdmin$1(KafkaOffsetReaderAdmin.scala:501)
[2025-07-18T16:25:34.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.withRetries(KafkaOffsetReaderAdmin.scala:518)
[2025-07-18T16:25:34.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.partitionsAssignedToAdmin(KafkaOffsetReaderAdmin.scala:498)
[2025-07-18T16:25:34.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchLatestOffsets(KafkaOffsetReaderAdmin.scala:297)
[2025-07-18T16:25:34.923+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.kafka010.KafkaMicroBatchStream.latestOffset(KafkaMicroBatchStream.scala:130)
[2025-07-18T16:25:34.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$4(MicroBatchExecution.scala:491)
[2025-07-18T16:25:34.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.927+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.928+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:490)
[2025-07-18T16:25:34.928+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[2025-07-18T16:25:34.930+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach(Iterator.scala:943)
[2025-07-18T16:25:34.931+0000] {subprocess.py:93} INFO - 	at scala.collection.Iterator.foreach$(Iterator.scala:943)
[2025-07-18T16:25:34.934+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
[2025-07-18T16:25:34.935+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
[2025-07-18T16:25:34.940+0000] {subprocess.py:93} INFO - 	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
[2025-07-18T16:25:34.944+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
[2025-07-18T16:25:34.947+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
[2025-07-18T16:25:34.953+0000] {subprocess.py:93} INFO - 	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[2025-07-18T16:25:34.961+0000] {subprocess.py:93} INFO - 	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
[2025-07-18T16:25:34.962+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:479)
[2025-07-18T16:25:34.965+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
[2025-07-18T16:25:34.966+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:810)
[2025-07-18T16:25:34.967+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:475)
[2025-07-18T16:25:34.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:268)
[2025-07-18T16:25:34.973+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:34.977+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
[2025-07-18T16:25:34.981+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
[2025-07-18T16:25:34.982+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
[2025-07-18T16:25:34.985+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
[2025-07-18T16:25:34.988+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)
[2025-07-18T16:25:34.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
[2025-07-18T16:25:34.995+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
[2025-07-18T16:25:35.001+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:35.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-07-18T16:25:35.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
[2025-07-18T16:25:35.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
[2025-07-18T16:25:35.009+0000] {subprocess.py:93} INFO - 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[2025-07-18T16:25:35.011+0000] {subprocess.py:93} INFO - 	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-07-18T16:25:35.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2025-07-18T16:25:35.013+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.TimeoutException: Call(callName=metadata, deadlineMs=1752855921951, tries=13, nextAllowedTryMs=1752855934734) timed out at 1752855934634 after 13 attempt(s)
[2025-07-18T16:25:35.014+0000] {subprocess.py:93} INFO - Caused by: org.apache.kafka.common.errors.DisconnectException: Cancelled metadata request with correlation id 32527 due to node 1 being disconnected
[2025-07-18T16:25:35.736+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: App info kafka.admin.client for adminclient-3 unregistered
[2025-07-18T16:25:35.747+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: App info kafka.admin.client for adminclient-2 unregistered
[2025-07-18T16:25:35.755+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2025-07-18T16:25:35.757+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:35.768+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:35.768+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.769+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:35.771+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:35.772+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.775+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics scheduler closed
[2025-07-18T16:25:35.778+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2025-07-18T16:25:35.780+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO Metrics: Metrics reporters closed
[2025-07-18T16:25:35.781+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.782+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.784+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.784+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.784+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.786+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.786+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.790+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.792+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.794+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.797+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.798+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.800+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.802+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.805+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.810+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.821+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.828+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.829+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.832+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.834+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.834+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.835+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.836+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.837+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.838+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.842+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.845+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.847+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.848+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.849+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.851+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.857+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.860+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.861+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.861+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.862+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.863+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.864+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.867+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.869+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.872+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.872+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.873+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.874+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.877+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.879+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.886+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.889+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.890+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.890+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.892+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.893+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.893+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.893+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.894+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.900+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.901+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.907+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.908+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.909+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.909+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.909+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.910+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.910+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.910+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.913+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.914+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.914+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.914+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.914+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.914+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.915+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.915+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.915+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.915+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.915+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.916+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.916+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.916+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.916+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.916+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.916+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.917+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.917+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.917+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.918+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.918+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.918+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.918+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.918+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:35.919+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:35.919+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:35.922+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:35.923+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:35.925+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:35.926+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:35.926+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:35.928+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:35.928+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:35.928+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:35.928+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:35.928+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:35.929+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:35.929+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.930+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:35.932+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:35.933+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:35.933+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:35.933+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:35.934+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:35.936+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:35.936+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:35.937+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:35.937+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:35.937+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:35.937+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:35.937+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:35.938+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:35.938+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:35.938+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:35.938+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:35.938+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:35.938+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:35.938+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:35.939+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:35.940+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:35.940+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:35.940+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:35.941+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:35.941+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:35.942+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:35.942+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:35.942+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:35.945+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:35.945+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:35.945+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:35.945+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:35.945+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:35.946+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:35.946+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:35.946+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:35.946+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: AdminClientConfig values:
[2025-07-18T16:25:35.946+0000] {subprocess.py:93} INFO - 	auto.include.jmx.reporter = true
[2025-07-18T16:25:35.946+0000] {subprocess.py:93} INFO - 	bootstrap.servers = [kafka:9092]
[2025-07-18T16:25:35.947+0000] {subprocess.py:93} INFO - 	client.dns.lookup = use_all_dns_ips
[2025-07-18T16:25:35.948+0000] {subprocess.py:93} INFO - 	client.id =
[2025-07-18T16:25:35.952+0000] {subprocess.py:93} INFO - 	connections.max.idle.ms = 300000
[2025-07-18T16:25:35.956+0000] {subprocess.py:93} INFO - 	default.api.timeout.ms = 60000
[2025-07-18T16:25:35.960+0000] {subprocess.py:93} INFO - 	metadata.max.age.ms = 300000
[2025-07-18T16:25:35.960+0000] {subprocess.py:93} INFO - 	metric.reporters = []
[2025-07-18T16:25:35.960+0000] {subprocess.py:93} INFO - 	metrics.num.samples = 2
[2025-07-18T16:25:35.961+0000] {subprocess.py:93} INFO - 	metrics.recording.level = INFO
[2025-07-18T16:25:35.961+0000] {subprocess.py:93} INFO - 	metrics.sample.window.ms = 30000
[2025-07-18T16:25:35.963+0000] {subprocess.py:93} INFO - 	receive.buffer.bytes = 65536
[2025-07-18T16:25:35.964+0000] {subprocess.py:93} INFO - 	reconnect.backoff.max.ms = 1000
[2025-07-18T16:25:35.966+0000] {subprocess.py:93} INFO - 	reconnect.backoff.ms = 50
[2025-07-18T16:25:35.966+0000] {subprocess.py:93} INFO - 	request.timeout.ms = 30000
[2025-07-18T16:25:35.967+0000] {subprocess.py:93} INFO - 	retries = 2147483647
[2025-07-18T16:25:35.986+0000] {subprocess.py:93} INFO - 	retry.backoff.ms = 100
[2025-07-18T16:25:35.993+0000] {subprocess.py:93} INFO - 	sasl.client.callback.handler.class = null
[2025-07-18T16:25:35.998+0000] {subprocess.py:93} INFO - 	sasl.jaas.config = null
[2025-07-18T16:25:36.010+0000] {subprocess.py:93} INFO - 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
[2025-07-18T16:25:36.011+0000] {subprocess.py:93} INFO - 	sasl.kerberos.min.time.before.relogin = 60000
[2025-07-18T16:25:36.028+0000] {subprocess.py:93} INFO - 	sasl.kerberos.service.name = null
[2025-07-18T16:25:36.035+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.jitter = 0.05
[2025-07-18T16:25:36.040+0000] {subprocess.py:93} INFO - 	sasl.kerberos.ticket.renew.window.factor = 0.8
[2025-07-18T16:25:36.042+0000] {subprocess.py:93} INFO - 	sasl.login.callback.handler.class = null
[2025-07-18T16:25:36.045+0000] {subprocess.py:93} INFO - 	sasl.login.class = null
[2025-07-18T16:25:36.047+0000] {subprocess.py:93} INFO - 	sasl.login.connect.timeout.ms = null
[2025-07-18T16:25:36.050+0000] {subprocess.py:93} INFO - 	sasl.login.read.timeout.ms = null
[2025-07-18T16:25:36.056+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.buffer.seconds = 300
[2025-07-18T16:25:36.062+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.min.period.seconds = 60
[2025-07-18T16:25:36.064+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.factor = 0.8
[2025-07-18T16:25:36.068+0000] {subprocess.py:93} INFO - 	sasl.login.refresh.window.jitter = 0.05
[2025-07-18T16:25:36.075+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.max.ms = 10000
[2025-07-18T16:25:36.077+0000] {subprocess.py:93} INFO - 	sasl.login.retry.backoff.ms = 100
[2025-07-18T16:25:36.079+0000] {subprocess.py:93} INFO - 	sasl.mechanism = GSSAPI
[2025-07-18T16:25:36.083+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.clock.skew.seconds = 30
[2025-07-18T16:25:36.094+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.audience = null
[2025-07-18T16:25:36.096+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.expected.issuer = null
[2025-07-18T16:25:36.099+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
[2025-07-18T16:25:36.102+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
[2025-07-18T16:25:36.105+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
[2025-07-18T16:25:36.106+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.jwks.endpoint.url = null
[2025-07-18T16:25:36.109+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.scope.claim.name = scope
[2025-07-18T16:25:36.124+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.sub.claim.name = sub
[2025-07-18T16:25:36.124+0000] {subprocess.py:93} INFO - 	sasl.oauthbearer.token.endpoint.url = null
[2025-07-18T16:25:36.124+0000] {subprocess.py:93} INFO - 	security.protocol = PLAINTEXT
[2025-07-18T16:25:36.125+0000] {subprocess.py:93} INFO - 	security.providers = null
[2025-07-18T16:25:36.128+0000] {subprocess.py:93} INFO - 	send.buffer.bytes = 131072
[2025-07-18T16:25:36.128+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.max.ms = 30000
[2025-07-18T16:25:36.129+0000] {subprocess.py:93} INFO - 	socket.connection.setup.timeout.ms = 10000
[2025-07-18T16:25:36.129+0000] {subprocess.py:93} INFO - 	ssl.cipher.suites = null
[2025-07-18T16:25:36.130+0000] {subprocess.py:93} INFO - 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
[2025-07-18T16:25:36.130+0000] {subprocess.py:93} INFO - 	ssl.endpoint.identification.algorithm = https
[2025-07-18T16:25:36.132+0000] {subprocess.py:93} INFO - 	ssl.engine.factory.class = null
[2025-07-18T16:25:36.133+0000] {subprocess.py:93} INFO - 	ssl.key.password = null
[2025-07-18T16:25:36.134+0000] {subprocess.py:93} INFO - 	ssl.keymanager.algorithm = SunX509
[2025-07-18T16:25:36.135+0000] {subprocess.py:93} INFO - 	ssl.keystore.certificate.chain = null
[2025-07-18T16:25:36.135+0000] {subprocess.py:93} INFO - 	ssl.keystore.key = null
[2025-07-18T16:25:36.136+0000] {subprocess.py:93} INFO - 	ssl.keystore.location = null
[2025-07-18T16:25:36.136+0000] {subprocess.py:93} INFO - 	ssl.keystore.password = null
[2025-07-18T16:25:36.137+0000] {subprocess.py:93} INFO - 	ssl.keystore.type = JKS
[2025-07-18T16:25:36.139+0000] {subprocess.py:93} INFO - 	ssl.protocol = TLSv1.3
[2025-07-18T16:25:36.140+0000] {subprocess.py:93} INFO - 	ssl.provider = null
[2025-07-18T16:25:36.144+0000] {subprocess.py:93} INFO - 	ssl.secure.random.implementation = null
[2025-07-18T16:25:36.145+0000] {subprocess.py:93} INFO - 	ssl.trustmanager.algorithm = PKIX
[2025-07-18T16:25:36.146+0000] {subprocess.py:93} INFO - 	ssl.truststore.certificates = null
[2025-07-18T16:25:36.146+0000] {subprocess.py:93} INFO - 	ssl.truststore.location = null
[2025-07-18T16:25:36.147+0000] {subprocess.py:93} INFO - 	ssl.truststore.password = null
[2025-07-18T16:25:36.148+0000] {subprocess.py:93} INFO - 	ssl.truststore.type = JKS
[2025-07-18T16:25:36.149+0000] {subprocess.py:93} INFO - 
[2025-07-18T16:25:36.149+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:36.149+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:36.149+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:36.149+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:36.151+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935786
[2025-07-18T16:25:36.151+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:36.151+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:36.151+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935790
[2025-07-18T16:25:36.152+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-07-18T16:25:36.153+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka version: 3.5.1
[2025-07-18T16:25:36.153+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka commitId: 2c6fb6c54472e90a
[2025-07-18T16:25:36.153+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO AppInfoParser: Kafka startTimeMs: 1752855935790
[2025-07-18T16:25:36.156+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:36.159+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:36.160+0000] {subprocess.py:93} INFO - 25/07/18 16:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[2025-07-18T16:25:44.257+0000] {local_task_job_runner.py:294} WARNING - State of this instance has been externally set to scheduled. Terminating instance.
[2025-07-18T16:25:44.267+0000] {process_utils.py:131} INFO - Sending 15 to group 1354. PIDs of all processes in the group: [1355, 1354]
[2025-07-18T16:25:44.268+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 1354
[2025-07-18T16:25:44.270+0000] {taskinstance.py:1632} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-07-18T16:25:44.274+0000] {subprocess.py:104} INFO - Sending SIGTERM signal to process group
[2025-07-18T16:25:44.305+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/operators/bash.py", line 201, in execute
    result = self.subprocess_hook.run_command(
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/hooks/subprocess.py", line 91, in run_command
    for raw_line in iter(self.sub_process.stdout.readline, b""):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 1634, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2025-07-18T16:25:44.321+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=restaurant_pipeline, task_id=stream_to_bronze, execution_date=20250718T161800, start_date=20250718T162007, end_date=20250718T162544
[2025-07-18T16:25:44.369+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 189 for task stream_to_bronze (Task received SIGTERM signal; 1354)
[2025-07-18T16:25:44.409+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1354, status='terminated', exitcode=1, started='16:20:07') (1354) terminated with exit code 1
[2025-07-18T16:25:44.411+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1355, status='terminated', started='16:20:07') (1355) terminated with exit code None
